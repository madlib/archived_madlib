import plpy
import datetime
from math import floor, log, pow, sqrt

"""@file svdmf.py_in

Implementation of partial SVD decomposition of a sparse matrix into U and V components.
"""

# ----------------------------------------
# Logging
# ----------------------------------------
def info( msg):
    #plpy.info( datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S") + ' : ' + msg);
	plpy.info( msg);
	
# ----------------------------------------
# Logging
# ----------------------------------------
def svdmf_run( madlib_schema, input_matrix, col_name, row_name, value, num_features):
   return(svdmf_run_full( madlib_schema, input_matrix, col_name, row_name, value, num_features, 1000, .0001))

# ----------------------------------------
# Main: svdmf_run
# ----------------------------------------
def svdmf_run_full( madlib_schema, input_matrix, col_name, row_name, value, num_features, NUM_ITERATIONS, MIN_IMPROVEMENT):
	"""
	These constants are important for the execution of the algorithms and may need to be changed for some types of data. 

	ORIGINAL_STEP - is a size of the step in the direction of gradient. 
	It is divided by ~M*N*(M+N). Too large of a step 
	may cause algorithm failure to converge and overflows. 
	Too small of the step will make execution longer than necessary.

	SPEEDUP_CONST, FAST_SPEEDUP_CONST and SLOWDOWN_CONST - 
	Size of step is changing dynamically during the execution, 
	as long as algorithm making a progress step size is increased 
	at each iteration by multiplying by SPEEDUP_CONST (speed-up constant > 1). 
	When algorithm starts to fail to converge step is made smaller abruptly 
	to avoid overflow by multiplying by SLOWDOWN_CONST. 
	In some instances initial step size is very small, 
	to quickly get it into the suitable range FAST_SPEEDUP_CONST (> 1) is used.
	This means that generally FAST_SPEEDUP_CONST >> SPEEDUP_CONST. 
	Those values may be changed if necessary.

	NUM_ITERATIONS is the maximum number of iterations to perform. 
	Generally this should not be the cause for termination, but on some data it is 
	possible that algorithm is making progress, but at a very slow pace, 
	in this case termination will occur when this number of iterations is reached.

	MIN_NUM_ITERATIONS - sometimes algorithm starts with very small progress 
	and it then increases. To avoid the case when it exits prematurely one can 
	set a min number of iterations after which progress will be checked

	MIN_IMPROVEMENT - minimum improvement that has to be sustained for 
	algorithm to continue. It is compared with consecutive error terms. 
	Sometimes it needs to be lowered 
	in instances when initial progress may be too small, or increased 
	if execution it taking too long, and level of needed precision is surpassed.   

	INIT_VALUE - initial value that new rows are starting with. 
	Value is almost irrelevant in most cases, but it should not be 0, 
	since this value is used as a multiple in subsequent steps.  

	EARLY_TEMINATE - tells algorithm to terminate early if after a given 
	dimension (< K) error is less than MIN_IMPROVEMENT. It should generally be TRUE. 
	Since there is no contribution that one should expect from adding consequent dimensions. 
	"""

	# Record the time
	start = datetime.datetime.now();

	ORIGINAL_STEP = .001; 
	SPEEDUP_CONST = 1.1;
	FAST_SPEEDUP_CONST = 10.0;
	SLOWDOWN_CONST = .1;
	#NUM_ITERATIONS = 1000;
	#MIN_IMPROVEMENT = 0.000001;
	MIN_NUM_ITERATIONS = 1;
	IMPROVEMENT_REACHED = True;
	INIT_VALUE = 0.1;
	EARLY_TEMINATE = True;

	error=0;
	keep_ind = 1;
	SD_ind = 1;

	# Find sizes of the input and number of elements in the input
	res = plpy.execute('SELECT count(distinct ' + col_name + ') AS c FROM ' + input_matrix + ';'); 
	feature_y = res[0]['c'];
	res = plpy.execute('SELECT count(distinct ' + row_name + ') AS c FROM ' + input_matrix + ';'); 
	feature_x = res[0]['c'];
	res = plpy.execute('SELECT count(*) AS c FROM ' + input_matrix + ';'); 
	cells = res[0]['c'];

	# Adjust step size based on the size  
	#ORIGINAL_STEP = ORIGINAL_STEP/(feature_x+feature_y);

	# Parameters summary:
	info( 'Started svdmf_run() with parameters:');
	info( ' * input_matrix = %s' % input_matrix);
	info( ' * col_name = %s' % col_name);
	info( ' * row_name = %s' % row_name);
	info( ' * value = %s' % value);
	info( ' * num_features = %s' % str(num_features));
	info('got there...');

	# Create output and intermediate (temp) tables necessary for the execution
	# matrix_u and matrix_v are for end results. tables e1, e2, e are for residual errors
	# tables S1, S2, D1, D2 are for estimates of row values of u and v.
	sql = '''
	DROP TABLE IF EXISTS ''' + madlib_schema + '''.matrix_u;
	CREATE TABLE ''' + madlib_schema + '''.matrix_u(
		col_num INT, 
		row_num INT,
		val FLOAT
	);
	DROP TABLE IF EXISTS ''' + madlib_schema + '''.matrix_v;
	CREATE TABLE ''' + madlib_schema + '''.matrix_v(
		row_num INT,
		col_num INT, 
		val FLOAT
	);
	DROP TABLE IF EXISTS e1;
	CREATE TEMP TABLE e1(
		row_num INT, 
		col_num INT,
		val FLOAT
	) m4_ifdef( `__GREENPLUM__', `DISTRIBUTED BY (row_num, col_num)');
	DROP TABLE IF EXISTS S1;
	CREATE TEMP TABLE S1(
		col_num INT,
		row_num INT,
		val FLOAT
	) m4_ifdef( `__GREENPLUM__', `DISTRIBUTED BY (col_num)');
	DROP TABLE IF EXISTS S2;
	CREATE TEMP TABLE S2(
		col_num INT,
		row_num INT,
		val FLOAT
	) m4_ifdef( `__GREENPLUM__', `DISTRIBUTED BY (col_num)');
	DROP TABLE IF EXISTS D1;
	CREATE TEMP TABLE D1(
		row_num INT,
		col_num INT,
		val FLOAT
	) m4_ifdef( `__GREENPLUM__', `DISTRIBUTED BY (row_num)');
	DROP TABLE IF EXISTS D2;
	CREATE TEMP TABLE D2(
		row_num INT,
		col_num INT,
		val FLOAT
	) m4_ifdef( `__GREENPLUM__', `DISTRIBUTED BY (row_num)');
	DROP TABLE IF EXISTS e;
	CREATE TABLE e(
		row_num INT, 
		col_num INT,
		val FLOAT
	) m4_ifdef( `__GREENPLUM__', `DISTRIBUTED BY (row_num,col_num)');
	''';
	plpy.execute(sql);

	# Copy original data into a temp table
	info( 'Copying the source data into a temporary table...');
	plpy.execute('INSERT INTO e1 SELECT ' + row_name + ', ' + col_name + ', ' + str(value) + ' FROM ' + input_matrix + ';');
    
	# Create tables to keep most of the execution data
	sql = '''
		TRUNCATE TABLE S1;
		TRUNCATE TABLE S2;
		TRUNCATE TABLE D1;
		TRUNCATE TABLE D2;
		''';
	plpy.execute(sql);
	
	# populate initial vectors
	plpy.execute('INSERT INTO S1 (row_num, col_num, val) SELECT (g.a-1)/' +str(num_features)+ '+1, (g.a-1)%' +str(num_features)+ '+1, random() FROM generate_series(1,'+str(feature_x*num_features)+') AS g(a);');
	plpy.execute('INSERT INTO D1 (col_num, row_num, val) SELECT (g.a-1)/' +str(num_features)+ '+1, (g.a-1)%' +str(num_features)+ '+1, random() FROM generate_series(1,'+str(feature_y*num_features)+') AS g(a);');
	
	SD_ind = 1;
	i = 0;
	step = ORIGINAL_STEP;
	imp_reached = False;
        
	while(True):
		
		i = i + 1;
		
		sql = '''            
			TRUNCATE TABLE e; 
			''';
		plpy.execute(sql);
		
		# Create current predictions for the values and compute error per term
		#plpy.execute('INSERT INTO e SELECT a.row_num, a.col_num, a.val-o.prod FROM (SELECT s.row_num, d.col_num, '+madlib_schema+'.array_dot(d.val,s.val) AS prod FROM (SELECT row_num, array_agg(val ORDER BY col_num) AS val FROM S'+str(SD_ind)+' GROUP BY row_num) AS s CROSS JOIN (SELECT col_num, array_agg(val ORDER BY row_num) AS val FROM D'+str(SD_ind)+' GROUP BY col_num) AS d) as o, e1 as a WHERE a.row_num=o.row_num AND a.col_num=o.col_num;');
		plpy.execute('INSERT INTO e SELECT a.row_num, a.col_num, a.val-o.prod FROM (SELECT s.row_num, d.col_num, sum(s.val*d.val) AS prod FROM S'+str(SD_ind)+' AS s JOIN D'+str(SD_ind)+' AS d ON d.row_num = s.col_num GROUP BY s.row_num, d.col_num) AS o, e1 AS a WHERE a.row_num=o.row_num AND a.col_num=o.col_num');
		old_error = error;
				
		res = plpy.execute('SELECT sqrt(sum(val*val)) AS c FROM e;');
		error = res[0]['c'];
		
		info( '...Iteration ' + str(i) + ': residual_error = '+ str(error) +', step_size = ' + str(step) + ', min_improvement = ' + str(MIN_IMPROVEMENT));
		
		# Check if progress is being made or max number of iterations reached
		if(((abs(error - old_error) < MIN_IMPROVEMENT) and (i >= MIN_NUM_ITERATIONS) and ((error < MIN_IMPROVEMENT) or (not IMPROVEMENT_REACHED) or (imp_reached))) or (NUM_ITERATIONS < i)):
			break;
		
		if((abs(error - old_error) >= MIN_IMPROVEMENT) and (old_error > 0)):
			imp_reached = True;
		
		# Check if step size need to be increased or decreased
		if((error > old_error) and (old_error != 0)) :
			error = 0;
			step = step*SLOWDOWN_CONST;
			SD_ind = SD_ind%2+1;
			continue;
		elif(sqrt((error - old_error)*(error - old_error)) < .1*MIN_IMPROVEMENT):
			step = step*FAST_SPEEDUP_CONST;
		else:
			step = step*SPEEDUP_CONST;
			
		# Empty intermediate tables    
		plpy.execute('TRUNCATE TABLE S'+str(SD_ind%2+1)+';');
		plpy.execute('TRUNCATE TABLE D'+str(SD_ind%2+1)+';');
	
		# Update values of the vectors   
		# The commented out line are an implementation that uses ordered aggrigates that we chose to avoid for compatibility reasons 
		#plpy.execute('INSERT INTO S'+str(SD_ind%2+1)+' SELECT s.col_num, s.row_num, s.val+'+str(step)+'*o.prod FROM (SELECT e.row_num, d.row_num as col_num, '+madlib_schema+'.array_dot(d.val,e.val) AS prod FROM (SELECT row_num, array_agg(val ORDER BY col_num) AS val FROM e GROUP BY row_num) AS e CROSS JOIN (SELECT row_num, array_agg(val ORDER BY col_num) AS val FROM D'+str(SD_ind)+' GROUP BY row_num) AS d) as o, S'+str(SD_ind)+' as s WHERE s.col_num = o.col_num AND s.row_num = o.row_num;');
		#plpy.execute('INSERT INTO D'+str(SD_ind%2+1)+' SELECT d.row_num, d.col_num, d.val+'+str(step)+'*o.prod FROM (SELECT s.col_num AS row_num, e.col_num, '+madlib_schema+'.array_dot(e.val,s.val) AS prod FROM (SELECT col_num, array_agg(val ORDER BY row_num) AS val FROM S'+str(SD_ind)+' GROUP BY col_num) AS s CROSS JOIN (SELECT col_num, array_agg(val ORDER BY row_num) AS val FROM e GROUP BY col_num) AS e) as o, D'+str(SD_ind)+' as d WHERE d.col_num = o.col_num AND d.row_num = o.row_num;');
		
		plpy.execute('INSERT INTO S'+str(SD_ind%2+1)+' SELECT s.col_num, s.row_num, s.val+'+str(step)+'*o.prod FROM (SELECT e.row_num, d.row_num AS col_num, sum(e.val*d.val) AS prod FROM e JOIN D'+str(SD_ind)+' AS d ON e.col_num = d.col_num GROUP BY e.row_num, d.row_num) AS o, S'+str(SD_ind)+' AS s WHERE s.row_num=o.row_num AND s.col_num=o.col_num');  
		plpy.execute('INSERT INTO D'+str(SD_ind%2+1)+' SELECT d.row_num, d.col_num, d.val+'+str(step)+'*o.prod FROM (SELECT s.col_num AS row_num, e.col_num, sum(e.val*s.val) AS prod FROM e JOIN S'+str(SD_ind)+' AS s ON s.row_num = e.row_num GROUP BY e.col_num, s.col_num) AS o, D'+str(SD_ind)+' AS d WHERE d.row_num=o.row_num AND d.col_num=o.col_num');
		
		SD_ind = SD_ind%2+1;
		
		# Save previous vector and prepare for computing another one
		keep_ind = keep_ind%2+1;
		
		if((error < MIN_IMPROVEMENT) and (EARLY_TEMINATE)):
			break;
	
	plpy.execute('INSERT INTO ' + madlib_schema + '.matrix_u SELECT * FROM S'+str(SD_ind)+';'); 
	plpy.execute('INSERT INTO ' + madlib_schema + '.matrix_v SELECT * FROM D'+str(SD_ind)+';'); 

	# Runtime evaluation
	end = datetime.datetime.now();
	minutes, seconds = divmod( (end - start).seconds, 60)
	microsec = (end - start).microseconds	            
	return ('''
		Finished SVD matrix factorisation for %s (%s, %s, %s). 
		Results: 
		 * total error = %s
		Output:
		 * table : ''' + madlib_schema + '''.matrix_u
		 * table : ''' + madlib_schema + '''.matrix_v
		Time elapsed: %d minutes %d.%d seconds.
		''') % (input_matrix, row_name, col_name, value, str(error), minutes, seconds, microsec)
    