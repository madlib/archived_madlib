/* ----------------------------------------------------------------------- */
/**
 *
 * @file svm.sql_in
 *
 * @brief SQL functions for SVM (Poisson)
 * @date July 2015
 *
 * @sa For a brief introduction to SVM (Poisson), see the
 *     module description \ref grp_svm.
 *
 */
/* ----------------------------------------------------------------------- */
m4_include(`SQLCommon.m4')

/**
@addtogroup grp_svm

<div class="toc"><b>Contents</b><ul>
<li class="level1"><a href="#svm_classification">Classification Function</a></li>
<li class="level1"><a href="#svm_regression">Regression Function</a></li>
<li class="level1"><a href="#kernel_params">Kernel Parameters</a></li>
<li class="level1"><a href="#parameters">Other Parameters</a></li>
<li class="level1"><a href="#predict">Prediction Functions</a></li>
<li class="level1"><a href="#example">Examples</a></li>
<li class="level1"><a href="#background">Technical Background</a></li>
<li class="level1"><a href="#literature">Literature</a></li>
<li class="level1"><a href="#related">Related Topics</a></li>
</ul></div>

Support Vector Machines (SVMs) are models for regression and classification
tasks. SVM models have two particularly desirable features: robustness in the
presence of noisy data and applicability to a variety of data configurations. At
its core, a <em>linear</em> SVM model is a hyperplane separating the two
distinct classes of data (in the case of classification problems), in such a way
that the distance between the hyperplane and the nearest training data point
(called the <em>margin</em>) is maximized. Vectors that lie on this margin are
called support vectors. With the support vectors fixed, perturbations of vectors
beyond the margin will not affect the model; this contributes to the models
robutstness.  Subsituting a kernel function for the usual inner product, one can
approximate a large variety of decision boundaries beyond linear hyperplanes.
@brief Solves classification and regression problems by separating the data with
a hyperplane or other nonlinear decision boundaries.

@anchor svm_classification
@par Classification Training Function
SVM classification training function has the following format:
<pre class="syntax">
svm_classification(
    source_table,
    model_table,
    dependent_varname,
    independent_varname,
    kernel_func,
    kernel_params,
    grouping_col,
    params,
    verbose
    )
</pre>
\b Arguments
<DL class="arglist">
  <DT>source_table</DT>
  <DD>TEXT. The name of the table containing the training data.</DD>

  <DT>model_table</DT>
  <DD>TEXT. Name of the output table containing the model. Details of the output
   tables provided below.
  </DD>

  <DT>dependent_varname</DT>
  <DD> TEXT. Name of the dependent variable column. For classification, this column
  can contain values of any type, but must assume exactly two distinct values.
  Otherwise, an error will be thrown.
  </DD>

  <DT>independent_varname</DT>
  <DD>TEXT. Expression list to evaluate for the
    independent variables. An intercept variable is not assumed. It is common to
    provide an explicit intercept term by including a single constant \c 1 term
    in the independent variable list. Expression should be able to be casted
    into DOUBLE PRECISION [].
    </DD>

  <DT>kernel_func (optional)</DT>
  <DD>TEXT, default: 'linear'.
    The type of kernel. Currently, three types are supported: 'linear',
    'gaussian', and 'polynomial'. The text can be any subset of the three
    strings; for eg. kernel_func='ga' will create a Gaussian kernel.
    </DD>

  <DT>kernel_params (optional)</DT>
  <DD>TEXT, defaults: NULL
   <br>Parameters for non-linear kernel in a comma-separated string of key-value pairs.
   The parameters differ depending on the value of \e kernel_func. See the description below for details.
  </DD>

  <DT>grouping_col (optional)</DT>
  <DD>TEXT, default: NULL. An expression list used to group
    the input dataset into discrete groups, running one model per group.
    Similar to the SQL "GROUP BY" clause. When this value is NULL, no
    grouping is used and a single model is generated.</DD>

  <DT>params (optional)</DT>
  <DD>TEXT, default: NULL.
    <br> Parameters for optimization and regularization in a comma-separated string of key-value pairs. If a list of values are provided, then cross-validation will be performed to select the \e best value from the list.
    See the description below for details. </DD>

  <DT>verbose (optional)</DT>
  <DD>BOOLEAN default: FALSE.
    Verbose output of the results of training.</DD>
</DL>

<b>Output tables</b>
<br>
    The model table produced by svm contains the following columns:
    <table class="output">
      <tr>
        <th>coef</th>
        <td>FLOAT8. Vector of the coefficients.</td>
      </tr>
      <tr>
        <th>grouping_key</th>
        <td>TEXT Identifies the group to which the datum belongs.</td>
      </tr>
      <tr>
        <th>num_rows_processed</th>
        <td>BIGINT. Numbers of rows processed.</td>
      </tr>
      <tr>
        <th>num_rows_skipped</th>
        <td>BIGINT. Numbers of rows skipped due to missing values or failures.</td>
      </tr>
      <tr>
        <th>num_iterations</th>
        <td>INTEGER. Number of iterations completed by stochastic gradient descent
        algorithm. The algorithm either converged in this number of iterations
        or hit the maximum number specified in the optimization parameters. </td>
      </tr>
      <tr>
        <th>__dep_var_mapping</th>
        <td>TEXT[]. Vector of dependent variable labels. The first entry will
        correspond to -1 and the second to +1, for internal use.</td>
      </tr>
    </table>

    An auxiliary table named \<model_table\>_random is created if the kernel is not linear. It contains data needed to embed test data into random feature space (see reference [2,3]). This data is used internally by svm_predict and not meaningful on its own. And a summary table named \<model_table\>_summary is also created at the same time, which has the following columns:
    <table class="output">
    <tr>
        <th>method</th>
        <td>'svm'</td>
    </tr>
    <tr>
        <th>version_number</th>
        <td>Version of madlib which was used to generate the model.</td>
    </tr>
    <tr>
        <th>source_table</th>
        <td>The data source table name.</td>
    </tr>
    <tr>
        <th>model_table</th>
        <td>The model table name.</td>
    </tr>
    <tr>
        <th>dependent_varname</th>
        <td>The dependent variable.</td>
    </tr>
    <tr>
        <th>independent_varname</th>
        <td>The independent variables.</td>
    </tr>
    <tr>
        <th>kernel_func</th>
        <td>The kernel function.</td>
    </tr>
    <tr>
        <th>kernel_parameters</th>
        <td>The kernel parameters, as well as random feature map data.</td>
    </tr>
    <tr>
        <th>grouping_col</th>
        <td>Columns on which to group.</td>
    </tr>
    <tr>
        <th>optim_params</th>
        <td>A string containing the optimization parameters.</td>
    </tr>
    <tr>
        <th>reg_params</th>
        <td>A string containing the regularization parameters.</td>
    </tr>
    <tr>
        <th>num_all_groups</th>
        <td>Number of groups in svm training.</td>
    </tr>
    <tr>
        <th>num_failed_groups</th>
        <td>Number of failed groups in svm training.</td>
    </tr>
    <tr>
      <th>total_rows_processed</th>
      <td>Total numbers of rows processed in all groups.</td>
    </tr>
    <tr>
      <th>total_rows_skipped</th>
      <td>Total numbers of rows skipped in all groups due to missing
      values or failures.</td>
    </tr>
   </table>


@anchor svm_regression
@par Regression Training Function
SVM regression training function has the following format:
<pre class="syntax">
svm_regression(source_table,
    model_table,
    dependent_varname,
    independent_varname,
    kernel_func,
    kernel_params,
    grouping_col,
    params,
    verbose
    )
</pre>
\b Arguments

Specifications for regression are largely the same as for classification. In the
model table, there is no dependent variable mapping. Also, the following
arguments have specifications which differ from svm_classification:

<DL class="arglist">

<DT>dependent_varname</DT>
  <DD>TEXT. Name of the dependent variable column. For regression, this column
  can contain only values or expressions that can be cast as DOUBLE PRECISION.
  Otherwise, an error will be thrown.
  </DD>

<DT>params (optional)</DT>
<DD>TEXT, default: NULL.
The parameters \e epsilon and \e eps_table are only meaningful for regression. See descriptions below for more details.
</DD>
</DL>


@anchor kernel_params
@par Kernel Parameters
Kernel parameters are supplied in a string containing a comma-delimited
list of name-value pairs. All of these named parameters are optional, and
their order does not matter. You must use the format "<param_name> = <value>"
to specify the value of a parameter, otherwise the parameter is ignored.

When the \ref svm_classification() \e kernel_func argument value is \b 'gaussian', the \e kernel_params argument is a string containing name-value pairs with the following format. (Line breaks are inserted for readability.)
<pre class="syntax">
  'gamma = &lt;value>,
   n_components = &lt;value>,
   random_state = &lt;value>'
</pre>
\b Parameters
<DL class="arglist">
<DT>gamma</dt>
<DD>Default: 1/num_features. The parameter \f$\gamma\f$ in the Radius Basis Function kernel, i.e., \f$\exp(-\gamma||x-y||^2)\f$. Choosing a proper value for \e gamma is critical to the performance of kernel machine, e.g., while a large \e gamma tends to cause overfitting, a small \e gamma will make the model too constrained to capture the complexity of the data.
.</DD>

<DT>n_components</DT>
<DD>Default: 2*num_features. The dimensionality of the transformed feature space. A larger value lowers the variance of the estimate of kernel but requires more memory and takes longer to train.</DD>

<DT>random_state</DT>
<DD>Default: 1. Seed used by the random number generator. </DD>
</DL>

When the \ref svm_classification() \e kernel_func argument value is \b 'poly', the \e kernel_params argument is a string containing name-value pairs with the following format. (Line breaks are inserted for readability.)
<pre class="syntax">
  'coef0 = &lt;value>,
   degree = &lt;value>,
   n_components = &lt;value>,
   random_state = &lt;value>'
</pre>
\b Parameters
<DL class="arglist">
<DT>coef0</dt>
<DD>Default: 1.0. The independent term \f$q\f$ in \f$ (\langle x,y\rangle + q)^r \f$. Must be larger or equal to 0. When it is 0, the polynomial kernel is in homogeneous form.
</DD>

<DT>degree</dt>
<DD>Default: 3. The parameter \f$r\f$ in \f$ (\langle x,y\rangle + q)^r \f$.
</DD>

<DT>n_components</DT>
<DD>Default: 2*num_features. The dimensionality of the transformed feature space. A larger value lowers the variance of the estimate of kernel but requires more memory and takes longer to train.</DD>

<DT>random_state</DT>
<DD>Default: 1. Seed used by the random number generator. </DD>
</DL>


@anchor parameters
@par Other Parameters
Parameters in this sections are supplied in \e params argument as a string containing a comma-delimited
list of name-value pairs. Hyperparameter optimization can be carried out through the built-in cross validation mechanism, which is activated by assigning a value greater than 1 to the parameter \e n_folds in \e params.
All of these named parameters are optional, and their order does not matter.
You must use the format "<param_name> = <value>" to specify the value of a parameter, otherwise the parameter is ignored.
The validating values of the parameter are provided in a list, i.e., "lambda = [0.01, 0.1, 1]".
For example, if one wanted to regularize with the L1 norm and use a lambda value from the set {0.3, 0.4, 0.5}, one would input 'lambda={0.3, 0.4, 0.5}, norm=L1, n_folds=10' in \e params. Note that the use of '{}' and '[]' are both valid here.

Not all parameters below can be cross-validated. And for those who do, their default values, as described below, are specified in a list, i.e., [0.01].
<pre class="syntax">
  'init_stepsize = &lt;value>,
   decay_factor = &lt;value>,
   max_iter = &lt;value>,
   tolerance = &lt;value>,
   lambda = &lt;value>,
   norm = &lt;value>,
   epsilon = &lt;value>,
   eps_table = &lt;value>,
   validation_result = &lt;value>,
   n_folds = &lt;value>'
</pre>
\b Parameters
<DL class="arglist">

<DT>init_stepsize</dt>
<DD>Default: [0.01]. Also known as the inital learning rate. A small value is usually desirable to ensure convergence, while a large value provides more room for progress during training. Since the best value depends on the condition number of the data, in practice one can search in a exponential grid using the built-in cross validation, i.e., "init_stepsize = [1, 0.1, 0.001]". This can be done on a subsampled dataset which usually provides a good estimate of the condition number of the whole dataset.
</DD>

<DT>decay_factor</DT>
<DD>Default: [0.9]. Control the learning rate schedule: 0 means constant rate; -1 means inverse scaling, i.e., stepsize = init_stepsize / iteration; > 0 means exponential decay, i.e., stepsize = init_stepsize * decay_factor^iteration.
</DD>

<DT>max_iter</dt>
<DD>Default: [100]. The maximum number of iterations allowed.
</DD>

<DT>tolerance</dt>
<DD>Default: 1e-10. The criteria to end iterations. The training stops whenever the difference between the training models of two consecutive iterations is smaller than \e tolerance or the iteration number is larger than \e max_iter.
</DD>

<DT>lambda</dt>
<DD>Default: [0.01]. Regularization parameter, positive.
</DD>

<DT>norm</dt>
<DD>Default: 'L2'. Name of the regularization, either 'L2' or 'L1'.
</DD>

<DT>epsilon</dt>
<DD>Default: [0.01].
Determines the \f$\epsilon\f$ for \f$\epsilon\f$-regression. Ignored during classification.
When training the model, differences of less than \f$\epsilon\f$ between estimated labels
and actual labels are ignored. A larger \f$\epsilon\f$ will yield a model
with fewer support vectors, but will not generalize as well to future data.
Generally, it has been suggested that epsilon should increase with noisier
data, and decrease with the number of samples. See [5].
</DD>

<DT>eps_tabl</dt>
<DD>Default: NULL.
Name of the table that contains values of epsilon for different groups. Ignored when \e grouping_col is NULL.
The table consists of the column named epsilon which specifies the epsilon values, as well as those columns used in \e grouping_col. Extra groups are ignored, and groups not present in this table will use the epsilon value specified in parameter \e epsilon.
</DD>

<DT>validation_result</dt>
<DD>Default: NULL.
Name of the table to store the cross validation results including the values of parameters and their averaged error values. For now simple metric like 0-1 loss is used for classification and mean square error is used for regression. The table is only created if the name is not NULL.
</DD>

<DT>n_folds</dt>
<DD>Default: 0.
Number of folds. Must be at least 2 to activate cross validation. If a value of k > 2 is specified, each fold is then used as a validation set once while the k - 1 remaining fold form the training set.
</DD>
</DL>


@anchor predict
@par Prediction Function
The prediction function is provided to estimate the conditional mean given a new
predictor. It has the following syntax:
<pre class="syntax">
svm_predict(model_table,
            new_data_table,
            id_col_name,
            output_table)
</pre>

\b Arguments
<DL class="arglist">
  <DT>model_table</DT>
  <DD>TEXT. Model table produced by the training function.</DD>

  <DT>new_data_table</DT>
  <DD>TEXT. Name of the table containing prediction data. This table is expected
  to contain the same features that were used during training. The table should
  also contain id_col_name used for identifying each row.</DD>

  <DT>id_col_name</DT>
  <DD>TEXT. The name of the id column in the input table.</DD>

  <DT>output_table</DT>

  <DD>TEXT. Name of the table to which output predictions are written. If this
table name is already in use then an error is returned. The table contains the
id_col_name column giving the 'id' for each prediction and the prediction
columns for the dependent variable.</DD>
</DL>

@anchor example
@par Examples
-#  Create an input data set.
<pre class="example">
CREATE TABLE houses (id INT, tax INT, bedroom INT, bath FLOAT, price INT,
            size INT, lot INT);
COPY houses FROM STDIN WITH DELIMITER '|';
  1 |  590 |       2 |    1 |  50000 |  770 | 22100
  2 | 1050 |       3 |    2 |  85000 | 1410 | 12000
  3 |   20 |       3 |    1 |  22500 | 1060 |  3500
  4 |  870 |       2 |    2 |  90000 | 1300 | 17500
  5 | 1320 |       3 |    2 | 133000 | 1500 | 30000
  6 | 1350 |       2 |    1 |  90500 |  820 | 25700
  7 | 2790 |       3 |  2.5 | 260000 | 2130 | 25000
  8 |  680 |       2 |    1 | 142500 | 1170 | 22000
  9 | 1840 |       3 |    2 | 160000 | 1500 | 19000
 10 | 3680 |       4 |    2 | 240000 | 2790 | 20000
 11 | 1660 |       3 |    1 |  87000 | 1030 | 17500
 12 | 1620 |       3 |    2 | 118600 | 1250 | 20000
 13 | 3100 |       3 |    2 | 140000 | 1760 | 38000
 14 | 2070 |       2 |    3 | 148000 | 1550 | 14000
 15 |  650 |       3 |  1.5 |  65000 | 1450 | 12000
\\.
</pre>
-#  Train a classification model. First, a linear model.
<pre class="example">
SELECT madlib.svm_classification('houses',
                                 'houses_svm',
                                 'price < 100000',
                                 'ARRAY[1, tax, bath, size]'
                           );
</pre>
-# Generate a nonlinear model using gaussian kernel. This time we specify the intital step size and maximum number of iteration to run. As part of the kernel parameter, we choose 10 as the dimension of the space where we train svm. A larger number will lead to a more powerful model but run the risk of overfitting. As a result, the model will be a 10 dimensional vector, instead of 4 as in the case of linear model, which we will verify when we examine the models.
<pre class="example">
SELECT madlib.linregr_train( 'houses',
                             'houses_svm_gaussian',
                             'price < 100000',
                             'ARRAY[1, tax, bath, size]',
                             'gaussian',
                             'n_components=10',
                             '',
                             'init_stepsize=1, max_iter=200'
                           );
</pre>
-# Examine the resulting models.
<pre class="example">
-- Set extended display on for easier reading of output
\\x ON
SELECT * FROM houses_svm;
</pre>
Result:
<pre class="result">
-[ RECORD 1 ]+---------------------------------------------------------------------------
coef               | [0.113989576847, -0.00226133300602, -0.0676303607996, 0.00179440841072]
loss               | 9.21745071385
norm_of_gradient   | 108.171180769
num_iterations     | 100
num_rows_processed | 15
num_rows_skipped   | 0
dep_var_mapping    | [False, True]
</pre>

-# View the results from kernel svm.
<pre class="example">
-- Set extended display on for easier reading of output
\\x ON
SELECT * FROM houses_svm_gaussian;
</pre>
Result:
<pre class="result">
-[ RECORD 1 ]+---------------------------------------------------------------------------
coef               | [-2.00789985255, 2.02625531256, -1.09903715824, 2.04431020735, 3.14208435644, 0.14838741816, 2.07527256499, 3.0816372960, 0.853428649407, 3.63747384926]
loss               | 0.255909866744
norm_of_gradient   | 0.0715415776655
num_iterations     | 184
num_rows_processed | 15
num_rows_skipped   | 0
dep_var_mapping    | [False, True]
</pre>

-# Use the prediction function to evaluate the models. The predicted results are in the \e prediction column and the actual data is in the \e target column.
For the linear model:
<pre class="example">
SELECT madlib.svm_predict('houses_svm', 'houses', 'id', 'houses_pred');
SELECT *, price < 100000 AS target FROM houses JOIN houses_pred USING (id)
</pre>
and for the gaussian model:
<pre class="example">
SELECT madlib.svm_predict('houses_svm_gaussian', 'houses', 'id', 'houses_pred_gaussian');
SELECT *, price < 100000 AS target FROM houses JOIN houses_pred_gaussian USING (id)
</pre>


@anchor background
@par Technical Background

To solve linear SVM, the following objective function is minimized:
\f[
    \underset{w,b}{\text{Minimize }} \lambda||w||^2 + \frac{1}{n}\sum_{i=1}^n \ell(y_i,f_{w,b}(x_i))

\f]

 where \f$(x_1,y_1),\ldots,(x_n,y_n)\f$ are labeled training data and
 \f$\ell(y,f(x))\f$ is a loss function. When performing classification,
 \f$\ell(y,f(x)) = \max(0,1-yf(x))\f$ is the <em>hinge loss</em>.
 For regression, the loss function \f$\ell(y,f(x)) = \max(0,|y-f(x)|-\epsilon)\f$
 is used.

 If \f$ f_{w,b}(x) = \langle w, x\rangle + b\f$ is linear, then the
 objective function is convex and incremental gradient descent (IGD, or SGD)
 can be applied to find a global minimum. See Feng, et al. [1] for more details.

To learn with Gaussian or polynomial kernels, the training data is first mapped
via a <em>random feature map</em> in such a way that the usual inner product in
the feature space approximates the kernel function in the input space. The
linear SVM training function is then run on the resulting data. See the papers
[2,3] for more information on random feature maps.

Also, see the book [4] by Scholkopf and Smola  for more details of SVMs in general.

@anchor literature
@literature

@anchor svm-lit-1
[1] Xixuan Feng, Arun Kumar, Ben Recht, and Christopher Re:
    Towards a Unified Architecture for in-RDBMS analytics,
    in SIGMOD Conference, 2012
    http://www.eecs.berkeley.edu/~brecht/papers/12.FengEtAl.SIGMOD.pdf

@anchor svm-lit-2
[2] Purushottam Kar and Harish Karnick: Random Feature Maps for Dot
    Product Kernels, Proceedings of the 15th International Conference
    on Artificial Intelligence and Statistics, 2012,
    http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2012_KarK12.pdf

@anchor svm-lit-3
[3] Ali Rahmini and Ben Recht: Random Features for Large-Scale
Kernel Machines, Neural Information Processing Systems 2007,
    http://www.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf

@anchor svm-lit-4
[4] Bernhard Scholkopf and Alexander Smola: Learning with Kernels,
    The MIT Press, Cambridge, MA, 2002.

@anchor svm-lit-5
[5] Vladimir Cherkassky and Yunqian Ma: Practical Selection of SVM Parameters
    and Noise Estimation for SVM Regression, Neural Networks, 2004
    http://www.ece.umn.edu/users/cherkass/N2002-SI-SVM-13-whole.pdf

@anchor related
@par Related Topics

File svm.sql_in documenting the training function

@internal
@sa Namespace SVM (documenting the driver/outer loop implemented in
    Python), Namespace
    \ref madlib::modules::regress documenting the implementation in C++
@endinternal
*/


DROP TYPE IF EXISTS MADLIB_SCHEMA.linear_svm_result CASCADE;
CREATE TYPE MADLIB_SCHEMA.linear_svm_result AS (
        coefficients        double precision[],
        loss                double precision,
        norm_of_gradient    double precision,
        num_rows_processed  bigint
);

--------------------------------------------------------------------------
-- create SQL functions for IGD optimizer
--------------------------------------------------------------------------
-- cannot be labeled as STRICT because we set previous_state NULL initially
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.linear_svm_igd_transition(
        state           double precision[],
        ind_var         double precision[],
        dep_var         double precision,
        previous_state  double precision[],
        dimension       integer,
        stepsize        double precision,
        reg             double precision,
        is_l2           boolean,
        n_tuples        integer,
        epsilon         double precision,
        is_svc          boolean
)
RETURNS double precision[] AS 'MODULE_PATHNAME'
LANGUAGE C IMMUTABLE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `NO SQL');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.linear_svm_igd_merge(
        state1 double precision[],
        state2 double precision[])
RETURNS double precision[] AS 'MODULE_PATHNAME'
LANGUAGE C IMMUTABLE STRICT
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `NO SQL');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.linear_svm_igd_final(
        state double precision[])
RETURNS double precision[] AS 'MODULE_PATHNAME'
LANGUAGE C IMMUTABLE STRICT
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `NO SQL');

/**
 * @internal
 * @brief Perform one iteration of the incremental gradient
 *        method for computing linear support vector machine
 */
DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.linear_svm_igd_step(
        /*+ ind_var */          double precision[],
        /*+ dep_var */          double precision,
        /*+ previous_state */   double precision[],
        /*+ dimension */        integer,
        /*+ stepsize */         double precision,
        /*+ reg */              double precision,
        /*+ is_l2 */            boolean,
        /*+ n_tuples */         integer,
        /*+ epsilon */          double precision,
        /*+ is_svc */           boolean
);
CREATE AGGREGATE MADLIB_SCHEMA.linear_svm_igd_step(
        /*+ ind_var */          double precision[],
        /*+ dep_var */          double precision,
        /*+ previous_state */   double precision[],
        /*+ dimension */        integer,
        /*+ stepsize */         double precision,
        /*+ reg */              double precision,
        /*+ is_l2 */            boolean,
        /*+ n_tuples */         integer,
        /*+ epsilon */          double precision,
        /*+ is_svc */           boolean
    ) (
    STYPE=double precision[],
    SFUNC=MADLIB_SCHEMA.linear_svm_igd_transition,
    m4_ifdef(`__POSTGRESQL__', `', `prefunc=MADLIB_SCHEMA.linear_svm_igd_merge,')
    FINALFUNC=MADLIB_SCHEMA.linear_svm_igd_final,
    INITCOND='{0,0,0,0,0,0,0}'
);

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.internal_linear_svm_igd_distance(
    /*+ state1 */ double precision[],
    /*+ state2 */ double precision[])
RETURNS double precision AS 'MODULE_PATHNAME'
LANGUAGE c IMMUTABLE STRICT
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `NO SQL');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.internal_linear_svm_igd_result(
    /*+ state */ double precision[])
RETURNS MADLIB_SCHEMA.linear_svm_result AS 'MODULE_PATHNAME'
LANGUAGE c IMMUTABLE STRICT
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `NO SQL');


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_regression(
    source_table text,
    model_table text,
    dependent_varname text,
    independent_varname text,
    kernel_func text,
    kernel_params text,
    grouping_col text,
    params text,
    verbose bool)
RETURNS void AS $$
    # indent according to PythonFunction
    global is_svc
    is_svc = False
    PythonFunction(svm, svm, svm)
$$ LANGUAGE plpythonu VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_regression(
    source_table text,
    model_table text,
    dependent_varname text,
    independent_varname text,
    kernel_func text,
    kernel_params text,
    grouping_col text,
    params text)
RETURNS void AS $$
    SELECT MADLIB_SCHEMA.svm_regression($1, $2, $3, $4, $5, $6, $7, $8, NULL);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_regression(
    source_table text,
    model_table text,
    dependent_varname text,
    independent_varname text,
    kernel_func text,
    kernel_params text,
    grouping_col text)
RETURNS void AS $$
    SELECT MADLIB_SCHEMA.svm_regression($1, $2, $3, $4, $5, $6, $7, NULL);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_regression(
    source_table text,
    model_table text,
    dependent_varname text,
    independent_varname text,
    kernel_func text,
    kernel_params text)
RETURNS void AS $$
    SELECT MADLIB_SCHEMA.svm_regression($1, $2, $3, $4, $5, $6, NULL);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_regression(
    source_table text,
    model_table text,
    dependent_varname text,
    independent_varname text,
    kernel_func text)
  RETURNS void AS $$
    SELECT MADLIB_SCHEMA.svm_regression($1, $2, $3, $4, $5, NULL);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_regression(
    source_table text,
    model_table text,
    dependent_varname text,
    independent_varname text)
  RETURNS void AS $$
    SELECT MADLIB_SCHEMA.svm_regression($1, $2, $3, $4, NULL);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');
-----------------

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_classification(
    source_table text,
    model_table text,
    dependent_varname text,
    independent_varname text,
    kernel_func text,
    kernel_params text,
    grouping_col text,
    params text,
    verbose bool)
RETURNS void AS $$
    # indent according to PythonFunction
    global is_svc
    is_svc = True
    PythonFunction(svm, svm, svm)
$$ LANGUAGE plpythonu VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');

-- all default value handling implemented in Python
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_classification(
    source_table text,
    model_table text,
    dependent_varname text,
    independent_varname text,
    kernel_func text,
    kernel_params text,
    grouping_col text,
    params text)
RETURNS void AS $$
    SELECT MADLIB_SCHEMA.svm_classification($1, $2, $3, $4, $5, $6, $7, $8, NULL);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_classification(
    source_table text,
    model_table text,
    dependent_varname text,
    independent_varname text,
    kernel_func text,
    kernel_params text,
    grouping_col text)
RETURNS void AS $$
    SELECT MADLIB_SCHEMA.svm_classification($1, $2, $3, $4, $5, $6, $7, NULL);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_classification(
    source_table text,
    model_table text,
    dependent_varname text,
    independent_varname text,
    kernel_func text,
    kernel_params text)
  RETURNS void AS $$
    SELECT MADLIB_SCHEMA.svm_classification($1, $2, $3, $4, $5, $6, NULL);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_classification(
    source_table text,
    model_table text,
    dependent_varname text,
    independent_varname text,
    kernel_func text)
  RETURNS void AS $$
    SELECT MADLIB_SCHEMA.svm_classification($1, $2, $3, $4, $5, NULL);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_classification(
    source_table text,
    model_table text,
    dependent_varname text,
    independent_varname text)
  RETURNS void AS $$
    SELECT MADLIB_SCHEMA.svm_classification($1, $2, $3, $4, NULL);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');

------ Prediction -------------------------------------------------------------
/**
 * @brief Scores the data points stored in a table using a learned linear support-vector model
 * @param model_table Name of table where the learned model to be used is stored
 * @param new_data_table Name of table/view containing the data points to be scored
 * @param id_col Name of column in new_data_table containing the integer identifier of data points
 *
 *
 *
 * @param output_table Name of table to store the results
 *
 * @return Textual summary of the algorithm run
 *
 * @internal
 * @sa This function is a wrapper for svm.svm_predict().
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_predict(
    model_table text,
    new_data_table text,
    id_col_name text,
    output_table text)
RETURNS void AS $$
PythonFunction(svm, svm, svm_predict)
$$ LANGUAGE plpythonu VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');

-- Online Help -----------------------------------------------------------

/**
 * @brief Help function
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_classification (
    message  TEXT
) RETURNS TEXT AS $$
    global is_svc
    is_svc = True
    PythonFunction(svm, svm, svm_help)
$$ LANGUAGE plpythonu
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `NO SQL', `');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_classification ()
RETURNS TEXT AS $$
  SELECT MADLIB_SCHEMA.svm_classification(NULL::TEXT)
$$ LANGUAGE SQL IMMUTABLE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `CONTAINS SQL', `');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_regression (
    message  TEXT
) RETURNS TEXT AS $$
    global is_svc
    is_svc = False
    PythonFunction(svm, svm, svm_help)
$$ LANGUAGE plpythonu
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `NO SQL', `');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_regression ()
RETURNS TEXT AS $$
  SELECT MADLIB_SCHEMA.svm_regression(NULL::TEXT)
$$ LANGUAGE SQL IMMUTABLE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `CONTAINS SQL', `');
