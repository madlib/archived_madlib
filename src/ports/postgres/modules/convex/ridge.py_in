
import plpy
import math
from validation.cv_utils import __cv_unique_string
from validation.cv_utils import __cv_produce_col_name_string
from validation.cv_utils import __cv_copy_data_with_id
from validation.cv_utils import __cv_split_data_using_id_col
from validation.cv_utils import __cv_split_data_using_id_tbl
from validation.cv_utils import __cv_summarize_result
from validation.cv_utils import __cv_generate_random_id
from convex.validate_args import __is_tbl_exists
from convex.validate_args import __is_tbl_has_rows
from convex.validate_args import __is_col_exists
from convex.validate_args import __is_tbl_exists_in_schema
from convex.validate_args import __is_scalar_col_no_null
from convex.validate_args import __is_array_col_same_dimension
from convex.validate_args import __is_array_col_no_null

## ========================================================================

def __ridge_cv_args(schema_madlib, func_args, param_to_try,
                    param_values, data_id,
                    id_is_random, validation_result, fold_num):
    """
    Generate argument list for the __ridge_newton_cv, used in the CV wrapper.
    
    Cross validation (CV) uses a universal interface for all the modules that it supports. Each module should
    provide its own adaptor to convert the inputs from the universal interface to the inputs that the
    module's CV function recognize.

    @param schema_madlib Name of the MADlib schema, properly escaped/quoted
    @param func_args A string list with each item having the form of "arg = value". It gives the arguments
                     needed by the modelling function in the cross validation.
    @param param_to_try The name of the parameter in the modelling function that CV will run through. For Ridge
                        regression, this one must be "lambda"
    @param param_values The different values that CV will run through.
    @param data_id Whether the data table has a unique ID associated with each row. If data_id exists, then
                   it would be be much easier to split the data into training and validation portions. Otherwise,
                   CV function will copy the original data (only copy the part used in the calculation) and add
                   a random ID for each row.
    @param id_is_random If data_id is not None, then whether this ID is randomly assigned to each row. If it is
                        not, then a mapping table will be created, mapping the original non-random ID to a
                        random ID.
    @param validation_result The table name to store the result of CV. It has 3 columns: lamda value, MSE error
                             average, and MSE error standard deviation.
    @param fold_num The cross validation fold number.

    The output is a dict with all parameters and their values for ridge's CV function __ridge_newton_cv
    """
    # ridge regression modelling function accepts the following parameters:
    # tbl_source - the data source table,
    # col_ind_var - independent variable column name,
    # col_dep_var - dependent variable column name,
    # tbl_output - output table of ridge (of no use and will be ignored)
    # lambda - the regulation parameter (of no use and will be ignored)
    # normalization - whether normalize the data (better convergence, consistent with R default and slower speed)
    allowed_args = set(["tbl_source", "col_ind_var", "col_dep_var", "tbl_output", "lambda", "normalization"])
    
    name_value = dict()
    name_value["schema_madlib"] = schema_madlib
    name_value["data_tbl"] = None
    name_value["col_ind_var"] = None
    name_value["col_dep_var"] = None
    name_value["normalization"] = None
    name_value["validation_result"] = validation_result
    name_value["fold_num"] = fold_num
    name_value["upto_fold"] = fold_num
    name_value["data_id"] = data_id
    name_value["id_is_random"] = id_is_random

    # the only parameter that CV can run for ridge regression is lambda
    if param_to_try != "lambda":
        plpy.error("Only lambda can be used to cross-validation in Ridge regression! {0} is not allowed.".format(param_to_try))
    name_value["lambda_values"] = param_values
 
    for s in func_args:
        items = s.split("=")
        if (len(items) != 2):
            plpy.error("Argument list syntax error!")
        arg_name = items[0].strip()
        arg_value = items[1].strip()

        if arg_name not in allowed_args:
            plpy.error("{0} is not a valid argument name for module Ridge.".format(arg_name))

        if arg_name == "tbl_source":
            name_value["data_tbl"] = arg_value
            continue

        if arg_name == "col_ind_var":
            name_value["col_ind_var"] = arg_value
            continue

        if arg_name == "col_dep_var":
            name_value["col_dep_var"] = arg_value
            continue

        if arg_name == "normalization":
            name_value["normalization"] = arg_value
            continue
    
    if name_value["normalization"] is None:
        name_value["normalization"] = False

    if name_value["data_tbl"] is None or name_value["col_ind_var"] is None or name_value["col_dep_var"] is None:
        plpy.error("tbl_source, col_ind_var and col_dep_var must be provided!")

    return name_value

## ========================================================================
    
def __ridge_ind_var_scales(**kwargs):
    """
    The mean and standard deviation for each element of the independent variable,
    which is an array in ridge and lasso.

    The output will be stored in a temp table: a mean array and a std array.

    This function is also used in lasso.
    """
    scale_factor = math.sqrt(1. - 1./float(kwargs["row_num"]))
    plpy.execute("""
        drop table if exists {tbl_scales};
        create temp table {tbl_scales} as
            select
                attr,
                avg(val) as mean,
                stddev(val) * {scale_factor} as std
            from (
                select
                    generate_series(1, {dimension}) as attr,
                    unnest({col_ind_var}) as val
                from {tbl_data}
            ) t
            group by attr
    """.format(scale_factor = scale_factor, **kwargs))
    return None

## ========================================================================
    
def __ridge_dep_var_scale(**kwargs):
    """
    The mean and standard deviation for each element of the dependent variable,
    which is a scalar in ridge and lasso.

    The output will be stored in a temp table: a mean array and a std array

    This function is also used in lasso.
    """
    plpy.execute("""
        drop table if exists {tbl_scale};
        create temp table {tbl_scale} as
            select
                avg({col_dep_var}) as dep_avg,
                1 as dep_std
            from {tbl_data}
    """.format(**kwargs))
    return None

## ========================================================================
    
def __ridge_normalize_data(**kwargs):
    """
    Normalize the independent and dependent variables using the calculated mean's and std's
    in __ridge_ind_var_scales and __ridge_dep_var_scale.

    Compute the scaled variables by: scaled_value = (origin_value - mean) / std, and special
    care is needed if std is zero.
    
    The output is a table with scaled independent and dependent variables.

    This function is also used in lasso.

    Parameters:
    tbl_data -- original data
    col_ind_var -- independent variables column 
    dimension -- length of independent variable array
    col_dep_var -- dependent variable column
    tbl_ind_scales -- independent variables scales array
    tbl_dep_scale -- dependent variable scale 
    tbl_data_scaled -- scaled data result
    """
    # intermediate table name to avoid name conflicts 
    foos = __cv_unique_string()
    plpy.execute("drop table if exists {tbl_data_scaled}".format(**kwargs))
    plpy.execute("""
        create temp table {tbl_data_scaled} as
            select
                array_agg(val order by attr ) as {col_ind_var},
                max(dep_var) as {col_dep_var}
            from (
                select
                    ids,
                    {foos}.attr as attr,
                    (dep_var - dep_avg) / dep_std as dep_var,
                    (
                        case when std = 0 then
                            val - mean
                        else
                            (val - mean) / (case when std = 0 then 1 else std end)
                        end
                    ) as val
                from
                    {tbl_ind_scales},
                    {tbl_dep_scale},
                    (
                        select
                            ids,
                            {col_dep_var} as dep_var,
                            generate_series(1, {dimension}) as attr,
                            unnest({col_ind_var}) as val
                        from (
                            select
                                row_number() over () as ids,
                                {col_dep_var},
                                {col_ind_var}
                            from {tbl_data}) t
                    ) as {foos}
                where {tbl_ind_scales}.attr = {foos}.attr
            ) as t
            group by ids
    """.format(foos = foos, **kwargs))
    return None

## ========================================================================
    
def __ridge_gather_results(**kwargs):
    """
    Put normalized fitting result together with scales to be used for nonlinear fitting.

    Non-linear fitting just uses normalized fitting coefficients to get prediction. Restore the coefficients
    to the original scale involves non-linear calculation and is usually not necessary.

    Parameters:
    tbl_coef - table contains the rsult of fitting
    col_coef - column name for the fitting coefficients
    col_others - column names other than the one for the fitting coefficients
    tbl_ind_scales - independent variable scales
    tbl_dep_scale - dependent variable scale
    """
    col_others_string = __cv_produce_col_name_string(kwargs["tbl_coef"], kwargs["col_others"])
    plpy.execute("drop table if exists {tbl_all_results}".format(**kwargs))
    plpy.execute("""
        create table {tbl_all_results} as
            select
                coefficients as {col_coef},
                intercept as intercept,
                array_agg({tbl_ind_scales}.mean order by {tbl_ind_scales}.attr) as ind_var_mean,
                {tbl_ind_scales}.std as ind_var_std,
                {col_others_string}
            from
                {tbl_coef},
                {tbl_ind_scales}
    """.format(col_others_string = col_others_string, **kwargs))
    return None

## ========================================================================
    
def __ridge_restore_linear_coef_scales_sql(**kwargs):
    """
    The query string for restoring the fitting coefficients using the calculated mean's and std's
    in __ridge_ind_var_scales and __ridge_dep_var_scale.

    Only for linear fitting.
    """
    subq2 = __cv_unique_string()
    query = """
        select
            subq1.coefficients as {col_coef},
            subq1.intercept + subq1.dep_avg as intercept
        from (
            select
                array_agg(val order by attr) as coefficients,
                sum(intercept_tmp) as intercept,
                max(dep_avg) as dep_avg
            from (
                select
                    {subq2}.attr,
                    (
                        case when std = 0 then
                            0.
                        else
                            coef * dep_std / (case when std = 0 then 1 else std end)
                        end
                    ) as val,
                    (
                        case when std = 0 then
                            0.
                        else
                            - coef * dep_std * mean / std
                        end
                    ) as intercept_tmp,
                    dep_avg
                from
                    {tbl_ind_scales},
                    {tbl_dep_scale},
                    (
                        select
                            generate_series(1, {dimension}) as attr,
                            unnest({col_coef}) as coef
                        from {tbl_coef}
                    ) as {subq2}
                where {tbl_ind_scales}.attr = {subq2}.attr
            ) as subq3
        ) as subq1
    """.format(subq2 = subq2, **kwargs)
    return query

## ========================================================================
    
def __ridge_restore_linear_coef_scales(**kwargs):
    """
    Restore the linear coefficients to the original scale.

    This function is also used in lasso.
    """
    col_others_string = __cv_produce_col_name_string(kwargs["tbl_coef"], kwargs["col_others"])
    foos = __cv_unique_string()
    goos = __cv_unique_string()
    sql_query = __ridge_restore_linear_coef_scales_sql(**kwargs)
    plpy.execute("drop table if exists {tbl_origin_coef}".format(**kwargs))
    plpy.execute("""
        create table {tbl_origin_coef} as
            select
                {foos}.{col_coef},
                {foos}.intercept,
                {goos}.ind_var_mean,
                {goos}.ind_var_std,
                {tbl_dep_scale}.dep_avg AS dep_var_mean,
                {tbl_dep_scale}.dep_std AS dep_var_std,
                {col_others_string}
            from
                ({sql_query}) as {foos},
                {tbl_coef},
                (
                    SELECT
                        array_agg({tbl_ind_scales}.mean ORDER BY {tbl_ind_scales}.attr) AS ind_var_mean,
                        array_agg({tbl_ind_scales}.std ORDER BY {tbl_ind_scales}.attr) AS ind_var_std
                    FROM {tbl_ind_scales}
                ) {goos},
                {tbl_dep_scale}
    """.format(col_others_string = col_others_string,
               foos = foos, goos = goos,
               sql_query = sql_query, **kwargs))
    return None

## ========================================================================
    
def __ridge_newton_cv_preprocess(kwargs):
    """
    Some common processes used in both ridge and lasso cross validation functions:

    copy data if needed,
    update name space,
    check the validity of fold_num,
    create table to store all the fitting coefficients
    """
    # table containing the data, which will be
    # split into training and validation parts
    data_tbl = kwargs["data_tbl"]
    # whether the user provides a unique ID for each row
    # if not, then it is None
    data_id = kwargs["data_id"]
    # whether the ID provided by user is random
    id_is_random = kwargs["id_is_random"]
    col_ind_var = kwargs["col_ind_var"]
    col_dep_var = kwargs["col_dep_var"]
    # how many fold validation, default: 10
    fold_num = kwargs["fold_num"]
    # how many fold actually will be used, default: 10.
    # If 1, it is just one validation.
    upto_fold = kwargs["upto_fold"]
    # if need to copy the data,
    # this is the copied table name
    tbl_all_data = __cv_unique_string()
    # table name before normalization
    tbl_inter = __cv_unique_string()
    # table name for training
    tbl_train = __cv_unique_string()
    # table name for validation
    tbl_valid = __cv_unique_string()
    # column name for random id
    col_random_id = __cv_unique_string()
    # table for random ID mapping, used when
    # data_id is not None and id_is_random is False
    tbl_random_id = __cv_unique_string()
    # accumulate the error information
    tbl_accum_error = __cv_unique_string()
    # independent variable (array) scales
    # inclduing mean's and std's
    tbl_ind_scales = __cv_unique_string()
    # dependent variable scale including mean and std
    tbl_dep_scale = __cv_unique_string()
    # table to store fitting coefficients from
    # all validations for all parameter values
    tbl_coef = __cv_unique_string() 
    kwargs.update(dict(tbl_accum_error = tbl_accum_error,
                       tbl_all_data = tbl_all_data,
                       tbl_inter = tbl_inter,
                       tbl_train = tbl_train,
                       tbl_valid = tbl_valid,
                       tbl_random_id = tbl_random_id,
                       col_random_id = col_random_id,
                       tbl_ind_scales = tbl_ind_scales,
                       tbl_dep_scale = tbl_dep_scale,
                       tbl_coef = tbl_coef))
    
    data_cols = [col_ind_var, col_dep_var]
    if data_id is None:
        # unique ID column is not given, has to copy the data and create the ID
        __cv_copy_data_with_id(data_tbl, data_cols, tbl_all_data, col_random_id)
        tbl_used = tbl_all_data
    elif id_is_random:
        # unique ID column is given and is random
        tbl_used = data_tbl
        col_random_id = data_id
        kwags["col_random_id"] = data_id
    else:
        # the provided unique ID is not random, create
        # a table mapping the given ID to a random ID
        __cv_generate_random_id(data_tbl, data_id, tbl_random_id, col_random_id)
        tbl_used = data_tbl

    # original data row number
    row_num = plpy.execute("select count(*) as row_num from {data_tbl}".format(**kwargs))[0]["row_num"]
    # length of the independent variable array
    dimension = plpy.execute("select max(array_upper({col_ind_var},1)) as dimension from {data_tbl}".format(**kwargs))[0]["dimension"]

    kwargs.update(dict(tbl_used = tbl_used, row_num = row_num, dimension = dimension))

    # table to append all fitting results
    # which are distinguished by id
    plpy.execute("""
        drop table if exists {tbl_coef};
        create temp table {tbl_coef} (id integer, coef double precision[], intercept double precision)
    """.format(**kwargs))
    return None

## ========================================================================
    
def __ridge_newton_cv_split_and_normalization(k, kwargs):
    """
    Another set of common processes needed by both ridge and lasso cross validation functions:

    split the original data into training and validation parts,
    normalize training data
    """
    data_id = kwargs["data_id"]
    id_is_random = kwargs["id_is_random"]
    tbl_used = kwargs["tbl_used"]
    col_random_id = kwargs["col_random_id"]
    row_num = kwargs["row_num"]
    tbl_train = kwargs["tbl_train"]
    tbl_valid = kwargs["tbl_valid"]
    fold_num = kwargs["fold_num"]
    tbl_random_id = kwargs["tbl_random_id"]
    tbl_inter = kwargs["tbl_inter"]
    normalization = kwargs["normalization"]
    dimension = kwargs["dimension"]
    tbl_ind_scales = kwargs["tbl_ind_scales"]
    tbl_dep_scale = kwargs["tbl_dep_scale"]
    col_ind_var = kwargs["col_ind_var"]
    col_dep_var = kwargs["col_dep_var"]

    col_data = [col_ind_var, col_dep_var]
    if (data_id is None) or (data_id is not None and id_is_random):
        __cv_split_data_using_id_col(tbl_used, col_data, col_random_id, row_num, tbl_inter, tbl_valid, fold_num, k+1)
    else:
        __cv_split_data_using_id_tbl(tbl_used, col_data, tbl_random_id, col_random_id, data_id, row_num, tbl_inter, tbl_valid, fold_num, k+1)

    row_num_train = plpy.execute("select count(*) as n from " + tbl_inter)[0]["n"]
    kwargs["row_num_train"] = row_num_train
    
    if normalization:
        __ridge_ind_var_scales(tbl_data = tbl_inter, col_ind_var = col_ind_var, row_num = row_num_train, dimension = dimension, tbl_scales = tbl_ind_scales)
        __ridge_dep_var_scale(tbl_data = tbl_inter, col_dep_var = col_dep_var, row_num = row_num_train, tbl_scale = tbl_dep_scale)
        __ridge_normalize_data(tbl_data = tbl_inter, col_ind_var = col_ind_var, dimension = dimension, col_dep_var = col_dep_var, tbl_ind_scales = tbl_ind_scales, tbl_dep_scale = tbl_dep_scale, tbl_data_scaled = tbl_train)
        # dep_std = plpy.execute("select dep_std as std from " + tbl_dep_scale)[0]["std"]
        kwargs["dep_std"] = 1
    else:
        kwargs["tbl_train"] = tbl_inter

    return None

## ========================================================================
    
def __ridge_accumulate_error(accum_count, tbl_accum_error, param_value, error):
    """
    Function needed by both ridge and lasso cross validation functions:

    accumulate measured errors from each validation for each lambda value.
    """
    if accum_count == 1:
        plpy.execute("create temp table {tbl_accum_error} (lambda double precision, mean_squared_error double precision)".format(tbl_accum_error = tbl_accum_error))
    plpy.execute("insert into {tbl_accum_error} values ({param_value}, {error})".format(
        tbl_accum_error = tbl_accum_error,
        param_value = param_value, error = error))

## ========================================================================
    
def __ridge_normalization_cv_restore(coef_count, **kwargs):
    """
    Function used by both ridge and lasso cross validation functions:
    
    Restore the fitting coefficients to the original scales without using
    extra table to avoid transaction lock limits.
    """
    sql_query = __ridge_restore_linear_coef_scales_sql(
        col_coef = "coef",
        dimension = kwargs["dimension"],
        tbl_ind_scales = kwargs["tbl_ind_scales"],
        tbl_dep_scale = kwargs["tbl_dep_scale"],
        tbl_coef = "{tbl_coef} where id = {coef_count}-1".format(
            coef_count = coef_count,
            **kwargs)
    )                
    plpy.execute("""
        insert into {tbl_coef}
            select
                {coef_count},
                coef,
                intercept
            from (
                {sql_query}
            ) t
    """.format(coef_count = coef_count, sql_query = sql_query,
               **kwargs))

## ========================================================================

def __ridge_newton_cv(**kwargs):
    __ridge_newton_cv_validate_args(kwargs)
    return __ridge_newton_cv_compute(**kwargs)

## ========================================================================

def __ridge_newton_cv_validate_args(args):
    """
    Validate the arguments to __ridge_newton_cv
    """
    if (args["data_tbl"] is None or args["validation_result"] is None
        or args["col_ind_var"] is None or args["col_dep_var"] is None
        or args["lambda_values"] is None or args["fold_num"] is None
        or args["upto_fold"] is None):
        plpy.error("Ridge CV error: You have unsupported Null value(s) in the arguments!")
    
    if not __is_tbl_exists(args["data_tbl"]):
        plpy.error("Ridge CV error: Data table does not exist!")

    if __is_tbl_exists_in_schema(args["validation_result"]):
        plpy.error("Ridge CV error: Output table already exists!")
        
    if not __is_tbl_has_rows(args["data_tbl"]):
        plpy.error("Ridge CV error: Data table is empty!")

    if not __is_col_exists(args["data_tbl"], [args["col_ind_var"], args["col_dep_var"]]):
        plpy.error("Ridge CV error: Some column does not exist!")

    if not __is_scalar_col_no_null(args["data_tbl"], args["col_dep_var"]):
        plpy.error("Ridge CV error: Dependent variable has Null values! Please filter out Null values before using this function!")

    if not __is_array_col_same_dimension(args["data_tbl"], args["col_ind_var"]):
        plpy.error("Ridge CV error: Independent variable arrays have unequal lengths!")

    if not __is_array_col_no_null(args["data_tbl"], args["col_ind_var"]):
        plpy.error("Ridge CV error: Independent variable arrays have Null values! Please filter out Null values before using this function!")

    for lambda_value in args["lambda_values"]:
        if lambda_value < 0:
            plpy.error("Ridge CV error: The regulation parameter lambda cannot be negative!")

    fold_num = args["fold_num"]
    upto_fold = args["upto_fold"]
    if fold_num <= 1:
        plpy.error("Ridge CV error: Cross validation total fold number should be larger than 1!")

    if upto_fold < 1 or upto_fold > fold_num:
        plpy.error("Ridge CV error: Cannot run with cross validation fold smalled than 1 or larger than total fold number!")

    row_num = plpy.execute("select count(*) from " + args["data_tbl"])[0]["count"]
    if row_num <= fold_num:
        plpy.error("Ridge CV error: Too few data! Fewer than fold_num.")

    return None
    
def __ridge_newton_cv_compute(**kwargs):
    """
    Cross validation for ridge without lock limits.

    The output is a table which has 3 columns: lamda value, MSE error average,
    and MSE error standard deviation.

    Parameters:
    schema_madlib -- Name of the MADlib schema, properly escaped/quoted
    data_tbl -- Name of data source table
    col_ind_var -- Name of indepdendent variable (an array) column
    col_dep_var -- Name of dependent variable (double) column
    lambda_values -- An array of values for lambda (the regulation parameter)
    data_id -- Whether the data table has a unique ID associated with each row. If data_id exists, then
               it would be be much easier to split the data into training and validation portions. Otherwise,
               CV function will copy the original data (only copy the part used in the calculation) and add
               a random ID for each row.
    id_is_random -- If data_id is not None, then whether this ID is randomly assigned to each row. If it is
                    not, then a mapping table will be created, mapping the original non-random ID to a
                    random ID.
    validation_result -- The table name to store the result of CV. It has 3 columns: lamda value, MSE error
                         average, and MSE error standard deviation.
    fold_num -- The cross validation fold number.
    upto_fold -- Which fold number the function will run up to
    """
    old_msg_level = plpy.execute("select setting from pg_settings where name='client_min_messages'")[0]['setting']
    plpy.execute("set client_min_messages to warning")

    # copy data if needed,
    # update name space,
    # check the validity of fold_num,
    # create table to store all the fitting coefficients
    __ridge_newton_cv_preprocess(kwargs)

    upto_fold = kwargs["upto_fold"]
    lambda_values = kwargs["lambda_values"]
    normalization = kwargs["normalization"]
    tbl_accum_error = kwargs["tbl_accum_error"]
    validation_result = kwargs["validation_result"]

    accum_count = 0
    coef_count = 0
    for k in range(upto_fold):
        # create training and validation sets
        # and normalize the training set if requested by
        # the user (parameter normalization in func_args is True)
        __ridge_newton_cv_split_and_normalization(k, kwargs)
        row_num_train = kwargs["row_num_train"]
        
        for value in lambda_values:
            coef_count += 1

            # Re-scaling of lambda
            # so that same results can be produced 
            # as R MASS package's lm.ridge function.
            # glmnet package produces different result,
            # which is due to a bug (but the bug actually does not
            # affect the final optimum fitting result)
            effective_lambda = value * row_num_train
            
            # actually compute the fitting coefficients
            # manually include the intercept as the last fitting parameter
            plpy.execute("""
                insert into {tbl_coef}
                    select {coef_count}, result[1:{dimension}], result[{dimension}+1]
                    from (
                        select {schema_madlib}.__ridge_newton_result(
                            {schema_madlib}.__ridge_newton_step(
                                array_append(({tbl_train}.{col_ind_var})::float8[], 1::float8),
                                ({tbl_train}.{col_dep_var})::float8,
                                NULL::float8[],
                                ({dimension}+1)::int2,
                                ({effective_lambda})::float8
                            )
                        ) as result
                        from {tbl_train}
                    ) t
            """.format(coef_count = coef_count,
                       effective_lambda = effective_lambda,
                       **kwargs))
       
            if normalization:
                # restore the fitting coefficients to the original scale
                # without introducing new tables
                coef_count += 1
                __ridge_normalization_cv_restore(coef_count, **kwargs)

            # directly compute error
            error = plpy.execute("""
                select avg((real_value - pred)^2) as error
                from (
                    select
                        {schema_madlib}.ridge_linear_newton_predict(coef, intercept, {tbl_valid}.{col_ind_var}) as pred,
                        {tbl_valid}.{col_dep_var} as real_value
                    from {tbl_valid}, {tbl_coef}
                    where {tbl_coef}.id = {coef_count}
                ) t
            """.format(coef_count = coef_count, **kwargs))[0]["error"]

            # append error into one table
            accum_count += 1
            __ridge_accumulate_error(accum_count, tbl_accum_error, value, error)

    # for each lambda value, compute the average and standard deviation of errors
    __cv_summarize_result(tbl_accum_error, validation_result, "lambda")

    # clean up the temporary tables
    plpy.execute("""
        drop table if exists {tbl_all_data};
        drop table if exists {tbl_train};
        drop table if exists {tbl_inter};
        drop table if exists {tbl_valid};
        drop table if exists {tbl_random_id};
        drop table if exists {tbl_coef};
        drop table if exists {tbl_ind_scales};
        drop table if exists {tbl_dep_scale};
        drop table if exists {tbl_accum_error}
    """.format(**kwargs))

    plpy.execute("set client_min_messages to " + old_msg_level)

    return None

## ========================================================================

def __ridge_newton_cv_help():
    return """
    Allowed modelling function arguments:

    * tbl_source - the data source table,
    * col_ind_var - independent variable column name,
    * col_dep_var - dependent variable column name,
    * tbl_output - output table of ridge (as an intermediate table of CV, it will be ignored if is given here)
    * lambda - the regulation parameter (in CV lambda is given as the parameter values to try, and any value given here will be ignored)
    * normalization - whether normalize the data (better convergence, consistent with R default and slower speed)

    Usage example:

    select madlib.cross_validation(
        'ridge',
        '{normalization = True, col_ind_var = val, col_dep_var = dep, tbl_source = cvtest}',
        'lambda',
        '{0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2}'::double precision[],
        NULL::varchar, -- no unique ID associated with rows
        False,  -- if there is a unique ID, whether it is random
        'validation_result',
        10    -- cross validation fold
    );
    """

## ========================================================================

def ridge_newton_train (schema_madlib, tbl_source, col_ind_var,
                        col_dep_var, tbl_output, lambda_value,
                        normalization, **kwargs):
    (tbl_source, col_ind_var,
     col_dep_var, tbl_output,
     lambda_value, normalization) = __ridge_validate_args(tbl_source, col_ind_var,
                                                          col_dep_var, tbl_output,
                                                          lambda_value, normalization)
    
    return ridge_newton_train_compute (schema_madlib, tbl_source, col_ind_var,
                                col_dep_var, tbl_output, lambda_value,
                                normalization, **kwargs)

## ========================================================================

def __ridge_validate_args(tbl_source, col_ind_var,
                          col_dep_var, tbl_output,
                          lambda_value, normalization):
    if (tbl_source is None or col_ind_var is None or col_dep_var is None
        or tbl_output is None or lambda_value is None or normalization is None):
        plpy.error("Ridge error: You have unsupported Null value(s) in arguments!")
    
    if not __is_tbl_exists(tbl_source):
        plpy.error("Ridge error: Data table " + tbl_source + " does not exist!")

    if __is_tbl_exists_in_schema(tbl_output):
        plpy.error("Ridge error: Output table " + tbl_output + " already exists!")

    if not __is_tbl_has_rows(tbl_source):
        plpy.error("Ridge error: Data table " + tbl_source + " is empty!")

    if not __is_col_exists(tbl_source, [col_ind_var, col_dep_var]):
        plpy.error("Ridge error: Some column does not exist!")

    if not __is_scalar_col_no_null(tbl_source, col_dep_var):
        plpy.error("Ridge error: Dependent variable has Null values! Please filter out Null values before using this function!")

    if not __is_array_col_same_dimension(tbl_source, col_ind_var):
        plpy.error("Ridge error: Independent variable arrays have unequal lengths!")

    if not __is_array_col_no_null(tbl_source, col_ind_var):
        plpy.error("Ridge error: Independent variable arrays have Null values! Please filter out Null values before using this function!")

    if lambda_value < 0:
        plpy.error("Ridge error: The regulation parameter lambda cannot be negative!")

    return (tbl_source, col_ind_var,
            col_dep_var, tbl_output,
            lambda_value, normalization)
                   
    
def ridge_newton_train_compute (schema_madlib, tbl_source, col_ind_var,
                                col_dep_var, tbl_output, lambda_value,
                                normalization, **kwargs):
    """
    @param tbl_source -- Name of data source table
    @param col_ind_var -- Name of indepdendent variable (an array) column
    @param col_dep_var -- Name of dependent variable (double) column
    @param tbl_output -- Name of tabel to store results
    @param lambda_value -- Value of teh regulation parameter
    @param normalization -- Whether to normalize the dependent variables
    """
    old_msg_level = plpy.execute("select setting from pg_settings where name='client_min_messages'")[0]['setting']
    plpy.execute("set client_min_messages to warning")

    # __validation_arguments()

    args = dict(schema_madlib = schema_madlib, tbl_source = tbl_source,
                col_ind_var = col_ind_var, col_dep_var = col_dep_var,
                tbl_output = tbl_output, lambda_value = lambda_value,
                tbl_ind_scales = __cv_unique_string(),
                tbl_dep_scale = __cv_unique_string(),
                tbl_data_scaled = __cv_unique_string())
    
    row_num = plpy.execute("select count(*) from " + tbl_source)[0]["count"]
    dimension = plpy.execute("""
                             select max(array_upper({col_ind_var},1)) as d
                             from {tbl_source}
                             """.format(**args))[0]["d"]
    args.update(dimension = dimension, row_num = row_num)
    
    if normalization:
        __ridge_ind_var_scales(tbl_data = tbl_source, col_ind_var = col_ind_var, row_num = row_num, dimension = dimension, tbl_scales = args["tbl_ind_scales"])
        __ridge_dep_var_scale(tbl_data = tbl_source, col_dep_var = col_dep_var, row_num = row_num, tbl_scale = args["tbl_dep_scale"])
        __ridge_normalize_data(tbl_data = tbl_source, col_ind_var = col_ind_var, dimension = dimension, col_dep_var = col_dep_var, tbl_ind_scales = args["tbl_ind_scales"], tbl_dep_scale = args["tbl_dep_scale"], tbl_data_scaled = args["tbl_data_scaled"])
        tbl_inter = args["tbl_data_scaled"]
        use_temp = "temp"
        tbl_inter_result = __cv_unique_string()
    else:
        tbl_inter = tbl_source
        tbl_inter_result = tbl_output
        use_temp = ""

    args.update(tbl_inter = tbl_inter, tbl_inter_result = tbl_inter_result,
                use_temp = use_temp)

    plpy.execute("""
                 drop table if exists {tbl_inter_result};
                 create {use_temp} table {tbl_inter_result} (
                    coefficients    double precision[],
                    intercept       double precision,
                    log_likelihood  double precision,
                    normalization   boolean)
                 """.format(**args))

    plpy.execute("""
                 insert into {tbl_inter_result}
                 select result[1:{dimension}], result[{dimension}+1], 0, False
                 from (
                    select {schema_madlib}.__ridge_newton_result(
                        {schema_madlib}.__ridge_newton_step(
                            array_append(({tbl_inter}.{col_ind_var})::double precision[],1::double precision),
                            ({tbl_inter}.{col_dep_var})::double precision,
                            (NULL)::double precision[],
                            ({dimension}+1)::int2,
                            {effective_lambda}::double precision)
                       ) as result
                    from {tbl_inter}
                 ) t                 
                 """.format(effective_lambda = lambda_value * row_num,
                            **args))

    # update the log likelihood
    plpy.execute("""
                 update {tbl_inter_result} set log_likelihood = ll from
                 (
                    select -(loss + module_2) as ll
                    from
                        (
                           select
                               avg(({col_dep_var} - {schema_madlib}.ridge_linear_newton_predict(coefficients, intercept, {col_ind_var}))^2) / 2. as loss
                           from
                               {tbl_inter_result},
                               {tbl_inter}
                        ) s,
                        (
                           select {lambda_value} * sum(coef^2) *0.5 as module_2
                           from (
                              select unnest(coefficients) as coef
                              from {tbl_inter_result}
                           ) t1                 
                         ) t
                 ) t2
                 """.format(**args))

    if normalization:
        __ridge_restore_linear_coef_scales(tbl_coef = tbl_inter_result,
                                           col_coef = 'coefficients',
                                           col_others = ["log_likelihood", "normalization"],
                                           dimension = dimension,
                                           tbl_ind_scales = args["tbl_ind_scales"],
                                           tbl_dep_scale = args["tbl_dep_scale"],
                                           tbl_origin_coef = tbl_output)
        plpy.execute("update {tbl_output} set normalization = True".format(**args))
        plpy.execute("drop table if exists {tbl_inter_result}".format(**args))

    plpy.execute("""
                 drop table if exists {tbl_ind_scales};
                 drop table if exists {tbl_dep_scale};
                 drop table if exists {tbl_data_scaled};
                 """.format(**args))
        
    plpy.execute("set client_min_messages to " + old_msg_level)
    return None
