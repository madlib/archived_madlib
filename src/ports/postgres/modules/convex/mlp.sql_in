/* ----------------------------------------------------------------------- *//**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 *
 * @file mlp.sql_in
 *
 * @brief SQL functions for multilayer perceptron
 * @date June 2012
 *
 *
 *//* ----------------------------------------------------------------------- */

m4_include(`SQLCommon.m4')

/**
@addtogroup grp_nn

<div class="toc"><b>Contents</b><ul>
<li class="level1"><a href="#mlp_classification">Classification</a></li>
<li class="level1"><a href="#mlp_regression">Regression</a></li>
<li class="level1"><a href="#optimizer_params">Optimizer Parameters</a></li>
<li class="level1"><a href="#predict">Prediction Functions</a></li>
<li class="level1"><a href="#example">Examples</a></li>
<li class="level1"><a href="#background">Technical Background</a></li>
<li class="level1"><a href="#literature">Literature</a></li>
<li class="level1"><a href="#related">Related Topics</a></li>
</ul></div>

Multilayer Perceptron (MLP) is a type of neural network that can be
used for regression and classification.

Also called "vanilla neural networks", MLPs consist of several
fully connected hidden layers with non-linear activation
functions.  In the case of classification, the final layer of the
neural net has as many nodes as classes, and the output of the
neural net can be interpreted as the probability of a given input
feature belonging to a specific class.


@brief Solves classification and regression problems with several
fully connected layers and nonlinear activation functions.

@anchor mlp_classification
@par Classification Training Function
The mlp classification training function has the following format:
<pre class="syntax">
mlp_classification(
    source_table,
    output_table,
    independent_varname,
    dependent_varname,
    hidden_layer_sizes,
    optimizer_params,
    activation,
    weights
    )
</pre>
\b Arguments
<DL class="arglist">
  <DT>source_table</DT>
  <DD>TEXT. Name of the table containing the training data.</DD>


  <DT>output_table</DT>
  <DD>TEXT. Name of the output table containing the model. Details of the output
   tables are provided below.
  </DD>

  <DT>independent_varname</DT>
  <DD>TEXT. Expression list to evaluate for the
    independent variables. An intercept variable should not be included as part
    of this expression. <b>Please note that expression should be encoded properly.</b>
    All values are cast to DOUBLE PRECISION, so categorical variables should be
    one-hot or dummy encoded.  See <a href="group__grp__encode__categorical.html">here</a>
    for more details.
  </DD>


  <DT>dependent_varname</DT>
  <DD> TEXT. Name of the dependent variable column. For classification, supported types are:
  text, varchar, character varying, char, character
  integer, smallint, bigint, and boolean.  </DD>

  <DT>hidden_layer_sizes </DT>
  <DD>INTEGER[]
  The number of neurons in each hidden layer.  The length of this array will
  determine the number of hidden layers.  NULL for no hidden layers.
  </DD>


  <DT>optimizer_params (optional)</DT>
  <DD>TEXT, default: NULL.
    Parameters for optimization in a comma-separated string
    of key-value pairs. See the description below for details.
  </DD>

  <DT>activation (optional)</DT>
  <DD>TEXT, default: 'sigmoid'.
    Activation function. Currently three functions are supported: 'sigmoid' (default),
    'relu', and 'tanh'. The text can be any prefix of the three
    strings; for e.g., activation='s' will use the sigmoid activation.
  </DD>


  <DT>weights (optional)</DT>
  <DD>TEXT, default: NULL.
    Weights for input rows. Column name which specifies the weight for each input row.
    This weight will be incorporated into the update during SGD, and will not be used
    for loss calculations. If not specified, weight for each row will default to 1.
    Column should be a numeric type.
  </DD>

  <DT>warm_start (optional)</DT>
  <DD>BOOLEAN, default: FALSE.
    Initalize weights with the coefficients from the last call.  If true, weights will
    be initialized from output_table. Note that all parameters other than optimizer_params,
    and verbose must remain constant between calls to warm_start.
  </DD>

  <DT>verbose (optional)</DT>
  <DD>BOOLEAN, default: FALSE. Provides verbose output of the results of training.</DD>
</DL>

<b>Output tables</b>
<br>
    The model table produced by mlp contains the following columns:
    <table class="output">
      <tr>
        <th>coeffs</th>
        <td>FLOAT8[]. Flat array containing the weights of the neural net</td>
      </tr>
      <tr>
        <th>n_iterations</th>
        <td>INTEGER. Number of iterations completed by stochastic gradient descent
        algorithm. The algorithm either converged in this number of iterations
        or hit the maximum number specified in the optimization parameters. </td>
      </tr>
      <tr>
        <th>loss</th>
        <td>FLOAT8. The cross entropy over the training data.
        See Technical Background section below for more details.</td>
      </tr>
    </table>


A summary table named \<output_table\>_summary is also created, which has the following columns:
    <table class="output">
    <tr>
        <th>source_table</th>
        <td>The source table.</td>
    </tr>
    <tr>
        <th>independent_varname</th>
        <td>The independent variables.</td>
    </tr>
    <tr>
        <th>dependent_varname</th>
        <td>The dependent variable.</td>
    </tr>
    <tr>
        <th>tolerance</th>
        <td>The tolerance as given in optimizer_params.</td>
    </tr>
    <tr>
        <th>learning_rate_init</th>
        <td>The initial learning rate as given in optimizer_params.</td>
    </tr>
    <tr>
        <th>learning_rate_policy</th>
        <td>The learning rate policy as given in optimizer_params.</td>
    </tr>
    <tr>
        <th>n_iterations</th>
        <td>The number of iterations run.</td>
    </tr>
    <tr>
        <th>n_tries</th>
        <td>The number of tries as given in optimizer_params.</td>
    </tr>
    <tr>
        <th>layer_sizes</th>
        <td>The number of units in each layer including the input and output layer.</td>
    </tr>
    <tr>
        <th>activation</th>
        <td>The activation function.</td>
    </tr>
    <tr>
        <th>is_classification</th>
        <td>True if the model was trained for classification, False if it was trained
        for regression.</td>
    </tr>
    <tr>
        <th>classes</th>
        <td>The classes which were trained against (empty for regression).</td>
    </tr>
    <tr>
        <th>weights</th>
        <td>The weight column used during training.</td>
    </tr>
    <tr>
        <th>x_means</th>
        <td>The mean for all input features (used for normalization).</td>
    </tr>
    <tr>
        <th>x_stds</th>
        <td>The standard deviation for all input features (used for normalization).</td>
    </tr>

   </table>


@anchor mlp_regression
@par Regression Training Function
The mlp regression training function has the following format:
<pre class="syntax">
mlp_regression(source_table,
    source_table,
    output_table,
    independent_varname,
    dependent_varname,
    hidden_layer_sizes,
    optimizer_params,
    activation,
    weights,
    verbose
    )
</pre>

\b Arguments

Specifications for regression are largely the same as for classification. In the
model table, the loss will refer to mean square error instead of cross entropy. In the
summary table, there is no classes column. The following
arguments have specifications which differ from mlp_classification:
<DL class="arglist">
<DT>dependent_varname</DT>
  <DD>TEXT. Name of the dependent variable column.
  For regression supported types are any numeric type, or array
  or numeric types (for multiple regression).
  </DD>
</DL>


@anchor optimizer_params
@par Optimizer Parameters
Parameters in this section are supplied in the \e optimizer_params argument as a string
containing a comma-delimited list of name-value pairs. All of these named
parameters are optional, and their order does not matter. You must use the
format "<param_name> = <value>" to specify the value of a parameter, otherwise
the parameter is ignored.


<pre class="syntax">
  'learning_rate_init = &lt;value>,
   n_iterations = &lt;value>,
   n_tries = &lt;value>,
   tolerance = &lt;value>'
</pre>
\b Optimizer Parameters
<DL class="arglist">

<DT>learning_rate_init</dt>
<DD>Default: 0.001.
Also known as the learning rate. A small value is usually desirable to
ensure convergence, while a large value provides more room for progress during
training. Since the best value depends on the condition number of the data, in
practice one often tunes this parameter.
</DD>

<DT>learning_rate_policy</dt>
<DD>Default: constant.
One of 'constant', 'exp', 'inv' or 'step' or any prefix of these.
'constant': learning_rate = learning_rate_init
'exp': learning_rate = learning_rate_init * gamma^(iter)
'inv': learning_rate = learning_rate_init * (iter+1)^(-power)
'step': learning_rate = learning_rate_init * gamma^(floor(iter/iterations_per_step))
Where iter is the current iteration of SGD.
</DD>

<DT>gamma</dt>
<DD>Default: 0.1.
Decay rate for learning rate when learning_rate_policy is 'exp' or 'step'.
</DD>

<DT>power</dt>
<DD>Default: 0.5.
Exponent for learning_rate_policy = 'inv'.
</DD>

<DT>iterations_per_step</dt>
<DD>Default: 100.
Number of iterations to run before decreasing the learning rate by
a factor of gamma.  Valid for learning rate policy = 'step'.
</DD>

<DT>n_iterations</dt>
<DD>Default: [100]. The maximum number of iterations allowed.
</DD>

<DT>n_tries</dt>
<DD>Default: [1]. Number of times to retrain the network with randomly initialized
weights.
</DD>

<DT>lambda</dt>
<DD>Default: 0. The regularization coefficient for L2 regularization.
</DD>

<DT>tolerance</dt>
<DD>Default: 0.001. The criterion to end iterations. The training stops whenever
the difference between the training models of two consecutive iterations is
smaller than \e tolerance or the iteration number is larger than \e max_iter.
</DD>

</DL>

@anchor predict
@par Prediction Function
Used to generate predictions given a previously trained model on novel data.
The same syntax is used for classification, and regression.
<pre class="syntax">
mlp_predict(model_table,
            data_table,
            id_col_name,
            output_table,
            pred_type)
</pre>

\b Arguments
<DL class="arglist">
  <DT>model_table</DT>
  <DD>TEXT. Model table produced by the training function.</DD>

  <DT>data_table</DT>
  <DD>TEXT. Name of the table containing the data for prediction. This table is expected
  to contain the same input features that were used during training. The table should
  also contain id_col_name used for identifying each row.</DD>

  <DT>id_col_name</DT>
  <DD>TEXT. The name of the id column in the input table.</DD>

  <DT>output_table</DT>
  <DD>TEXT. Name of the table where output predictions are written. If this
table name is already in use, then an error is returned.  Table contains:</DD>
    <table class="output">
      <tr>
        <th>id</th>
        <td>Gives the 'id' for each prediction, corresponding to each row from the data_table.</td>
      </tr>
      <tr>
        <th>estimated_COL_NAME</th>
        <td>
        (For pred_type='response') The estimated class
         for classification or value for regression, where
         COL_NAME is the name of the column to be
         predicted from training data.
        </td>
      </tr>
      <tr>
        <th>prob_CLASS</th>
        <td>
        (For pred_type='prob' for classification) The
        probability of a given class CLASS as given by
        softmax. There will be one column for each class
        in the training data.
        </td>
      </tr>


  <DT>pred_type</DT>
  <DD>TEXT.

The type of output requested:
'response' gives the actual prediction,
'prob' gives the probability of each class.
For regression, only type='response' is defined.
The name of the id column in the input table.</DD>
</DL>
</table>

@anchor example
@par Examples
-#  Create an input data set.
<pre class="example">
CREATE TABLE iris_data(
    id integer,
    attributes numeric[],
    class_text varchar,
    class integer
);
INSERT INTO iris_data VALUES
(1,ARRAY[5.1,3.5,1.4,0.2],'Iris-setosa',1),
(2,ARRAY[4.9,3.0,1.4,0.2],'Iris-setosa',1),
(3,ARRAY[4.7,3.2,1.3,0.2],'Iris-setosa',1),
(4,ARRAY[4.6,3.1,1.5,0.2],'Iris-setosa',1),
(5,ARRAY[5.0,3.6,1.4,0.2],'Iris-setosa',1),
(6,ARRAY[5.4,3.9,1.7,0.4],'Iris-setosa',1),
(7,ARRAY[4.6,3.4,1.4,0.3],'Iris-setosa',1),
(8,ARRAY[5.0,3.4,1.5,0.2],'Iris-setosa',1),
(9,ARRAY[4.4,2.9,1.4,0.2],'Iris-setosa',1),
(10,ARRAY[4.9,3.1,1.5,0.1],'Iris-setosa',1),
(11,ARRAY[7.0,3.2,4.7,1.4],'Iris-versicolor',2),
(12,ARRAY[6.4,3.2,4.5,1.5],'Iris-versicolor',2),
(13,ARRAY[6.9,3.1,4.9,1.5],'Iris-versicolor',2),
(14,ARRAY[5.5,2.3,4.0,1.3],'Iris-versicolor',2),
(15,ARRAY[6.5,2.8,4.6,1.5],'Iris-versicolor',2),
(16,ARRAY[5.7,2.8,4.5,1.3],'Iris-versicolor',2),
(17,ARRAY[6.3,3.3,4.7,1.6],'Iris-versicolor',2),
(18,ARRAY[4.9,2.4,3.3,1.0],'Iris-versicolor',2),
(19,ARRAY[6.6,2.9,4.6,1.3],'Iris-versicolor',2),
(20,ARRAY[5.2,2.7,3.9,1.4],'Iris-versicolor',2);
</pre>
-# Generate a multilayer perceptron with a single hidden layer of 5 units.
Use the attributes column as the independent variables, and use the class
column as the classification. Set the tolerance to 0 so that 5000
iterations will be run. Use a hyperbolic tangent activation function.
The model will be written to mlp_model.
<pre class="example">
DROP TABLE IF EXISTS mlp_model;
DROP TABLE IF EXISTS mlp_model_summary;
-- Set seed so results are reproducible
SELECT setseed(0);
SELECT madlib.mlp_classification(
    'iris_data',      -- Source table
    'mlp_model',      -- Destination table
    'attributes',     -- Input features
    'class_text',     -- Label
    ARRAY[5],         -- Number of units per layer
    'learning_rate_init=0.003,
    n_iterations=500,
    tolerance=0',     -- Optimizer params
    'tanh',           -- Activation function
    NULL,             -- Default weight (1)
    FALSE,            -- No warm start
    TRUE              -- Verbose
);
</pre>
-# View the result for the model.
<pre class="example">
-- Set extended display on for easier reading of output
\\x ON
-- Results may vary depending on platform
SELECT * FROM mlp_model;
</pre>
Result:
<pre class="result">
-[ RECORD 1 ]--+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
coeff          | {-0.172392477419,-0.0836446652758,-0.0162194484142,-0.647268294231,-0.504884325538,0.184825723596,0.351728174731,-0.601148967035,0.720999542651,0.26521898248,0.245760922013,0.264645322438,-0.349957739904,0.797653395667,0.725747963566,-0.344498001796,0.261481840947,0.329074383545,0.379503434339,-0.267398086353,-0.0238069072658,0.330239268187,-0.178736289201,-0.0563356339946,-0.0333791780453,0.262137386864,0.491390436498,-1.02635831573,-1.29541478382,0.246017274,-0.0623575215434,0.0826297373887,-0.671671189842,0.853494672576,1.21671423502,0.296424359217,0.15294606861}
loss           | 0.0136695756314
num_iterations | 500
</pre>
-# Next train a regression example.  First create some test data.  This dataset
contains housing prices data.
<pre class="example">
CREATE TABLE lin_housing (id serial, x float8[], grp_by_col int, y float8);
COPY lin_housing (x, grp_by_col, y) FROM STDIN NULL '?' DELIMITER '|';
{1,0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98}|1|24.00
{1,0.02731,0.00,7.070,0,0.4690,6.4210,78.90,4.9671,2,242.0,17.80,396.90,9.14}|1|21.60
{1,0.02729,0.00,7.070,0,0.4690,7.1850,61.10,4.9671,2,242.0,17.80,392.83,4.03}|1|34.70
{1,0.03237,0.00,2.180,0,0.4580,6.9980,45.80,6.0622,3,222.0,18.70,394.63,2.94}|1|33.40
{1,0.06905,0.00,2.180,0,0.4580,7.1470,54.20,6.0622,3,222.0,18.70,396.90,5.33}|1|36.20
{1,0.02985,0.00,2.180,0,0.4580,6.4300,58.70,6.0622,3,222.0,18.70,394.12,5.21}|1|28.70
{1,0.08829,12.50,7.870,0,0.5240,6.0120,66.60,5.5605,5,311.0,15.20,395.60,12.43}|1|22.90
{1,0.14455,12.50,7.870,0,0.5240,6.1720,96.10,5.9505,5,311.0,15.20,396.90,19.15}|1|27.10
{1,0.21124,12.50,7.870,0,0.5240,5.6310,100.00,6.0821,5,311.0,15.20,386.63,29.93}|1|16.50
{1,0.17004,12.50,7.870,0,0.5240,6.0040,85.90,6.5921,5,311.0,15.20,386.71,17.10}|1|18.90
{1,0.22489,12.50,7.870,0,0.5240,6.3770,94.30,6.3467,5,311.0,15.20,392.52,20.45}|1|15.00
{1,0.11747,12.50,7.870,0,0.5240,6.0090,82.90,6.2267,5,311.0,15.20,396.90,13.27}|1|18.90
{1,0.09378,12.50,7.870,0,0.5240,5.8890,39.00,5.4509,5,311.0,15.20,390.50,15.71}|1|21.70
{1,0.62976,0.00,8.140,0,0.5380,5.9490,61.80,4.7075,4,307.0,21.00,396.90,8.26}|1|20.40
{1,0.63796,0.00,8.140,0,0.5380,6.0960,84.50,4.4619,4,307.0,21.00,380.02,10.26}|1|18.20
{1,0.62739,0.00,8.140,0,0.5380,5.8340,56.50,4.4986,4,307.0,21.00,395.62,8.47}|1|19.90
{1,1.05393,0.00,8.140,0,0.5380,5.9350,29.30,4.4986,4,307.0,21.00,386.85,6.58}|1| 23.10
{1,0.78420,0.00,8.140,0,0.5380,5.9900,81.70,4.2579,4,307.0,21.00,386.75,14.67}|1|17.50
{1,0.80271,0.00,8.140,0,0.5380,5.4560,36.60,3.7965,4,307.0,21.00,288.99,11.69}|1|20.20
{1,0.72580,0.00,8.140,0,0.5380,5.7270,69.50,3.7965,4,307.0,21.00,390.95,11.28}|1|18.20
\\.
</pre>
-# Now train a regression model using a multilayer perceptron a single hidden layer of two nodes.
<pre class="example">
DROP TABLE IF EXISTS mlp_regress;
DROP TABLE IF EXISTS mlp_regress_summary;
SELECT setseed(0);
SELECT madlib.mlp_regression(
    'lin_housing',         -- Source table
    'mlp_regress',         -- Desination table
    'x',                   -- Input features
    'y',                   -- Dependent variable
    ARRAY[25,25],            -- Number of units per layer
    'learning_rate_init=0.001,
    n_iterations=500,
    lambda=0.001,
    tolerance=0',
    'relu',
    NULL,             -- Default weight (1)
    FALSE,            -- No warm start
    TRUE              -- Verbose
);
</pre>
-# Check the results of the model
<pre class="example">
-- Set extended display on for easier reading of output.
\\x ON
-- Results may vary depending on platform.
SELECT * FROM mlp_regress;
</pre>
Result:
<pre class="result">
[ RECORD 1 ]--+-----------------------------------------------------------------------------------
coeff          | {-0.135647108464,0.0315402969485,-0.117580589352,-0.23084537701,-0.10868726702...
loss           | 0.114125125042
num_iterations | 500
</pre>
-# Now let's look at the prediction functions. In the following examples we will
use the training data set for prediction as well, which is not usual but serves to
show the syntax. First we will test the classification example.
The prediction is in the the estimated_class_text column with the
actual value in the class_text column.
<pre class="example">
DROP TABLE IF EXISTS mlp_prediction;
SELECT madlib.mlp_predict(
         'mlp_model',         -- Model table
         'iris_data',         -- Test data table
         'id',                -- Id column in test table
         'mlp_prediction',    -- Output table for predictions
         'response'           -- Output classes, not probabilities
     );
SELECT * FROM mlp_prediction JOIN iris_data USING (id);
</pre>
Result for the classification model:
<pre class="result">
 id | estimated_class_text |    attributes     |   class_text    | class
----+----------------------+-------------------+-----------------+-------
  1 | Iris-setosa          | {5.1,3.5,1.4,0.2} | Iris-setosa     |     1
  2 | Iris-setosa          | {4.9,3.0,1.4,0.2} | Iris-setosa     |     1
  3 | Iris-setosa          | {4.7,3.2,1.3,0.2} | Iris-setosa     |     1
  4 | Iris-setosa          | {4.6,3.1,1.5,0.2} | Iris-setosa     |     1
  5 | Iris-setosa          | {5.0,3.6,1.4,0.2} | Iris-setosa     |     1
  6 | Iris-setosa          | {5.4,3.9,1.7,0.4} | Iris-setosa     |     1
  7 | Iris-setosa          | {4.6,3.4,1.4,0.3} | Iris-setosa     |     1
  8 | Iris-setosa          | {5.0,3.4,1.5,0.2} | Iris-setosa     |     1
  9 | Iris-setosa          | {4.4,2.9,1.4,0.2} | Iris-setosa     |     1
 10 | Iris-setosa          | {4.9,3.1,1.5,0.1} | Iris-setosa     |     1
 11 | Iris-versicolor      | {7.0,3.2,4.7,1.4} | Iris-versicolor |     2
 12 | Iris-versicolor      | {6.4,3.2,4.5,1.5} | Iris-versicolor |     2
 13 | Iris-versicolor      | {6.9,3.1,4.9,1.5} | Iris-versicolor |     2
 14 | Iris-versicolor      | {5.5,2.3,4.0,1.3} | Iris-versicolor |     2
 15 | Iris-versicolor      | {6.5,2.8,4.6,1.5} | Iris-versicolor |     2
 16 | Iris-versicolor      | {5.7,2.8,4.5,1.3} | Iris-versicolor |     2
 17 | Iris-versicolor      | {6.3,3.3,4.7,1.6} | Iris-versicolor |     2
 18 | Iris-versicolor      | {4.9,2.4,3.3,1.0} | Iris-versicolor |     2
 19 | Iris-versicolor      | {6.6,2.9,4.6,1.3} | Iris-versicolor |     2
 20 | Iris-versicolor      | {5.2,2.7,3.9,1.4} | Iris-versicolor |     2
</pre>
-# Prediction using the regression model:
<pre class="example">
DROP TABLE IF EXISTS mlp_regress_prediction;
SELECT madlib.mlp_predict(
         'mlp_regress',               -- Model table
         'lin_housing',               -- Test data table
         'id',                        -- Id column in test table
         'mlp_regress_prediction',    -- Output table for predictions
         'response'                   -- Output values, not probabilities
     );
</pre>
View results
<pre class="example">
SELECT * FROM lin_housing JOIN mlp_regress_prediction USING (id);
</pre>
Result for the regression model:
<pre class="result">
 id |                                    x                                    | grp_by_col |  y   |   estimated_y
----+-------------------------------------------------------------------------+------------+------+------------------
  1 | {1,0.00632,18,2.31,0,0.538,6.575,65.2,4.09,1,296,15.3,396.9,4.98}       |          1 |   24 |  23.973628645041
  2 | {1,0.02731,0,7.07,0,0.469,6.421,78.9,4.9671,2,242,17.8,396.9,9.14}      |          1 | 21.6 | 21.6389086856109
  3 | {1,0.02729,0,7.07,0,0.469,7.185,61.1,4.9671,2,242,17.8,392.83,4.03}     |          1 | 34.7 | 34.6766441639675
  4 | {1,0.03237,0,2.18,0,0.458,6.998,45.8,6.0622,3,222,18.7,394.63,2.94}     |          1 | 33.4 | 33.4521871118756
  5 | {1,0.06905,0,2.18,0,0.458,7.147,54.2,6.0622,3,222,18.7,396.9,5.33}      |          1 | 36.2 | 36.2899491706428
  6 | {1,0.02985,0,2.18,0,0.458,6.43,58.7,6.0622,3,222,18.7,394.12,5.21}      |          1 | 28.7 | 28.6994076427827
  7 | {1,0.08829,12.5,7.87,0,0.524,6.012,66.6,5.5605,5,311,15.2,395.6,12.43}  |          1 | 22.9 | 22.4882117113923
  8 | {1,0.14455,12.5,7.87,0,0.524,6.172,96.1,5.9505,5,311,15.2,396.9,19.15}  |          1 | 27.1 | 26.5148927040405
  9 | {1,0.21124,12.5,7.87,0,0.524,5.631,100,6.0821,5,311,15.2,386.63,29.93}  |          1 | 16.5 | 16.0669778867327
 10 | {1,0.17004,12.5,7.87,0,0.524,6.004,85.9,6.5921,5,311,15.2,386.71,17.1}  |          1 | 18.9 | 17.4237448788601
 11 | {1,0.22489,12.5,7.87,0,0.524,6.377,94.3,6.3467,5,311,15.2,392.52,20.45} |          1 |   15 | 14.5944028616784
 12 | {1,0.11747,12.5,7.87,0,0.524,6.009,82.9,6.2267,5,311,15.2,396.9,13.27}  |          1 | 18.9 | 19.6071061560237
 13 | {1,0.09378,12.5,7.87,0,0.524,5.889,39,5.4509,5,311,15.2,390.5,15.71}    |          1 | 21.7 | 21.7585638578804
 14 | {1,0.62976,0,8.14,0,0.538,5.949,61.8,4.7075,4,307,21,396.9,8.26}        |          1 | 20.4 | 20.2832271533629
 15 | {1,0.63796,0,8.14,0,0.538,6.096,84.5,4.4619,4,307,21,380.02,10.26}      |          1 | 18.2 | 18.3440540662206
 16 | {1,0.62739,0,8.14,0,0.538,5.834,56.5,4.4986,4,307,21,395.62,8.47}       |          1 | 19.9 | 20.0246074554594
 17 | {1,1.05393,0,8.14,0,0.538,5.935,29.3,4.4986,4,307,21,386.85,6.58}       |          1 | 23.1 | 23.1458505146148
 18 | {1,0.7842,0,8.14,0,0.538,5.99,81.7,4.2579,4,307,21,386.75,14.67}        |          1 | 17.5 | 17.4602306566804
 19 | {1,0.80271,0,8.14,0,0.538,5.456,36.6,3.7965,4,307,21,288.99,11.69}      |          1 | 20.2 | 20.1785296856357
 20 | {1,0.7258,0,8.14,0,0.538,5.727,69.5,3.7965,4,307,21,390.95,11.28}       |          1 | 18.2 | 18.1810300625137
(20 rows)
</pre>
Note that the results you get for all examples may vary with the platform you are using.

@anchor background
@par Technical Background

To train a neural net, the respective loss function is minimized using stochastic gradient descent.
In the case of classification, the loss function is cross entropy.  For regression, mean square error
is used. Weights in the neural net are updated via the backpropogation process, which uses dynamic
programming to compute the partial derivative of each weight with respect to the overall loss. This
partial derivative incorporates the respective activation function used, so this requires that the
activation function be differentiable.

For an overview of multilayer perceptrons, see website [1].

For details on backpropogation, see the notes at [2].

@anchor literature
@literature

@anchor mlp-lit-1
[1] "Multilayer Perceptron." Wikipedia. Wikimedia Foundation,
    12 July 2017. Web. 12 July 2017.

[2] Yu Hen Hu. "Lecture 11. MLP (III): Back-Propagation."
    University of Wisconsin Madison: Computer-Aided Engineering. Web. 12 July 2017,
    http://homepages.cae.wisc.edu/~ece539/videocourse/notes/pdf/lec%2011%20MLP%20(3)%20BP.pdf

@anchor related
@par Related Topics

File mlp.sql_in documenting the training function

*/

CREATE TYPE MADLIB_SCHEMA.mlp_step_result AS (
        state    DOUBLE PRECISION[],
        loss     DOUBLE PRECISION
);

CREATE TYPE MADLIB_SCHEMA.mlp_result AS (
        coeff    DOUBLE PRECISION[],
        loss     DOUBLE PRECISION
);

--------------------------------------------------------------------------
-- create SQL functions for IGD optimizer
--------------------------------------------------------------------------
CREATE FUNCTION MADLIB_SCHEMA.mlp_igd_transition(
        state              DOUBLE PRECISION[],
        ind_var            DOUBLE PRECISION[],
        dep_var            DOUBLE PRECISION[],
        previous_state     DOUBLE PRECISION[],
        layer_sizes        DOUBLE PRECISION[],
        learning_rate_init DOUBLE PRECISION,
        activation         INTEGER,
        is_classification  INTEGER,
        weight             DOUBLE PRECISION,
        warm_start         BOOLEAN,
        warm_start_coeff   DOUBLE PRECISION[],
        n_tuples           INTEGER,
        lambda             DOUBLE PRECISION,
        x_means            DOUBLE PRECISION[],
        x_stds             DOUBLE PRECISION[]
    )
RETURNS DOUBLE PRECISION[]
AS 'MODULE_PATHNAME'
LANGUAGE C IMMUTABLE;

CREATE FUNCTION MADLIB_SCHEMA.mlp_igd_merge(
        state1 DOUBLE PRECISION[],
        state2 DOUBLE PRECISION[])
RETURNS DOUBLE PRECISION[]
AS 'MODULE_PATHNAME'
LANGUAGE C IMMUTABLE STRICT;

CREATE FUNCTION MADLIB_SCHEMA.mlp_igd_final(
        state DOUBLE PRECISION[])
RETURNS MADLIB_SCHEMA.mlp_step_result
AS 'MODULE_PATHNAME'
LANGUAGE C IMMUTABLE STRICT;

/**
 * @internal
 * @brief Perform one iteration of backprop
 */
CREATE AGGREGATE MADLIB_SCHEMA.mlp_igd_step(
        /* ind_var */             DOUBLE PRECISION[],
        /* dep_var */             DOUBLE PRECISION[],
        /* previous_state */      DOUBLE PRECISION[],
        /* layer_sizes */         DOUBLE PRECISION[],
        /* learning_rate_init */  DOUBLE PRECISION,
        /* activation */          INTEGER,
        /* is_classification */   INTEGER,
        /* weight */              DOUBLE PRECISION,
        /* warm_start */          BOOLEAN,
        /* warm_start_coeff */    DOUBLE PRECISION[],
        /* n_tuples */            INTEGER,
        /* lambda */              DOUBLE PRECISION,
        /* x_means */             DOUBLE PRECISION[],
        /* x_stds */              DOUBLE PRECISION[]
        )(
    STYPE=DOUBLE PRECISION[],
    SFUNC=MADLIB_SCHEMA.mlp_igd_transition,
    m4_ifdef(`__POSTGRESQL__', `', `prefunc=MADLIB_SCHEMA.mlp_igd_merge,')
    FINALFUNC=MADLIB_SCHEMA.mlp_igd_final,
    INITCOND='{0,0,0,0,0,0,0,0}'
);
-------------------------------------------------------------------------

CREATE FUNCTION MADLIB_SCHEMA.internal_mlp_igd_distance(
    /*+ state1 */ DOUBLE PRECISION[],
    /*+ state2 */ DOUBLE PRECISION[])
RETURNS DOUBLE PRECISION AS
'MODULE_PATHNAME'
LANGUAGE c IMMUTABLE STRICT;

CREATE FUNCTION MADLIB_SCHEMA.internal_mlp_igd_result(
    /*+ state */ DOUBLE PRECISION[])
RETURNS MADLIB_SCHEMA.mlp_result AS
'MODULE_PATHNAME'
LANGUAGE c IMMUTABLE STRICT;
-------------------------------------------------------------------------

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_classification(
    source_table         VARCHAR,
    output_table         VARCHAR,
    independent_varname  VARCHAR,
    dependent_varname    VARCHAR,
    hidden_layer_sizes   INTEGER[],
    optimizer_params     VARCHAR,
    activation           VARCHAR,
    weights              VARCHAR,
    warm_start           BOOLEAN,
    verbose              BOOLEAN
) RETURNS VOID AS $$
    PythonFunctionBodyOnly(`convex', `mlp_igd')
    mlp_igd.mlp(
        schema_madlib,
        source_table,
        output_table,
        independent_varname,
        dependent_varname,
        hidden_layer_sizes,
        optimizer_params,
        activation,
        True,
        weights,
        warm_start,
        verbose
    )
$$ LANGUAGE plpythonu VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA', `');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_classification(
    source_table         VARCHAR,
    output_table         VARCHAR,
    independent_varname  VARCHAR,
    dependent_varname    VARCHAR,
    hidden_layer_sizes   INTEGER[],
    optimizer_params     VARCHAR,
    activation           VARCHAR,
    weights              VARCHAR,
    warm_start           BOOLEAN
) RETURNS VOID AS $$
    SELECT MADLIB_SCHEMA.mlp_classification($1, $2, $3, $4, $5, $6, $7, $8, $9, NULL);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_classification(
    source_table         VARCHAR,
    output_table         VARCHAR,
    independent_varname  VARCHAR,
    dependent_varname    VARCHAR,
    hidden_layer_sizes   INTEGER[],
    optimizer_params     VARCHAR,
    activation           VARCHAR,
    weights              VARCHAR
) RETURNS VOID AS $$
    SELECT MADLIB_SCHEMA.mlp_classification($1, $2, $3, $4, $5, $6, $7, $8, NULL, NULL);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_classification(
    source_table         VARCHAR,
    output_table         VARCHAR,
    independent_varname  VARCHAR,
    dependent_varname    VARCHAR,
    hidden_layer_sizes   INTEGER[],
    optimizer_params     VARCHAR,
    activation           VARCHAR
) RETURNS VOID AS $$
    SELECT MADLIB_SCHEMA.mlp_classification($1, $2, $3, $4, $5, $6, $7, NULL, NULL, NULL);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_classification(
    source_table         VARCHAR,
    output_table         VARCHAR,
    independent_varname  VARCHAR,
    dependent_varname    VARCHAR,
    hidden_layer_sizes   INTEGER[],
    optimizer_params     VARCHAR
) RETURNS VOID AS $$
    SELECT MADLIB_SCHEMA.mlp_classification($1, $2, $3, $4, $5, $6, NULL, NULL, NULL, FALSE);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_classification(
    source_table         VARCHAR,
    output_table         VARCHAR,
    independent_varname  VARCHAR,
    dependent_varname    VARCHAR,
    hidden_layer_sizes   INTEGER[]
) RETURNS VOID AS $$
    SELECT MADLIB_SCHEMA.mlp_classification($1, $2, $3, $4, $5, NULL, NULL, NULL, FALSE, FALSE);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');



CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_regression(
    source_table         VARCHAR,
    output_table         VARCHAR,
    independent_varname  VARCHAR,
    dependent_varname    VARCHAR,
    hidden_layer_sizes   INTEGER[],
    optimizer_params     VARCHAR,
    activation           VARCHAR,
    weights              VARCHAR,
    warm_start           BOOLEAN,
    verbose              BOOLEAN
) RETURNS VOID AS $$
    PythonFunctionBodyOnly(`convex', `mlp_igd')
    mlp_igd.mlp(
        schema_madlib,
        source_table,
        output_table,
        independent_varname,
        dependent_varname,
        hidden_layer_sizes,
        optimizer_params,
        activation,
        False,
        weights,
        warm_start,
        verbose
    )
$$ LANGUAGE plpythonu VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA', `');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_regression(
    source_table         VARCHAR,
    output_table         VARCHAR,
    independent_varname  VARCHAR,
    dependent_varname    VARCHAR,
    hidden_layer_sizes   INTEGER[],
    optimizer_params     VARCHAR,
    activation           VARCHAR,
    weights              VARCHAR,
    warm_start           BOOLEAN
) RETURNS VOID AS $$
    SELECT MADLIB_SCHEMA.mlp_regression($1, $2, $3, $4, $5, $6, $7, $8, $9, NULL);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_regression(
    source_table         VARCHAR,
    output_table         VARCHAR,
    independent_varname  VARCHAR,
    dependent_varname    VARCHAR,
    hidden_layer_sizes   INTEGER[],
    optimizer_params     VARCHAR,
    activation           VARCHAR,
    weights              VARCHAR
) RETURNS VOID AS $$
    SELECT MADLIB_SCHEMA.mlp_regression($1, $2, $3, $4, $5, $6, $7, $8, NULL, NULL);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_regression(
    source_table         VARCHAR,
    output_table         VARCHAR,
    independent_varname  VARCHAR,
    dependent_varname    VARCHAR,
    hidden_layer_sizes   INTEGER[],
    optimizer_params     VARCHAR,
    activation           VARCHAR
) RETURNS VOID AS $$
    SELECT MADLIB_SCHEMA.mlp_regression($1, $2, $3, $4, $5, $6, $7, NULL, NULL, NULL);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_regression(
    source_table         VARCHAR,
    output_table         VARCHAR,
    independent_varname  VARCHAR,
    dependent_varname    VARCHAR,
    hidden_layer_sizes   INTEGER[],
    optimizer_params     VARCHAR
) RETURNS VOID AS $$
    SELECT MADLIB_SCHEMA.mlp_regression($1, $2, $3, $4, $5, $6, NULL, NULL, NULL, FALSE);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_regression(
    source_table         VARCHAR,
    output_table         VARCHAR,
    independent_varname  VARCHAR,
    dependent_varname    VARCHAR,
    hidden_layer_sizes   INTEGER[]
) RETURNS VOID AS $$
    SELECT MADLIB_SCHEMA.mlp_regression($1, $2, $3, $4, $5, NULL, NULL, NULL, FALSE, FALSE);
$$ LANGUAGE sql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_predict(
    model_table      VARCHAR,
    data_table      VARCHAR,
    id_col_name    VARCHAR,
    output_table      VARCHAR,
    pred_type      VARCHAR
) RETURNS VOID AS $$
    PythonFunctionBodyOnly(`convex', `mlp_igd')
    mlp_igd.mlp_predict(
        schema_madlib,
        model_table,
        data_table,
        id_col_name,
        output_table,
        pred_type)
$$ LANGUAGE plpythonu VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA', `');

CREATE FUNCTION MADLIB_SCHEMA.internal_predict_mlp(
        coeff DOUBLE PRECISION[],
        independent_varname DOUBLE PRECISION[],
        is_classification DOUBLE PRECISION,
        activation DOUBLE PRECISION,
        layer_sizes DOUBLE PRECISION[],
        is_response INTEGER,
        x_means DOUBLE PRECISION[],
        x_stds DOUBLE PRECISION[]
    )
RETURNS DOUBLE PRECISION[]
AS 'MODULE_PATHNAME'
LANGUAGE C IMMUTABLE STRICT;


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_classification(
   message TEXT
) RETURNS TEXT AS $$
    PythonFunctionBodyOnly(`convex', `mlp_igd')
    return mlp_igd.mlp_help(schema_madlib,message,True)
$$ LANGUAGE plpythonu
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `NO SQL', `');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_classification()
RETURNS TEXT AS $$
  SELECT MADLIB_SCHEMA.mlp_classification(NULL::TEXT)
$$ LANGUAGE SQL IMMUTABLE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `CONTAINS SQL', `');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_regression(
   message TEXT
) RETURNS TEXT AS $$
    PythonFunctionBodyOnly(`convex', `mlp_igd')
    return mlp_igd.mlp_help(schema_madlib,message,False)
$$ LANGUAGE plpythonu
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `NO SQL', `');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_regression()
RETURNS TEXT AS $$
  SELECT MADLIB_SCHEMA.mlp_regression(NULL::TEXT)
$$ LANGUAGE SQL IMMUTABLE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `CONTAINS SQL', `');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_predict(
   message TEXT
) RETURNS TEXT AS $$
    PythonFunctionBodyOnly(`convex', `mlp_igd')
    return mlp_igd.mlp_predict_help(schema_madlib,message)
$$ LANGUAGE plpythonu
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `NO SQL', `');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.mlp_predict()
RETURNS TEXT AS $$
  SELECT MADLIB_SCHEMA.mlp_predict(NULL::TEXT)
$$ LANGUAGE SQL IMMUTABLE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `CONTAINS SQL', `');
