# coding=utf-8
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
"""
@file mlp_igd.py_in

@brief Multilayer perceptron using IGD: Driver functions

@namespace mlp_igd
"""
import math
import plpy

from utilities.utilities import add_postfix
from utilities.utilities import py_list_to_sql_string
from utilities.utilities import extract_keyvalue_params
from utilities.utilities import _assert
from utilities.utilities import _assert_equal
from utilities.utilities import unique_string
from utilities.utilities import strip_end_quotes
from utilities.validate_args import cols_in_tbl_valid
from utilities.validate_args import table_exists
from utilities.validate_args import input_tbl_valid
from utilities.validate_args import is_var_valid
from utilities.validate_args import output_tbl_valid
from utilities.validate_args import get_expr_type
from utilities.validate_args import array_col_has_same_dimension
from utilities.validate_args import array_col_dimension

from convex.utils_regularization import __utils_ind_var_scales

from elastic_net.elastic_net_utils import _tbl_dimension_rownum


def mlp(schema_madlib, source_table, output_table, independent_varname,
        dependent_varname, hidden_layer_sizes, optimizer_param_str, activation,
        is_classification, weights, warm_start, verbose=False):
    """
    Args:
        @param schema_madlib
        @param source_table
        @param output_table
        @param independent_varname
        @param dependent_varname
        @param hidden_layer_sizes
        @param optimizer_param_str

    Returns:
        None
    """
    warm_start = bool(warm_start)
    optimizer_params = _get_optimizer_params(optimizer_param_str or "")
    summary_table = add_postfix(output_table, "_summary")
    weights = '1' if not weights or not weights.strip() else weights.strip()
    hidden_layer_sizes = hidden_layer_sizes or []
    activation = _get_activation_function_name(activation)
    learning_rate_policy = _get_learning_rate_policy_name(
        optimizer_params["learning_rate_policy"])
    activation_index = _get_activation_index(activation)

    _validate_args(source_table, output_table, summary_table, independent_varname,
                   dependent_varname, hidden_layer_sizes,
                   optimizer_params, is_classification, weights,
                   warm_start, activation)

    current_iteration = 1
    prev_state = None
    tolerance = optimizer_params["tolerance"]
    n_iterations = optimizer_params["n_iterations"]
    step_size_init = optimizer_params["learning_rate_init"]
    iterations_per_step = optimizer_params["iterations_per_step"]
    power = optimizer_params["power"]
    gamma = optimizer_params["gamma"]
    step_size = step_size_init
    n_tries = optimizer_params["n_tries"]
    # lambda is a reserved word in python
    lmbda = optimizer_params["lambda"]
    iterations_per_step = optimizer_params["iterations_per_step"]
    num_input_nodes = array_col_dimension(source_table,
                                          independent_varname)
    num_output_nodes = 0
    classes = []
    dependent_type = get_expr_type(dependent_varname, source_table)
    original_dependent_varname = dependent_varname
    dimension, n_tuples = _tbl_dimension_rownum(
        schema_madlib, source_table, independent_varname)
    x_scales = __utils_ind_var_scales(
        source_table, independent_varname, dimension, schema_madlib)
    x_means = py_list_to_sql_string(
        x_scales["mean"], array_type="DOUBLE PRECISION")
    filtered_stds = [x if x != 0 else 1 for x in x_scales["std"]]
    x_stds = py_list_to_sql_string(
        filtered_stds, array_type="DOUBLE PRECISION")

    if is_classification:
        dependent_variable_sql = """
        SELECT DISTINCT {dependent_varname}
        FROM {source_table}
        """.format(
            dependent_varname=dependent_varname, source_table=source_table)
        labels = plpy.execute(dependent_variable_sql)
        one_hot_dependent_varname = 'ARRAY['
        num_output_nodes = len(labels)
        for label_obj in labels:
            label = _format_label(label_obj[dependent_varname])
            classes.append(label)
        classes.sort()
        for c in classes:
            one_hot_dependent_varname += dependent_varname + \
                "=" + str(c) + ","
        # Remove the last comma
        one_hot_dependent_varname = one_hot_dependent_varname[:-1]
        one_hot_dependent_varname += ']::integer[]'
        dependent_varname = one_hot_dependent_varname
    else:
        if "[]" not in dependent_type:
            dependent_varname = "ARRAY[" + dependent_varname + "]"
        num_output_nodes = array_col_dimension(
            source_table, dependent_varname)
    layer_sizes = [num_input_nodes] + \
        hidden_layer_sizes + [num_output_nodes]

    # Need layers sizes before validating for warm_start
    coeff = []
    for i in range(len(layer_sizes) - 1):
        fan_in = layer_sizes[i]
        fan_out = layer_sizes[i + 1]
        # Initalize according to Glorot and Bengio (2010)
        # See design doc for more info
        span = math.sqrt(6.0 / (fan_in + fan_out))
        dim = (layer_sizes[i] + 1) * layer_sizes[i + 1]
        rand = plpy.execute("""SELECT array_agg({span}*(random()-0.5))
                               AS random
                               FROM generate_series(0,{dim})
                """.format(span=span, dim=dim))[0]["random"]
        coeff += rand

    if warm_start:
        coeff, x_means, x_stds = _validate_warm_start(
                source_table, output_table, summary_table, independent_varname,
                original_dependent_varname, layer_sizes, optimizer_params,
                is_classification, weights, warm_start, activation)
        plpy.execute("DROP TABLE IF EXISTS {0}".format(output_table))
        plpy.execute("DROP TABLE IF EXISTS {0}".format(summary_table))
    best_state = []
    best_loss = [float('inf')]
    prev_loss = float('inf')
    loss = None
    for _ in range(n_tries):
        while True:
            if prev_state:
                prev_state_str = py_list_to_sql_string(
                    prev_state, array_type="double precision")
            else:
                prev_state_str = "(NULL)::DOUBLE PRECISION[]"
            # else block is for "constant", so don't do anything
            zero_indexed_iteration = current_iteration - 1
            if learning_rate_policy == "exp":
                step_size = step_size_init * gamma**zero_indexed_iteration
            elif learning_rate_policy == "inv":
                step_size = step_size_init * (current_iteration)**(-power)
            elif learning_rate_policy == "step":
                step_size = step_size_init * gamma**(
                    math.floor(zero_indexed_iteration / iterations_per_step))


            train_sql = """
            SELECT
                (result).state as state,
                (result).loss  as loss
            FROM (
            SELECT
                {schema_madlib}.mlp_igd_step(
                    ({independent_varname})::DOUBLE PRECISION[],
                    ({dependent_varname})::DOUBLE PRECISION[],
                    {prev_state},
                    {layer_sizes},
                    ({step_size})::FLOAT8,
                    {activation},
                    {is_classification},
                    ({weights})::DOUBLE PRECISION,
                    {warm_start},
                    ({warm_start_coeff})::DOUBLE PRECISION[],
                    {n_tuples},
                    {lmbda},
                    {x_means},
                    {x_stds}
                    ) as result
            FROM {source_table} as _src) _step_q
            """.format(
                schema_madlib=schema_madlib,
                independent_varname=independent_varname,
                dependent_varname=dependent_varname,
                prev_state=prev_state_str,
                # c++ uses double internally
                layer_sizes=py_list_to_sql_string(
                    layer_sizes, array_type="DOUBLE PRECISION"),
                step_size=step_size,
                source_table=source_table,
                activation=activation_index,
                is_classification=int(is_classification),
                weights=weights,
                warm_start=warm_start,
                warm_start_coeff=py_list_to_sql_string(
                    coeff, array_type="DOUBLE PRECISION"),
                n_tuples=n_tuples,
                lmbda=lmbda,
                x_means=x_means,
                x_stds=x_stds)
            step_result = plpy.execute(train_sql)[0]
            curr_state = step_result['state']
            loss = step_result['loss']
            if verbose and 1 < current_iteration <= n_iterations:
                plpy.info("Iteration: " + str(current_iteration -
                                              1) + ", Loss: " + str(loss))
            state_dist = abs(loss-prev_loss)
            if ((state_dist and state_dist < tolerance)
                    or current_iteration > n_iterations):
                break
            prev_state = curr_state
            prev_loss = loss
            current_iteration += 1
        # We use previous state because the last iteration
        # just calculates loss
        if loss < best_loss:
            best_state = prev_state
            best_loss = loss
        current_iteration = 1
        prev_state = None
    _build_model_table(schema_madlib, output_table, best_state,
                       best_loss, n_iterations)
    layer_sizes_str = py_list_to_sql_string(
        layer_sizes, array_type="integer")
    classes_str = py_list_to_sql_string(
        [strip_end_quotes(cl, "'") for cl in classes],
        array_type=dependent_type)
    summary_table_creation_query = """
    CREATE TABLE {summary_table}(
        source_table TEXT,
        independent_varname TEXT,
        dependent_varname TEXT,
        tolerance FLOAT,
        learning_rate_init FLOAT,
        learning_rate_policy TEXT,
        n_iterations INTEGER,
        n_tries INTEGER,
        layer_sizes INTEGER[],
        activation TEXT,
        is_classification BOOLEAN,
        classes {dependent_type}[],
        weights VARCHAR,
        x_means DOUBLE PRECISION[],
        x_stds DOUBLE PRECISION[]
    )""".format(summary_table=summary_table,
                dependent_type=dependent_type)

    summary_table_update_query = """
    INSERT INTO {summary_table} VALUES(
        '{source_table}',
        '{independent_varname}',
        '{original_dependent_varname}',
        {tolerance},
        {step_size_init},
        '{learning_rate_policy}',
        {n_iterations},
        {n_tries},
        {layer_sizes_str},
        '{activation}',
        {is_classification},
        {classes_str},
        '{weights}',
        {x_means},
        {x_stds}
    )
    """.format(**locals())
    plpy.execute(summary_table_creation_query)
    plpy.execute(summary_table_update_query)
    return None


def _get_loss(schema_madlib, state):
    return plpy.execute("""
    SELECT
        (result).loss  AS loss
    FROM (
        SELECT
            {schema_madlib}.internal_mlp_igd_result(
                {final_state_str}
            ) AS result
    ) rel_state_subq
    """.format(
        schema_madlib=schema_madlib,
        final_state_str=py_list_to_sql_string(state)))[0]["loss"]


def _build_model_table(schema_madlib, output_table, final_state, loss, n_iterations):
    final_state_str = py_list_to_sql_string(
        final_state, array_type="double precision")

    model_table_query = """
    CREATE TABLE {output_table} AS
        SELECT
            (result).coeff as coeff,
            {loss}  as loss,
            {n_iterations} as num_iterations
        FROM (
            SELECT
                {schema_madlib}.internal_mlp_igd_result(
                    {final_state_str}
                ) AS result
        ) rel_state_subq
    """.format(**locals())
    plpy.execute(model_table_query)


def _get_optimizer_params(param_str):
    params_defaults = {
        "learning_rate_init": (0.001, float),
        "n_iterations": (100, int),
        "n_tries": (1, int),
        "tolerance": (0.001, float),
        "learning_rate_policy": ("constant", str),
        "gamma": (0.1, float),
        "iterations_per_step": (100, int),
        "power": (0.5, float),
        "lambda": (0, float)
    }
    param_defaults = dict([(k, v[0]) for k, v in params_defaults.items()])
    param_types = dict([(k, v[1]) for k, v in params_defaults.items()])

    if not param_str:
        return param_defaults

    name_value = extract_keyvalue_params(
        param_str, param_types, param_defaults, ignore_invalid=False)
    return name_value


def _validate_args_classification(source_table, dependent_varname):
    expr_type = get_expr_type(dependent_varname, source_table)
    int_types = ['integer', 'smallint', 'bigint']
    text_types = ['text', 'varchar', 'character varying', 'char', 'character']
    boolean_types = ['boolean']
    _assert("[]" in expr_type
            or expr_type in int_types + text_types + boolean_types,
            "Dependent variable column should refer to an "
            "integer, boolean, text, varchar, or character type.")


def _validate_args_regression(source_table, dependent_varname):
    expr_type = get_expr_type(dependent_varname, source_table)
    int_types = ['integer', 'smallint', 'bigint']
    float_types = ['double precision', 'real']
    _assert(
        "[]" in expr_type or expr_type in int_types + float_types,
        "Dependent variable column should refer to an array or numeric type")
    if "[]" in expr_type:
        _assert(
            array_col_has_same_dimension(source_table, dependent_varname),
            "Dependent variable column should refer to arrays of the same length"
        )


def _validate_summary_table(summary_table):
    input_tbl_valid(summary_table, 'MLP')
    cols_in_tbl_valid(summary_table, [
        'dependent_varname', 'independent_varname', 'activation',
        'tolerance', 'learning_rate_init', 'n_iterations', 'n_tries',
        'classes', 'layer_sizes', 'source_table', 'x_means', 'x_stds'
    ], 'MLP')


def _validate_warm_start(source_table, output_table, summary_table, independent_varname,
                         dependent_varname, layer_sizes,
                         optimizer_params, is_classification, weights,
                         warm_start, activation):
    _assert(table_exists(output_table),
            "MLP error: Warm start failed due to missing model table: " + output_table)
    _assert(table_exists(summary_table),
            "MLP error: Warm start failed due to missing summary table: " + summary_table)

    _assert(optimizer_params["n_tries"] == 1,
            "MLP error: warm_start is only compatible for n_tries = 1")

    summary = plpy.execute("SELECT * FROM {0}".format(summary_table))[0]
    params = [
        "independent_varname", "dependent_varname", "layer_sizes",
        "is_classification", "weights", "activation"
    ]
    for param in params:
        _assert_equal(eval(param), summary[param],
                      "MLP error: warm start failed due to different parameter value: " +
                      param)
    output = plpy.execute("SELECT * FROM {0}".format(output_table))[0]
    coeff = output['coeff']
    num_coeffs = sum(
        map(lambda i: (layer_sizes[i] + 1) * (layer_sizes[i + 1]),
            range(len(layer_sizes) - 1)))
    _assert_equal(num_coeffs,
                  len(coeff),
                  "MLP error: Warm start failed to invalid output_table: " +
                  output_table + ". Invalid number of coefficients in model.")
    x_means = py_list_to_sql_string(
        summary["x_means"], array_type="DOUBLE PRECISION")
    x_stds = py_list_to_sql_string(
        summary["x_stds"], array_type="DOUBLE PRECISION")

    return coeff, x_means, x_stds


def _validate_args(source_table, output_table, summary_table, independent_varname,
                   dependent_varname, hidden_layer_sizes,
                   optimizer_params, is_classification, weights, warm_start, activation):
    input_tbl_valid(source_table, "MLP")
    if not warm_start:
        output_tbl_valid(output_table, "MLP")
        output_tbl_valid(summary_table, "MLP")

    _assert(
        is_var_valid(source_table, independent_varname),
        "MLP error: invalid independent_varname "
        "('{independent_varname}') for source_table "
        "({source_table})!".format(
            independent_varname=independent_varname,
            source_table=source_table))

    _assert(
        is_var_valid(source_table, dependent_varname),
        "MLP error: invalid dependent_varname "
        "('{dependent_varname}') for source_table "
        "({source_table})!".format(
            dependent_varname=dependent_varname, source_table=source_table))
    _assert(
        isinstance(hidden_layer_sizes, list),
        "hidden_layer_sizes must be an array of integers")
    # TODO put this check earlier
    _assert(
        all(isinstance(value, int) for value in hidden_layer_sizes),
        "MLP error: Hidden layers sizes must be integers")
    _assert(
        all(value >= 0 for value in hidden_layer_sizes),
        "MLP error: Hidden layers sizes must be greater than 0.")
    _assert(optimizer_params["lambda"] >= 0,
            "MLP error: lambda should be greater than or equal to 0.")
    _assert(optimizer_params["tolerance"] >= 0,
            "MLP error: tolerance should be greater than or equal to 0.")
    _assert(optimizer_params["n_tries"] >= 1,
            "MLP error: n_tries should be greater than or equal to 1")
    _assert(
        optimizer_params["n_iterations"] >= 1,
        "MLP error: n_iterations should be greater than or equal to 1")
    _assert(optimizer_params["power"] > 0,
            "MLP error: power should be greater than 0.")
    _assert(0 < optimizer_params["gamma"] <= 1,
            "MLP error: gamma should be between 0 and 1.")
    _assert(optimizer_params["iterations_per_step"] > 0,
            "MLP error: iterations_per_step should be greater than 0.")
    _assert(optimizer_params["learning_rate_init"] > 0,
            "MLP error: learning_rate_init should be greater than 0.")
    _assert("[]" in get_expr_type(independent_varname, source_table),
            "Independent variable column should refer to an array")
    _assert(
        array_col_has_same_dimension(source_table, independent_varname),
        "Independent variable column should refer to arrays of the same length"
    )

    int_types = ['integer', 'smallint', 'bigint']
    float_types = ['double precision', 'real']
    _assert(
        get_expr_type(weights, source_table) in int_types + float_types,
        "MLP error: Weights should be a numeric type")

    if is_classification:
        _validate_args_classification(source_table, dependent_varname)
    else:
        _validate_args_regression(source_table, dependent_varname)


def _get_learning_rate_policy_name(learning_rate_policy):
    if not learning_rate_policy:
        learning_rate_policy = 'constant'
    else:
        supported_learning_rate_policies = ['constant', 'exp', 'inv', 'step']
        try:
            learning_rate_policy = next(
                x for x in supported_learning_rate_policies
                if x.startswith(learning_rate_policy))
        except StopIteration:
            plpy.error(
                "MLP Error: Invalid learning rate policy: "
                "{0}. Supported learning rate policies are ({1})".format(
                    learning_rate_policy,
                    ','.join(sorted(supported_learning_rate_policies))))
    return learning_rate_policy


def _get_activation_function_name(activation):
    if not activation:
        activation = 'sigmoid'
    else:
        supported_activation_function = ['sigmoid', 'tanh', 'relu']
        try:
            activation = next(
                x for x in supported_activation_function
                if x.startswith(activation))
        except StopIteration:
            plpy.error("MLP Error: Invalid activation function: "
                       "{0}. Supported activation functions are ({1})".format(
                           activation,
                           ','.join(sorted(supported_activation_function))))
    return activation


def _get_activation_index(activation_name):
    table = {"relu": 0, "sigmoid": 1, "tanh": 2}
    return table[activation_name]


def _format_label(label):
    if isinstance(label, str):
        return "'" + label + "'"
    return label


def mlp_predict(schema_madlib,
                model_table,
                data_table,
                id_col_name,
                output_table,
                pred_type='response',
                **kwargs):
    """ Score new observations using a trained neural network

    @param schema_madlib Name of the schema where MADlib is installed
    @param model_table Name of learned model
    @param data_table Name of table/view containing the data
                          points to be scored
    @param id_col_name Name of column in source_table containing
                       (integer) identifier for data point
    @param output_table Name of table to store the results
    @param pred_type: str, The type of output required:
                    'response' gives the actual response values,
                    'prob' gives the probability of the classes in a
                  For regression, only type='response' is defined.
    """
    input_tbl_valid(model_table, 'MLP')
    cols_in_tbl_valid(model_table, ['coeff'], 'MLP')
    summary_table = add_postfix(model_table, "_summary")
    _validate_summary_table(summary_table)

    summary = plpy.execute("SELECT * FROM {0}".format(summary_table))[0]
    coeff = py_list_to_sql_string(plpy.execute(
        "SELECT * FROM {0}".format(model_table))[0]["coeff"])
    dependent_varname = summary['dependent_varname']
    independent_varname = summary['independent_varname']
    source_table = summary['source_table']
    activation = _get_activation_index(summary['activation'])
    layer_sizes = py_list_to_sql_string(
        summary['layer_sizes'], array_type="DOUBLE PRECISION")
    is_classification = int(summary["is_classification"])
    is_response = int(pred_type == 'response')
    x_means = py_list_to_sql_string(
        summary["x_means"], array_type="DOUBLE PRECISION")
    x_stds = py_list_to_sql_string(
        summary["x_stds"], array_type="DOUBLE PRECISION")

    pred_name = (
        '"prob_{0}"' if pred_type == "prob" else
        '"estimated_{0}"').format(dependent_varname.replace('"', '').strip())

    input_tbl_valid(data_table, 'MLP')

    _assert(
        is_var_valid(data_table, independent_varname),
        "MLP Error: independent_varname ('{0}') is invalid for data_table ({1})".
        format(independent_varname, data_table))
    _assert(id_col_name is not None, "MLP Error: id_col_name is NULL")
    _assert(
        is_var_valid(data_table, id_col_name),
        "MLP Error: id_col_name ('{0}') is invalid for {1}".format(
            id_col_name, data_table))
    output_tbl_valid(output_table, 'MLP')

    header = "CREATE TABLE " + output_table + " AS "
    # Regression
    if not is_classification:
        dependent_type = get_expr_type(dependent_varname, source_table)
        unnest_if_not_array = ""
        # Return the same type as the user provided.  Internally we always
        # use an array, but if they provided a scaler, unnest it for
        # the user
        if "[]" not in dependent_type:
            unnest_if_not_array = "UNNEST"
        sql = header + """
            SELECT {id_col_name},
                   {unnest_if_not_array}({schema_madlib}.internal_predict_mlp(
                        {coeff},
                        {independent_varname}::DOUBLE PRECISION[],
                        {is_classification},
                        {activation},
                        {layer_sizes},
                        {is_response},
                        {x_means},
                        {x_stds}
                    )) as {pred_name}
            FROM {data_table}
            """
    else:
        summary_query = """
        SELECT classes FROM {0}
        """.format(summary_table)
        classes = plpy.execute(summary_query)[0]['classes']
        if pred_type == "response":
            classes_with_index_table = unique_string()
            classes_table = unique_string()
            sql = header + """
                    SELECT
                         q.{id_col_name}
                        ,(ARRAY{classes})[pred_idx[1]+1] as {pred_name}
                    FROM (
                         SELECT
                            {id_col_name},
                            {schema_madlib}.internal_predict_mlp(
                                    {coeff}::DOUBLE PRECISION[],
                                    {independent_varname}::DOUBLE PRECISION[],
                                    {is_classification},
                                    {activation},
                                    {layer_sizes},
                                    {is_response},
                                    {x_means},
                                    {x_stds}
                                    )
                           as pred_idx
                        FROM {data_table}
                    ) q
                """
        else:
            # Incomplete
            intermediate_col = unique_string()
            score_format = ',\n'.join([
                'CAST({interim}[{j}] as DOUBLE PRECISION) as "estimated_prob_{c_str}"'.
                format(j=i + 1, c_str=str(c).strip(' "'),
                       interim=intermediate_col)
                for i, c in enumerate(classes)])
            sql = header + """
                SELECT
                    {id_col_name},
                    {score_format}
                    FROM (
                        SELECT {id_col_name},
                               {schema_madlib}.internal_predict_mlp(
                                   {coeff}::DOUBLE PRECISION[],
                                   {independent_varname}::DOUBLE PRECISION[],
                                   {is_classification},
                                   {activation},
                                   {layer_sizes},
                                   {is_response},
                                   {x_means},
                                   {x_stds}
                                   )::TEXT[]
                                        AS {intermediate_col}
                        FROM {data_table}
                    ) q
                """
    sql = sql.format(**locals())
    plpy.execute(sql)


def mlp_help(schema_madlib, message, is_classification):
    method = 'mlp_classification' if is_classification else 'mlp_regression'
    int_types = ['integer', 'smallint', 'bigint']
    text_types = ['text', 'varchar', 'character varying', 'char', 'character']
    boolean_types = ['boolean']
    supported_types = " " * 33 + ", ".join(text_types) + "\n" +\
        " " * 33 + ", ".join(int_types + boolean_types)
    label_description_classification = "Name of a column which specifies label.\n" +\
        " " * 33 + "Supported types are:\n" + supported_types
    label_description_regression = "Dependent variable. May be an array for multiple\n" +\
        " " * 33 + "regression or the name of a column which is any\n" + " " * 33 +\
        "numeric type for single regression"
    label_description = label_description_classification if is_classification\
        else label_description_regression
    args = dict(schema_madlib=schema_madlib, method=method,
                label_description=label_description)

    summary = """
    ----------------------------------------------------------------
                            SUMMARY
    ----------------------------------------------------------------
    Multilayer Perceptron (MLP) is a model for regression and
    classification

    Also called "vanilla neural networks", they consist of several
    fully connected hidden layers with non-linear activation
    functions.

    For more details on function usage:
        SELECT {schema_madlib}.{method}('usage')

    For a small example on using the function:
        SELECT {schema_madlib}.{method}('example')""".format(**args)

    usage = """
    ---------------------------------------------------------------------------
                                    USAGE
    ---------------------------------------------------------------------------
    SELECT {schema_madlib}.{method}(
        source_table,         -- TEXT. name of input table
        output_table,         -- TEXT. name of output model table
        independent_varname,  -- TEXT. name of independent variable
        dependent_varname,    -- TEXT. {label_description}
        hidden_layer_sizes,   -- INTEGER[]. Array of integers indicating the
                                 number of hidden units per layer.
                                 Length equal to the number of hidden layers.
        optimizer_params,     -- TEXT. optional, default NULL
                                 parameters for optimization in
                                 a comma-separated string of key-value pairs.
                                 To find out more:

                      SELECT {schema_madlib}.{method}('optimizer_params')

        activation            -- TEXT. optional, default: 'sigmoid'.
                                 supported activations: 'relu', 'sigmoid',
                                 and 'tanh'

        weights               -- TEXT. optional, default: NULL.
                                 Weights for input rows. Column name which
                                 specifies the weight for each input row.
                                 This weight will be incorporated into the
                                 update during SGD, and will not be used
                                 for loss calculations. If not specified,
                                 weight for each row will default to 1.
                                 Column should be a numeric type.

        warm_start            -- BOOLEAN. optional, default: FALSE.
                                 Initalize weights with the coefficients from
                                 the last call.  If true, weights will
                                 be initialized from output_table. Note that
                                 all parameters other than optimizer_params,
                                 and verbose must remain constant between calls
                                 to warm_start.

        verbose               -- BOOLEAN. optional, default: FALSE
                                 Provides verbose output of the results of
                                 training.
    );


    ---------------------------------------------------------------------------
                                    OUTPUT
    ---------------------------------------------------------------------------
    The model table produced by MLP contains the following columns:

    coeffs             -- Flat array containing the weights of the neural net

    loss               -- The total loss over the training data. Cross entropy
                          for classification and MSE for regression

    num_iterations     -- The total number of training iterations

    """.format(**args)

    regression_example = """
    - Create input table

    CREATE TABLE lin_housing_wi (id serial, x float8[], grp_by_col int, y float8);
    COPY lin_housing_wi (x, grp_by_col, y) FROM STDIN NULL '?' DELIMITER '|';
    {1,0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98} | 1 | 24.00
    {1,0.02731,0.00,7.070,0,0.4690,6.4210,78.90,4.9671,2,242.0,17.80,396.90,9.14} | 1 | 21.60
    {1,0.02729,0.00,7.070,0,0.4690,7.1850,61.10,4.9671,2,242.0,17.80,392.83,4.03} | 1 | 34.70
    {1,0.03237,0.00,2.180,0,0.4580,6.9980,45.80,6.0622,3,222.0,18.70,394.63,2.94} | 1 | 33.40
    {1,0.06905,0.00,2.180,0,0.4580,7.1470,54.20,6.0622,3,222.0,18.70,396.90,5.33} | 1 | 36.20
    {1,0.02985,0.00,2.180,0,0.4580,6.4300,58.70,6.0622,3,222.0,18.70,394.12,5.21} | 1 | 28.70
    {1,0.08829,12.50,7.870,0,0.5240,6.0120,66.60,5.5605,5,311.0,15.20,395.60,12.43} | 1 | 22.90
    {1,0.14455,12.50,7.870,0,0.5240,6.1720,96.10,5.9505,5,311.0,15.20,396.90,19.15} | 1 | 27.10
    {1,0.21124,12.50,7.870,0,0.5240,5.6310,100.00,6.0821,5,311.0,15.20,386.63,29.93} | 1 | 16.50
    {1,0.17004,12.50,7.870,0,0.5240,6.0040,85.90,6.5921,5,311.0,15.20,386.71,17.10} | 1 | 18.90
    {1,0.22489,12.50,7.870,0,0.5240,6.3770,94.30,6.3467,5,311.0,15.20,392.52,20.45} | 1 | 15.00
    {1,0.11747,12.50,7.870,0,0.5240,6.0090,82.90,6.2267,5,311.0,15.20,396.90,13.27} | 1 | 18.90
    {1,0.09378,12.50,7.870,0,0.5240,5.8890,39.00,5.4509,5,311.0,15.20,390.50,15.71} | 1 | 21.70
    \.

    - Generate a multilayer perception with a two hidden layers of 25 units
    each. Use the x column as the independent variables, and use the class
    column as the classification. Set the tolerance to 0 so that 500
    iterations will be run. Use a sigmoid activation function.
    The model will be written to mlp_regress_result.

    DROP TABLE IF EXISTS mlp_regress;
    DROP TABLE IF EXISTS mlp_regress_summary;
    SELECT madlib.mlp_regression(
        'lin_housing',         -- Source table
        'mlp_regress',         -- Desination table
        'x',                   -- Input features
        'y',                   -- Dependent variable
        ARRAY[25,25],            -- Number of units per layer
        'learning_rate_init=0.001,
        n_iterations=500,
        lambda=0.001,
        tolerance=0',
        'relu',
        NULL,             -- Default weight (1)
        FALSE,            -- No warm start
        TRUE              -- Verbose
    );

    """

    classification_example = """
    -- Create input table

    CREATE TABLE iris_data(
        id integer,
        attributes numeric[],
        class_text varchar,
        class integer
    );

    INSERT INTO iris_data VALUES
    (1,ARRAY[5.1,3.5,1.4,0.2],'Iris-setosa',1),
    (2,ARRAY[4.9,3.0,1.4,0.2],'Iris-setosa',1),
    (3,ARRAY[4.7,3.2,1.3,0.2],'Iris-setosa',1),
    (4,ARRAY[4.6,3.1,1.5,0.2],'Iris-setosa',1),
    (5,ARRAY[5.0,3.6,1.4,0.2],'Iris-setosa',1),
    (6,ARRAY[5.4,3.9,1.7,0.4],'Iris-setosa',1),
    (7,ARRAY[4.6,3.4,1.4,0.3],'Iris-setosa',1),
    (8,ARRAY[5.0,3.4,1.5,0.2],'Iris-setosa',1),
    (9,ARRAY[4.4,2.9,1.4,0.2],'Iris-setosa',1),
    (10,ARRAY[4.9,3.1,1.5,0.1],'Iris-setosa',1),
    (11,ARRAY[7.0,3.2,4.7,1.4],'Iris-versicolor',2),
    (12,ARRAY[6.4,3.2,4.5,1.5],'Iris-versicolor',2),
    (13,ARRAY[6.9,3.1,4.9,1.5],'Iris-versicolor',2),
    (14,ARRAY[5.5,2.3,4.0,1.3],'Iris-versicolor',2),
    (15,ARRAY[6.5,2.8,4.6,1.5],'Iris-versicolor',2),
    (16,ARRAY[5.7,2.8,4.5,1.3],'Iris-versicolor',2),
    (17,ARRAY[6.3,3.3,4.7,1.6],'Iris-versicolor',2),
    (18,ARRAY[4.9,2.4,3.3,1.0],'Iris-versicolor',2),
    (19,ARRAY[6.6,2.9,4.6,1.3],'Iris-versicolor',2),
    (20,ARRAY[5.2,2.7,3.9,1.4],'Iris-versicolor',2);


    -- Generate a multilayer perception with a single hidden layer of 5 units.
    Use the attributes column as the independent variables, and use the class
    column as the classification. Set the tolerance to 0 so that 500
    iterations will be run. Use a hyperbolic tangent activation function.
    The model will be written to mlp_model.

    DROP TABLE IF EXISTS mlp_model;
    DROP TABLE IF EXISTS mlp_model_summary;
    SELECT madlib.mlp_classification(
        'iris_data',      -- Source table
        'mlp_model',      -- Destination table
        'attributes',     -- Input features
        'class_text',     -- Label
        ARRAY[5],         -- Number of units per layer
        'learning_rate_init=0.003,
        n_iterations=500,
        tolerance=0',     -- Optimizer params
        'tanh',           -- Activation function
        NULL,             -- Default weight (1)
        FALSE,            -- No warm start
        TRUE              -- Verbose
    );

    SELECT * FROM mlp_model;

    """.format(**args)
    example = classification_example if is_classification else regression_example
    optimizer_params = """
    ------------------------------------------------------------------------------------------------
                                               OPTIMIZER PARAMS
    ------------------------------------------------------------------------------------------------
    learning_rate_init DOUBLE PRECISION, -- Default: 0.001
                                            Initial learning rate
    learning_rate_policy VARCHAR,        -- Default: 'constant'
                                            One of 'constant','exp','inv','step'
                                            'constant': learning_rate =
                                            learning_rate_init
                                            'exp': learning_rate =
                                            learning_rate_init * gamma^(iter)
                                            'inv': learning_rate =
                                            learning_rate_init * (iter+1)^(-power)
                                            'step': learning_rate =
                                            learning_rate_init * gamma^(floor(iter/iterations_per_step))
                                            Where iter is the current iteration of SGD.
    gamma DOUBLE PRECISION,              -- Default: '0.1'
                                            Decay rate for learning rate.
                                            Valid for learning_rate_policy = 'exp', or 'step'
    power DOUBLE PRECISION,              -- Default: '0.5'
                                            Exponent for learning_rate_policy = 'inv'
    iterations_per_step INTEGER,             -- Default: '100'
                                            Number of iterations to run before decreasing the learning
                                            rate by a factor of gamma.  Valid for learning rate
                                            policy = 'step'
    n_iterations INTEGER,                -- Default: 100
                                            Number of iterations per try
    n_tries INTEGER,                     -- Default: 1
                                            Total number of training cycles,
                                            with random initializations to avoid
                                            local minima.
    tolerance DOUBLE PRECISION,          -- Default: 0.001
                                            If the distance in loss between
                                            two iterations is less than the
                                            tolerance training will stop, even if
                                            n_iterations has not been reached.
    """.format(**args)

    if not message:
        return summary
    elif message.lower() in ('usage', 'help', '?'):
        return usage
    elif message.lower() == 'example':
        return example
    elif message.lower() == 'optimizer_params':
        return optimizer_params
    return """
        No such option. Use "SELECT {schema_madlib}.{method}()" for help.
    """.format(**args)


def mlp_predict_help(schema_madlib, message):
    args = dict(schema_madlib=schema_madlib)

    summary = """
    ----------------------------------------------------------------
                            SUMMARY
    ----------------------------------------------------------------
    Multilayer Perceptron (MLP) is a model for regression and
    classification

    Also called "vanilla neural networks", they consist of several
    fully connected hidden layers with non-linear activation
    functions.

    For more details on function usage:
        SELECT {schema_madlib}.mlp_predict('usage')

    For a small example on using the function:
        SELECT {schema_madlib}.mlp_predict('example')""".format(**args)

    usage = """
    ---------------------------------------------------------------------------
                                    USAGE
    ---------------------------------------------------------------------------
    SELECT {schema_madlib}.mlp_predict(
        model_table,        -- name of model table
        data_table,         -- name of data table
        id_col_name,        -- id column for data table
        output_table,       -- name of output table
        pred_type           -- the type of output requested:
                            -- 'response' gives the actual prediction,
                            -- 'prob' gives the probability of each class.
                            -- for regression, only type='response' is defined.
    );


    ---------------------------------------------------------------------------
                                    OUTPUT
    ---------------------------------------------------------------------------
    The model table produced by mlp contains the following columns:

    id                      -- The provided id for the given input vector

    estimated_<COL_NAME>    -- (For pred_type='response') The estimated class
                               for classification or value for regression, where
                               <COL_NAME> is the name of the column to be
                               predicted from training data

    prob_<CLASS>           -- (For pred_type='prob' for classification) The
                              probability of a given class <CLASS> as given by
                              softmax. There will be one column for each class
                              in the training data.

    """.format(**args)

    example = """
    -- See {schema_madlib}.mlp_classification('example') for test
    -- and model tables

    -- Predict classes using
    SELECT {schema_madlib}.mlp_predict(
        'mlp_model',         -- Model table
        'iris_data',         -- Test data table
        'id',                -- Id column in test table
        'mlp_prediction',    -- Output table for predictions
        'response'           -- Output classes, not probabilities
    );

    SELECT * FROM mlp_prediction;

    WITH total_count AS (SELECT count(*) AS c FROM iris_data)
    SELECT count(*)/((SELECT c FROM total_count)::DOUBLE PRECISION)
    AS train_accuracy
    FROM
        (
            SELECT iris_data.class_text AS actual_label,
                mlp_prediction.estimated_class_text AS predicted_label
            FROM mlp_prediction
            INNER JOIN iris_data ON iris_data.id=mlp_prediction.id
        ) q
    WHERE q.actual_label=q.predicted_label;
    """.format(**args)

    if not message:
        return summary
    elif message.lower() in ('usage', 'help', '?'):
        return usage
    elif message.lower() == 'example':
        return example
    return """
        No such option. Use "SELECT {schema_madlib}.mlp_predict()" for help.
    """.format(**args)
