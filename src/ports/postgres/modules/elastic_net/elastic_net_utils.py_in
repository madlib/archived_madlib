
import plpy
import math
import re
from utilities.utilities import _string_to_array
from utilities.utilities import _array_to_string
from convex.utils_regularization import __utils_ind_var_scales
from convex.utils_regularization import __utils_dep_var_scale
from convex.utils_regularization import __utils_normalize_data
from utilities.validate_args import table_is_empty
# from utilities.validate_args import columns_exist_in_table
from utilities.validate_args import table_exists
# from utilities.validate_args import scalar_col_has_no_null
# from utilities.validate_args import array_col_has_same_dimension
# from utilities.validate_args import array_col_has_no_null
from utilities.control import IterationController
#from convex.lasso_igd import IterationControllerNoTableDrop
from utilities.utilities import __mad_version

version_wrapper = __mad_version()
mad_vec = version_wrapper.select_vecfunc()

# ========================================================================

def __process_results (coef, intercept, outstr_array):
    """
    Return features, features_selected, dense_coef
    """
    features = _array_to_string(outstr_array)
    selected_array = []
    dense_coef = []
    for i in range(len(coef)):
        if coef[i] != 0:
            selected_array.append(outstr_array[i])
            dense_coef.append(coef[i])
    features_selected = _array_to_string(selected_array)
    dense_coef = _array_to_string(dense_coef)

    return (features, features_selected, dense_coef, _array_to_string(coef))

# ========================================================================

def __process_warmup_lambdas (lambdas, lambda_value):
    """
    Convert the string of warmup_lambdas into an double array
    @param lambdas The string which will be converted to an array
    @param lambda_value The target value of lambda, which must be equal to
    the last element of the input array
    """
    matched = re.match(r"^[\[\{\(](.*)[\]\}\)]$", lambdas)
    if matched is None:
        plpy.error("Elastic Net error: warmup_lambdas must be NULL or something like {3,2,1} !")

    elm = _string_to_array(matched.group(1))
    for i in range(len(elm)):
        elm[i] = float(elm[i])

    if elm[len(elm)-1] != lambda_value:
        plpy.error("""
                   Elastic Net error: The last element of warmup_lambdas must
                   be equal to the lambda value that you want to compute !
                   """)

    if len(elm) > 1:
        for i in range(len(elm)-1):
            if elm[i] <= elm[i+1]:
                plpy.error("""
                           Elastic Net error: The given warm-up array must be
                           in a strict descent order.
                           """)

    return elm

# ========================================================================

def __generate_warmup_lambda_sequence (tbl_used, col_ind_var, col_dep_var,
                                       dimension, row_num, lambda_value,
                                       alpha, num_steps, sq):
    """
    Compute lambda sequence, when warmup is True and warmup_lambdas are
    not given
    """
    if num_steps == 1:
        return [lambda_value]

    # mean_y = plpy.execute("select avg({col_dep_var}) from {tbl_used}".format(
    #     col_dep_var = col_dep_var, tbl_used = tbl_used))[0]["avg"]
    # xy = [0] * dimension
    # for i in range(1,dimension+1):
    #     xy[i-1] = plpy.execute("""
    #                          select abs(sum({col_ind_var}[{i}] * ({col_dep_var} - {mean_y})))
    #                          from {tbl_used}
    #                          """.format(col_ind_var = col_ind_var,
    #                                     col_dep_var = col_dep_var,
    #                                     mean_y = mean_y,
    #                                     tbl_used = tbl_used,
    #                                     i = i))[0]["abs"]
    # max_sq = max(sq)
    # epsilon = 0.001
    # effective_alpha = alpha + (1 - alpha) * epsilon
    # largest = (max(xy)/ float(row_num) + epsilon * max_sq) / effective_alpha
    largest = 1e5
    if lambda_value == 0.:
        smallest = 0.001 * largest
    else:
        smallest = lambda_value
    step = math.log(largest / smallest) / (float(num_steps) - 1)
    seq = range(num_steps)
    seq.reverse()
    for i in range(num_steps):
        seq[i] = math.exp(seq[i] * step + math.log(smallest))
    if lambda_value == 0:
        seq.append(0)

    return seq

# ========================================================================

def __compute_average_sq(**args):
    """
    Compute the average squares of all features, used to estimtae the largest lambda
    Actually only the largest value is used, so order does not matter here
    """
    sq = [1] * args["dimension"]
    if args["normalization"] is False:
        for i in range(args["dimension"]):
            sq[i] = (args["x_scales"]["std"][i])**2 + (args["x_scales"]["mean"][i])**2

    return sq

# ========================================================================

def __compute_log_likelihood (coef, coef_str, intercept, **args):
    """
    Compute the log-likelihood at the end of calculation
    """
    if args["family"] == "gaussian": # linear models
        loss = plpy.execute(
            """
            select
                avg(({col_dep_var_new} - {schema_madlib}.elastic_net_gaussian_predict(
                                                                    '{coefficients}'::double precision[],
                                                                    {intercept}::double precision,
                                                                    {col_ind_var_new}))^2) / 2.
                    as loss
            from
                {tbl_used}
            """.format(coefficients = coef_str,
                       intercept = intercept,
                       **args))[0]["loss"]


    elif args["family"] == "binomial": # logistic models
        loss = plpy.execute(
            """
            select
                avg({schema_madlib}.__elastic_net_binomial_loglikelihood(
                                    '{coefficients}'::double precision[],
                                    {intercept},
                                    {col_dep_var_new},
                                    {col_ind_var_new}))
                    as loss
            from {tbl_used}
            """.format(coefficients = coef_str,
                       intercept = intercept,
                       **args))[0]["loss"]

    module_1 = sum(x*x for x in coef)
    module_2 = sum(map(abs, coef))

    log_likelihood = -(loss + args["lambda_value"] * ((1 - args["alpha"]) \
                        * module_1 / 2. + args["alpha"] * module_2))

    return log_likelihood

# ========================================================================

def __elastic_net_validate_args(tbl_source, col_ind_var, col_dep_var,
                                tbl_result, lambda_value, alpha,
                                normalization, max_iter, tolerance):
    if (tbl_source is None or col_ind_var is None or col_dep_var is None
        or tbl_result is None or lambda_value is None or alpha is None
        or normalization is None or len(tbl_source) == 0 or len(col_ind_var) == 0
        or len(col_dep_var) == 0 or len(tbl_result) == 0):
        plpy.error("Elastic Net error: You have unsupported NULL/empty value(s) in the arguments!")

    #     plpy.error("Elastic Net error: Data table " + tbl_source + " does not exist!")

    if table_exists(tbl_result):
        plpy.error("Elastic Net error: Output table " + tbl_result + " already exists!")

    # if table_is_empty(tbl_source):
    #     plpy.error("Elastic Net error: Data table " + tbl_source + " is empty!")

    # if not columns_exist_in_table(tbl_source, [col_ind_var, col_dep_var]):
    #     plpy.error("Elastic Net error: Some column does not exist!")

    # if not scalar_col_has_no_null(tbl_source, col_dep_var):
    #     plpy.error("Elastic Net error: Dependent variable has Null values! Please filter out Null values before using this function!")

    # if not array_col_has_same_dimension(tbl_source, col_ind_var):
    #     plpy.error("Elastic Net error: Independent variable arrays have unequal lengths!")

    # if not array_col_has_no_null(tbl_source, col_ind_var):
    #     plpy.error("Elastic Net error: Independent variable arrays have Null values! Please filter out Null values before using this function!")

    if lambda_value < 0:
        plpy.error("Elastic Net error: The regularization parameter lambda cannot be negative!")

    if alpha < 0 or alpha > 1:
        plpy.error("Elastic Net error: The elastic net control parameter alpha must be in [0,1] !")

    if max_iter <= 0:
        plpy.error("Elastic Net error: max_iter must be positive!")

    if tolerance < 0:
        plpy.error("Elastic Net error: tolerance must be positive!")

    return None

## ========================================================================

def __compute_data_scales (args):
    args["x_scales"] = __utils_ind_var_scales(tbl_data = args["tbl_source"], col_ind_var = args["col_ind_var"],
                                              dimension = args["dimension"], schema_madlib = args["schema_madlib"])

    if args["family"] == "binomial":
        args["y_scale"] = dict(mean = 0, std = 1)
    else:
        args["y_scale"] = __utils_dep_var_scale (tbl_data = args["tbl_source"], col_dep_var = args["col_dep_var"])

    args["xmean_str"] = _array_to_string(args["x_scales"]["mean"])

## ========================================================================

def __normalize_data(args):
    """
    Compute the scaling factors for independent and dependent
    variables, and then scale the original data.

    The output is stored in tbl_data_scaled
    """
    __compute_data_scales(args)

    y_decenter = True if args["family"] == "gaussian" else False;

    __utils_normalize_data(y_decenter = y_decenter,
                           tbl_data = args["tbl_source"],
                           col_ind_var = args["col_ind_var"],
                           col_dep_var = args["col_dep_var"],
                           tbl_data_scaled = args["tbl_data_scaled"],
                           col_ind_var_norm_new = args["col_ind_var_norm_new"],
                           col_dep_var_norm_new = args["col_dep_var_norm_new"],
                           schema_madlib = args["schema_madlib"],
                           x_mean_str = args["xmean_str"],
                           x_std_str = _array_to_string(args["x_scales"]["std"]),
                           y_mean = args["y_scale"]["mean"],
                           y_std = args["y_scale"]["std"])

    return None

## ========================================================================

def __tbl_dimension_rownum(tbl_source, col_ind_var):
    """
    Measure the dimension and row number of source data table
    """
        # independent variable array length
    dimension = plpy.execute("""
                             select array_upper({col_ind_var},1) as dimension
                             from {tbl_source} limit 1
                             """.format(tbl_source = tbl_source,
                                        col_ind_var = col_ind_var))[0]["dimension"]
    # total row number of data source table
    row_num = plpy.execute("""
                           select count(*) from {tbl_source}
                           """.format(tbl_source = tbl_source))[0]["count"]

    return (dimension, row_num)

# ========================================================================

def __compute_means (**args):
    """
    Compute the averages of dependent (y) and independent (x) variables
    """
    if args["normalization"]:
        xmean_str = _array_to_string([0] * args["dimension"])
        ymean = 0
        return (xmean_str, ymean)
    else:
        return (args["xmean_str"], args["y_scale"]["mean"])

# ========================================================================

## ========================================================================

class IterationControllerNoTableDrop (IterationController):
    """
    IterationController but without table dropping

    Useful if one wants to use it in cross validation
    where dropping tables in a loop would use up all the locks
    and get "out of memory" error
    """
    ## ------------------------------------------------------------------------

    def __init__(self, rel_args, rel_state, stateType,
                 temporaryTables = True,
                 truncAfterIteration = False,
                 schema_madlib = "MADLIB_SCHEMA_MISSING",
                 verbose = False,
                 **kwargs):
        # Need to call super class's init method to initialize
        # member fields
        IterationController.__init__(self, rel_args, rel_state, stateType,
                                     temporaryTables, truncAfterIteration,
                                     schema_madlib, verbose, **kwargs)
        # self.kwargs["rel_state"] = "pg_temp" + rel_state, but for testing
        # the existence of a table, schema name should be used together
        self.state_exists = plpy.execute("select count(*) from information_schema.tables where table_name = '{0}' and table_schema = 'pg_temp'".format(rel_state))[0]['count'] == 1
        # The current total row number of rel_state table
        if self.state_exists:
            self.state_row_num = plpy.execute("select count(*) from {rel_state}".format(**self.kwargs))[0]["count"]

    ## ------------------------------------------------------------------------

    def update(self, newState):
        """
        Update state of calculation.
        """
        newState = newState.format(iteration = self.iteration, **self.kwargs)
        self.iteration += 1
        if self.state_exists and self.iteration <= self.state_row_num:
            # If the rel_state table already exists, and
            # iteration number is smaller than total row number,
            # use UPDATE instead of append. UP/tmp/madlib.mT37SL/convex/test/cross_validation.sql_in.logDATE does not use
            # extra locks.
            self.runSQL("""
                update {rel_state} set _state = ({newState})
                where _iteration = {iteration}
            """.format(iteration = self.iteration,
                       newState = newState,
                       **self.kwargs))
        else:
            # rel_state table is newly created, and
            # append data to this table
            self.runSQL("""
                INSERT INTO {rel_state}
                    SELECT
                        {iteration},
                        ({newState})
            """.format(iteration = self.iteration,
                       newState = newState,
                       **self.kwargs))

    ## ------------------------------------------------------------------------

    def __enter__(self):
        """
        __enter__ and __exit__ methods are special. They are automatically called
        when using "with" block.
        """
        if self.state_exists is False:
            # create rel_state table when it does not already exist
            IterationController.__enter__(self)
        self.inWith = True
        return self

# ------------------------------------------------------------------------

class IterationControllerTableAppend (IterationControllerNoTableDrop):
    def __init__(self, rel_args, rel_state, stateType,
                 temporaryTables = True,
                 truncAfterIteration = False,
                 schema_madlib = "MADLIB_SCHEMA_MISSING",
                 verbose = False,
                 **kwargs):
        self.kwargs = kwargs
        self.kwargs.update(
            rel_args = rel_args,
            rel_state = rel_state,
            stateType = stateType.format(schema_madlib = schema_madlib),
            schema_madlib = schema_madlib)
        self.temporaryTables = temporaryTables
        self.truncAfterIteration = truncAfterIteration
        self.verbose = verbose
        self.inWith = False
        self.iteration = -1

        self.state_exists = plpy.execute("""
                                         select count(*)
                                         from information_schema.tables
                                         where table_name = '{rel_state}'
                                         """.format(**self.kwargs))[0]['count'] == 1

    ## ------------------------------------------------------------------------

    def update(self, newState):
        """
        Update state of calculation.
        """
        newState = newState.format(iteration = self.iteration, **self.kwargs)
        self.iteration += 1
        self.runSQL("""
                    INSERT INTO {rel_state}
                    SELECT
                        {iteration},
                        ({newState})
                    """.format(iteration = self.iteration,
                               newState = newState,
                               **self.kwargs))
