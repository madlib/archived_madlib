# coding=utf-8

"""@file bayes.py_in

@brief Naive Bayes classification with user-defined smoothing factor (default:
Laplacian smoothing).

@namespace bayes

Naive Bayes: Setup Functions

@internal
    @implementation

    For the Naive Bayes Classification, we need a product over probabilities.
    However, multiplying lots of small numbers can lead to an exponent overflow.
    E.g., multiplying more than 324 numbers at most 0.1 will yield a product of 0
    in machine arithmetic. A safer way is therefore summing logarithms.

    By the IEEE 754 standard, the smallest number representable as
    DOUBLE PRECISION (64bit) is $2^{-1022}$, i.e., approximately 2.225e-308.
    See, e.g., http://en.wikipedia.org/wiki/Double_precision
    Hence, log(x) = log_10(x) for any non-zero DOUBLE PRECISION @f$x \ge -308@f$.

    Note for theorists:
    - Even adding infinitely many \f$ \log_{10}(x)@f$ for @f$0 < x \le 1 \f$ will
      never cause an overflow because addition will have no effect once the sum
      reaches approx $308 * 2^{53}$ (correspnding to the machine precision).

    The functions __get_*_sql are private because we do not want to commit ourselves
    to a particular interface. We might want to be able to change implementation
    details should the need arise.
@endinternal
"""

import plpy

def __get_feature_probs_sql(**kwargs):
    """Return SQL query with columns (class, attr, value, cnt, attr_cnt).

    For class c, attr i, and value a, cnt is #(c,i,a) and attr_cnt is \#i.

    Note that the query will contain a row for every pair (class, value)
    occuring in the training data (so it might also contain rows where
    \#(c,i,a) = 0).

    @param classPriorsSource Relation (class, class_cnt, all_cnt) where
           class is c, class_cnt is \#c, all_cnt is the number of rows in
           \em trainingSource
    @param attrValuesSource Relation (attr, value) containing all distinct
           attribute, value pairs. If omitted, will use __get_attr_values_sql()
    @param attrCountsSource Relation (attr, attr_cnt) where attr is i and
           attr_cnt is \#i. If omitted, will use __get_attr_counts_sql()
    @param trainingSource name of relation containing training data
    @param trainingClassColumn name of column with class
    @param trainingAttrColumn name of column with attributes array
    @param numAttrs Number of attributes to use for classification

    For meanings of \#(c,i,a), \#c, and \#i see the general description of
    \ref bayes.
    """

    if not 'attrValuesSource' in kwargs:
        kwargs.update(dict(
                attrValuesSource = "(" + __get_attr_values_sql(**kwargs) + ")"
            ))
    if not 'attrCountsSource' in kwargs:
        kwargs.update(dict(
                attrCountsSource = "(" + __get_attr_counts_sql(**kwargs) + ")"
            ))

    # {trainingSource} cannot be a subquery, because we use it more than once in
    # our generated SQL.
    return """
        SELECT
            class,
            attr,
            value,
            coalesce(cnt, 0) AS cnt,
            attr_cnt
        FROM
        (
            SELECT *
            FROM
                {classPriorsSource} AS classes
            CROSS JOIN
                {attrValuesSource} AS attr_values
        ) AS required_triples
        LEFT OUTER JOIN
        (
            SELECT
                trainingSource.{trainingClassColumn} AS class,
                generate_series(1, {numAttrs}) AS attr,
                unnest(trainingSource.{trainingAttrColumn}) AS value,
                count(*) AS cnt
            FROM
                {trainingSource} AS trainingSource
            GROUP BY
                class,
                attr,
                value
        ) AS triple_counts
        USING (class, attr, value)
        INNER JOIN
            {attrCountsSource} AS attr_counts
        USING (attr)
        """.format(**kwargs)


def __get_attr_values_sql(**kwargs): # need to filter out NULL values
    """
    Return SQL query with columns (attr, value).

    The query contains a row for each pair that occurs in the training data.

    @param trainingSource Name of relation containing the training data
    @param trainingAttrColumn Name of attributes-array column in training data
    @param numAttrs Number of attributes to use for classification

    @internal
    \par Implementation Notes:
    If PostgreSQL supported count(DISTINCT ...) for window functions, we could
    consolidate this function with __get_attr_counts_sql():
    @verbatim
    [...] count(DISTINCT value) OVER (PARTITION BY attr) [...]
    @endverbatim
    @endinternal

    """

    return """
        SELECT
            generate_series(1, {numAttrs}) AS attr,
            unnest(trainingSource.{trainingAttrColumn}) AS value
        FROM
            {trainingSource} AS trainingSource
        GROUP BY attr, value
        """.format(**kwargs)


def __get_attr_counts_sql(**kwargs):
    """
    Return SQL query with columns (attr, attr_cnt)

    For attr i, attr_cnt is \#i.

    @param trainingSource Name of relation containing the training data
    @param trainingAttrColumn Name of attributes-array column in training data
    @param numAttrs Number of attributes to use for classification

    """

    return """
        SELECT
            attr, count(value) AS attr_cnt
        FROM
        (
            SELECT
                attr, value
            FROM (
                SELECT
                    generate_series(1, {numAttrs}) AS attr,
                    unnest(trainingSource.{trainingAttrColumn}) AS value
                FROM
                    {trainingSource} AS trainingSource
                ) l
            GROUP BY attr, value
        ) m
        GROUP BY attr
        """.format(**kwargs)


def __get_class_priors_sql(**kwargs):
    """
    Return SQL query with columns (class, class_cnt, all_cnt)

    For class c, class_cnt is \#c. all_cnt is the total number of records in the
    training data.

    @param trainingSource Name of relation containing the training data
    @param trainingClassColumn Name of class column in training data

    """

    return """
        SELECT * FROM
            (
            SELECT
                trainingSource.{trainingClassColumn} AS class,
                count(*) AS class_cnt
            FROM {trainingSource} AS trainingSource
            GROUP BY trainingSource.{trainingClassColumn}
            ) l
        CROSS JOIN
            (
            SELECT
                count(*) AS all_cnt
            FROM {trainingSource} AS trainingSource
            ) m
        """.format(**kwargs)


def __get_keys_and_prob_values_sql(**kwargs):
    """
    Return SQL query with columns (key, class, log_prob).

    For class c and the attribute array identified by key k, log_prob is
    log( P(C = c) * P(A = a(k)[] | C = c) ).

    For each key k and class c, the query also contains a row (k, c, NULL). This
    is for technical reasons (we want every key-class pair to appear in the
    query. NULL serves as a default value if there is insufficient training data
    to compute a probability value).

    @param numAttrs Number of attributes to use for classification
    @param classifySource Name of the relation that contains data to be classified
    @param classifyKeyColumn Name of column in \em classifySource that can
           serve as unique identifier
    @param classifyAttrColumn Name of attributes-array column in \em classifySource
    @param classPriorsSource
           Relation (class, class_cnt, all_cnt) where
           class is c, class_cnt is \#c, all_cnt is the number of training
           samples.
    @param featureProbsSource
           Relation (class, attr, value, cnt, attr_cnt) where
           (class, attr, value) = (c,i,a), cnt = \#(c,i,a), and attr_cnt = \#i
    @param smoothingFactor Smoothing factor for computing feature
           feature probabilities. Default value: 1.0 (Laplacian Smoothing).

    """

    # {classifySource} cannot be a subquery, because we use it more than once in
    # our generated SQL.
    return """
    SELECT t1.key, t1.class,
    CASE WHEN {smoothingFactor} = 0 THEN
      t1.log_prob
    ELSE
      -- log_prob is separated into two parts:
      --     The following part of log_prob is for denominators, whose number should always be {numAttrs}.
      -- A strange thing: When all of this query is put into a function definition, all branches of "case" will be kind of evaluated
      --     and if {smoothingFactor} is 0 at this point, we will have "logrithm of zero" error. Since we are going to re-use
      --     this code in __get_prob_values_sql, where everything will be put into a function definition, we have to use
      --     a small trick to avoid this error: use "case when $2=0 then 1 else $2 end" inside log(), which is "sf" in the following.
      --     This happens on both GPDB and PG
     (t1.log_prob + sum(log(sf / (classPriors.class_cnt + ({smoothingFactor}::DOUBLE PRECISION) * attrCnts.attr_cnt))) )
    END
    AS log_prob
    FROM
    (
        SELECT
            classify.key,
            classPriors.class,
            CASE WHEN count(*) < {numAttrs} AND {smoothingFactor} = 0 -- smoothing factor < 0 case is dealt with in
                                                                      --     create_classification_function
                                                                      --     which is the only place that smoothing
                                                                      --     factor can be different from 1
                 THEN
                    NULL
                 WHEN {smoothingFactor} = 0 THEN -- train set has all the values
                       log(classPriors.class_cnt::DOUBLE PRECISION / classPriors.all_cnt)
                     + sum( log(featureProbs.cnt::DOUBLE PRECISION / classPriors.class_cnt) )
                 ELSE
                      -- log_prob is separated into two parts:
                      --     The part here is mainly for numerators, whose number might be smaller
                      --     than {numAttrs} if the test set has values that are not in the training
                      --     set. The prior probability is also included.
                        log(classPriors.class_cnt::DOUBLE PRECISION / classPriors.all_cnt)
                    + sum( log(featureProbs.cnt::DOUBLE PRECISION / sf + 1) )
            END
            AS log_prob
        FROM
            {featureProbsSource} AS featureProbs,
            {classPriorsSource} AS classPriors,
            (
                SELECT
                    classifySource.{classifyKeyColumn} AS key,
                    generate_series(1, {numAttrs}) AS attr,
                    unnest(classifySource.{classifyAttrColumn}) AS value
                FROM
                    {classifySource} AS classifySource
            ) AS classify,
            (select case when {smoothingFactor}=0 then 1 else {smoothingFactor} end as sf) as foo
        WHERE
            featureProbs.class = classPriors.class AND
            featureProbs.attr = classify.attr AND
            featureProbs.value = classify.value AND
            (featureProbs.cnt > 0) 
        GROUP BY
            classify.key, classPriors.class, classPriors.class_cnt, classPriors.all_cnt
    ) AS t1,
        (select distinct class, attr, attr_cnt from {featureProbsSource}) AS attrCnts,
        {classPriorsSource} AS classPriors,
        (select case when {smoothingFactor}=0 then 1 else {smoothingFactor} end as sf) as foo
    WHERE
        classPriors.class = t1.class AND
        attrCnts.class = t1.class
    GROUP BY
        t1.key, t1.class, t1.log_prob
    """.format(**kwargs)


def __get_prob_values_sql(**kwargs):
    """
    Return SQL query with columns (class, log_prob), given an array of
    attributes.

    The query binds to an attribute array a[]. For every class c, log_prob
    is log( P(C = c) * P(A = a[] | C = c) ).

    @param classifyAttrColumn Array of attributes to bind to. This can be
           a column name of an outer query or a literal.
    @param smoothingFactor Smoothing factor to use for estimating the feature
           probabilities.
    @param numAttrs Number of attributes to use for classification
    @param classPriorsSource
           Relation (class, class_cnt, all_cnt) where
           class is c, class_cnt is \#c, all_cnt is the number of training
           samples.
    @param featureProbsSource
           Relation (class, attr, value, cnt, attr_cnt) where
           (class, attr, value) = (c,i,a), cnt = \#(c,i,a), and attr_cnt = \#i

    Note that unless \em classifyAttrColumn is a literal, the SQL query will
    become a correlated subquery and will not work in Greenplum.

    """
    # Re-use the computation code from __get_keys_and_prob_values_sql
    # We just need to attach a classifyKeyColumn to the array
    # since there is only one row, we can set 1 as the key column
    kwargs.update(
        classifySource = "(SELECT 1 AS id, {classifyAttrColumn} AS attrColumn)".format(**kwargs),
        classifyKeyColumn = "id",
        classifyAttrColumn = "attrColumn"
    )

    return __get_keys_and_prob_values_sql(**kwargs)

def __get_classification_sql(**kwargs):
    """
    Return SQL query with columns (key, nb_classification, nb_log_probability)

    @param keys_and_prob_values Relation (key, class, log_prob)

    """

    kwargs.update(
        keys_and_prob_values = "(" + __get_keys_and_prob_values_sql(**kwargs) + ")"
    )
    return """
        SELECT
            key,
            {schema_madlib}.argmax(class, log_prob) AS nb_classification,
            max(log_prob) AS nb_log_probability
        FROM {keys_and_prob_values} AS keys_and_nb_values
        GROUP BY key
        """.format(**kwargs)

def create_prepared_data_table(**kwargs):
    """Wrapper around create_prepared_data() to create a table (and not a view)
    """
    kwargs.update(whatToCreate = 'TABLE')
    create_prepared_data(**kwargs)

def create_prepared_data(**kwargs):
    """Precompute all class priors and feature probabilities.

    When the precomputations are stored in a table, this function will create
    indices that speed up lookups necessary for Naive Bayes classification.
    Moreover, it runs ANALYZE on the new tables to allow for optimized query
    plans.

    Class priors are stored in a relation with columns
    (class, class_cnt, all_cnt).

    @param trainingSource Name of relation containing the training data
    @param trainingClassColumn Name of class column in training data
    @param trainingAttrColumn Name of attributes-array column in training data
    @param numAttrs Number of attributes to use for classification

    @param whatToCreate (Optional) Either \c 'TABLE' OR \c 'VIEW' (the default).
    @param classPriorsDestName Name of class-priors relation to create
    @param featureProbsDestName Name of feature-probabilities relation to create

    """
    if not kwargs.has_key('numAttrs'):
        plpy.error("'numAttrs' must be provided")

    if not kwargs.has_key('trainingSource'):
        plpy.error("'trainingSource' must be provided")

    if not kwargs.has_key('trainingAttrColumn'):
        plpy.error("'trainingAttrColumn' must be provided")

    __verify_attr_num(
        kwargs["trainingSource"],
        kwargs["trainingAttrColumn"],
        kwargs["numAttrs"])

    oldMsgLevel = plpy.execute("SELECT setting FROM pg_settings WHERE name='client_min_messages'")[0]['setting']
    
    if kwargs['whatToCreate'] == 'TABLE':
        # FIXME: ANALYZE is not portable.
        kwargs.update(dict(
            attrCountsSource = '_madlib_nb_attr_counts',
            attrValuesSource = '_madlib_nb_attr_values'
        ))
        plpy.execute("""
            SET client_min_messages = error;

            DROP TABLE IF EXISTS {attrCountsSource};
            CREATE TEMPORARY TABLE {attrCountsSource}
            AS
            {attr_counts_sql};
            ALTER TABLE {attrCountsSource} ADD PRIMARY KEY (attr);
            ANALYZE {attrCountsSource};

            DROP TABLE IF EXISTS {attrValuesSource};
            CREATE TEMPORARY TABLE {attrValuesSource}
            AS
            {attr_values_sql};
            ALTER TABLE {attrValuesSource} ADD PRIMARY KEY (attr, value);
            ANALYZE {attrValuesSource};

            SET client_min_messages = {oldMsgLevel};
            """.format(
                attrCountsSource = kwargs['attrCountsSource'],
                attrValuesSource = kwargs['attrValuesSource'],
                attr_counts_sql = "(" + __get_attr_counts_sql(**kwargs) + ")",
                attr_values_sql = "(" + __get_attr_values_sql(**kwargs) + ")",
                oldMsgLevel = oldMsgLevel
                )
            )


    kwargs.update(dict(
            sql = __get_class_priors_sql(**kwargs)
        ))

    kwargs.update(oldMsgLevel = oldMsgLevel)
    
    plpy.execute("""
        CREATE {whatToCreate} {classPriorsDestName}
        AS
        {sql}
        """.format(**kwargs)
        )
    if kwargs['whatToCreate'] == 'TABLE':
        plpy.execute("""
            SET client_min_messages = error;
            ALTER TABLE {classPriorsDestName} ADD PRIMARY KEY (class);
            ANALYZE {classPriorsDestName};
            SET client_min_messages = {oldMsgLevel};
            """.format(**kwargs))

    kwargs.update(dict(
            classPriorsSource = kwargs['classPriorsDestName']
        ))
    kwargs.update(dict(
            sql = __get_feature_probs_sql(**kwargs)
        ))
    plpy.execute("""
        CREATE {whatToCreate} {featureProbsDestName} AS
        {sql}
        """.format(**kwargs)
        )
    
    if kwargs['whatToCreate'] == 'TABLE':
        plpy.execute("""
            SET client_min_messages = error;
                     
            ALTER TABLE {featureProbsDestName} ADD PRIMARY KEY (class, attr, value);
            ANALYZE {featureProbsDestName};
            DROP TABLE {attrCountsSource};
            DROP TABLE {attrValuesSource};
                     
            SET client_min_messages = {oldMsgLevel};      
            """.format(**kwargs))


def create_classification_view(**kwargs):
    """Wrapper around create_classification() to create a view (and not a table)
    """
    kwargs.update(whatToCreate = 'VIEW')
    create_classification(**kwargs)


def create_classification(**kwargs):
    """
    Create a view/table with columns (key, nb_classification).

    The created relation will be

    <tt>{TABLE|VIEW} <em>destName</em> (key, nb_classification)</tt>

    where \c nb_classification is an array containing the most likely
    class(es) of the record in \em classifySource identified by \c key.

    There are two sets of arguments this function can be called with. The
    following parameters are always needed:
    @param numAttrs Number of attributes to use for classification
    @param destName Name of the table or view to create
    @param whatToCreate (Optional) Either \c 'TABLE' OR \c 'VIEW' (the default).
    @param smoothingFactor (Optional) Smoothing factor for computing feature
           feature probabilities. Default value: 1.0 (Laplacian Smoothing).
    @param classifySource Name of the relation that contains data to be classified
    @param classifyKeyColumn Name of column in \em classifySource that can
           serve as unique identifier
    @param classifyAttrColumn Name of attributes-array column in \em classifySource

    Furthermore, provide either:
    @param classPriorsSource
           Relation (class, class_cnt, all_cnt) where
           class is c, class_cnt is \#c, all_cnt is the number of training
           samples.
    @param featureProbsSource
           Relation (class, attr, value, cnt, attr_cnt) where
           (class, attr, value) = (c,i,a), cnt = \#(c,i,a), and attr_cnt = \#i

    Or have this function operate on the "raw" training data:
    @param trainingSource
           Name of relation containing the training data
    @param trainingClassColumn
           Name of class column in training data
    @param trainingAttrColumn
           Name of attributes-array column in \em trainingSource

    """

    __init_prepared_data(kwargs)

    if kwargs.has_key('trainingSource') <> kwargs.has_key('trainingAttrColumn'):
        plpy.error("'trainingSource' and 'trainingAttrColumn' must be provided together")

    if not kwargs.has_key('numAttrs'):
        plpy.error("'numAttrs' must be provided")

    if 'trainingSource' in kwargs:
        __verify_attr_num(
            kwargs["trainingSource"],
            kwargs["trainingAttrColumn"],
            kwargs["numAttrs"])

    if not kwargs.has_key('classifySource'):
        plpy.error("'classifySource' must be provided")

    if not kwargs.has_key('classifyAttrColumn'):
        plpy.error("'classifyAttrColumn' must be provided")

    __verify_attr_num(
        kwargs["classifySource"],
        kwargs["classifyAttrColumn"],
        kwargs["numAttrs"])

    kwargs.update(
        keys_and_prob_values = "(" + __get_keys_and_prob_values_sql(**kwargs) + ")"
        )
    plpy.execute("""
        CREATE {whatToCreate} {destName} AS
        SELECT
            key,
            {schema_madlib}.argmax(class, log_prob) AS nb_classification
        FROM {keys_and_prob_values} AS keys_and_nb_values
        GROUP BY key
        """.format(**kwargs))


def create_bayes_probabilities_view(**kwargs):
    """Wrapper around create_bayes_probabilities() to create a view (and not a table)
    """
    kwargs.update(whatToCreate = 'VIEW')
    create_bayes_probabilities(**kwargs)

def create_bayes_probabilities(**kwargs):
    """Create table/view with columns (key, class, nb_prob)

    The created relation will be

    <tt>{TABLE|VIEW} <em>destName</em> (key, class, nb_prob)</tt>

    where \c nb_prob is the Naive-Bayes probability that \c class is the true
    class of the record in \em classifySource identified by \c key.

    There are two sets of arguments this function can be called with. The
    following parameters are always needed:
    @param numAttrs Number of attributes to use for classification
    @param destName Name of the table or view to create
    @param whatToCreate (Optional) Either \c 'TABLE' OR \c 'VIEW' (the default).
    @param smoothingFactor (Optional) Smoothing factor for computing feature
           feature probabilities. Default value: 1.0 (Laplacian Smoothing).

    Furthermore, provide either:
    @param classPriorsSource
           Relation (class, class_cnt, all_cnt) where
           class is c, class_cnt is \#c, all_cnt is the number of training
           samples.
    @param featureProbsSource
           Relation (class, attr, value, cnt, attr_cnt) where
           (class, attr, value) = (c,i,a), cnt = \#(c,i,a), and attr_cnt = \#i

    Or have this function operate on the "raw" training data:
    @param trainingSource
           Name of relation containing the training data
    @param trainingClassColumn
           Name of class column in training data
    @param trainingAttrColumn
           Name of attributes-array column in training data

    @internal
    \par Implementation Notes:

    We have two numerical problems when copmuting the probabilities
    @verbatim
               P(C = c) * P(A = a | C = c)
 P(C = c) = ---------------------------------    (*)
            --
            \   P(C = c') * P(A = a | C = c')
            /_
              c'
                         __
where P(A = a | C = c) = ||  P(A_i = a_i | C = c).
                           i
    @endverbatim

    1. P(A = a | C = c) could be a very small number not representable in
       double-precision floating-point arithmetic.
       - Solution: We have log( P(C = c) * P(A = a | C = c) ) as indermediate
         results. We will add the maximum absolute value of these intermediate
         results to all of them. This corresponds to multiplying numerator and
         denominator of (*) with the same factor. The "normalization" ensures
         that the numerator of (*) can never be 0 (in FP arithmetic) for all c.

    2. PostgreSQL raises an error in case of underflows, even when 0 is the
       desirable outcome.
       - Solution: if log_10 ( P(A = a | C = c) ) < -300, we interprete
         P(A = a | C = c) = 0. Note here that 1e-300 is roughly in the order of
         magnitude of the smallest double precision FP number.
    @endinternal
    """

    __init_prepared_data(kwargs)

    if kwargs.has_key('trainingSource') <> kwargs.has_key('trainingAttrColumn'):
        plpy.error("'trainingSource' and 'trainingAttrColumn' must be provided together")

    if not kwargs.has_key('numAttrs'):
        plpy.error("'numAttrs' must be provided")

    if 'trainingSource' in kwargs:
        __verify_attr_num(
            kwargs["trainingSource"],
            kwargs["trainingAttrColumn"],
            kwargs["numAttrs"])

    if not kwargs.has_key('classifySource'):
        plpy.error("'classifySource' must be provided")

    if not kwargs.has_key('classifyAttrColumn'):
        plpy.error("'classifyAttrColumn' must be provided")

    __verify_attr_num(
        kwargs["classifySource"],
        kwargs["classifyAttrColumn"],
        kwargs["numAttrs"])

    kwargs.update(dict(
        keys_and_prob_values = "(" + __get_keys_and_prob_values_sql(**kwargs) + ")"
        ))
    plpy.execute("""
        CREATE {whatToCreate} {destName} AS
        SELECT
            key,
            class,
            nb_prob / sum(nb_prob) OVER (PARTITION BY key) AS nb_prob
        FROM
        (
            SELECT
                key,
                class,
                CASE WHEN max(log_prob) - max(max(log_prob)) OVER (PARTITION BY key) < -300 THEN 0
                     ELSE pow(10, max(log_prob) - max(max(log_prob)) OVER (PARTITION BY key))
                END AS nb_prob
            FROM
                {keys_and_prob_values} AS keys_and_nb_values
            GROUP BY
                key, class
        ) AS keys_and_nb_values
        ORDER BY
            key, class
        """.format(**kwargs))


def create_classification_function(**kwargs):
    """Create a SQL function mapping arrays of attribute values to the Naive
    Bayes classification.

    The created SQL function will be:

    <tt>
    FUNCTION <em>destName</em> (attributes INTEGER[], smoothingFactor DOUBLE PRECISION)
    RETURNS INTEGER[]</tt>

    There are two sets of arguments this function can be called with. The
    following parameters are always needed:
    @param classifyAttrColumn Array of attributes to bind to. This can be
           a column name of an outer query or a literal.
    @param smoothingFactor Smoothing factor to use for estimating the feature
           probabilities.
    @param numAttrs Number of attributes to use for classification

    Furthermore, provide either:
    @param classPriorsSource
           Relation (class, class_cnt, all_cnt) where
           class is c, class_cnt is \#c, all_cnt is the number of training
           samples.
    @param featureProbsSource
           Relation (class, attr, value, cnt, attr_cnt) where
           (class, attr, value) = (c,i,a), cnt = \#(c,i,a), and attr_cnt = \#i

    Or have this function operate on the "raw" training data:
    @param trainingSource Name of relation containing the training data
    @param trainingClassColumn Name of class column in training data
    @param trainingAttrColumn Name of attributes-array column in training data

    Note: Greenplum does not support executing STABLE and VOLATILE functions on
    segments. The created function can therefore only be called on the master.
    """
    kwargs.update(dict(
        classifyAttrColumn = "$1",
        smoothingFactor = "$2"
        ))
    
    __init_prepared_data(kwargs)

    kwargs.update(
        keys_and_prob_values = "(" + __get_prob_values_sql(**kwargs) + ")"
        )

    oldMsgLevel = plpy.execute("SELECT setting FROM pg_settings WHERE name='client_min_messages'")[0]['setting']
    kwargs.update(oldMsgLevel = oldMsgLevel)
    
    plpy.execute("""
        CREATE FUNCTION {destName} (inAttributes INTEGER[], inSmoothingFactor DOUBLE PRECISION)
        RETURNS INTEGER[] AS
        $$
        DECLARE rst INTEGER[];
        BEGIN
            SET client_min_messages = error;
                 
            IF inSmoothingFactor < 0 THEN
                 RAISE EXCEPTION 'Smoothing factor cannot be smaller than 0!';
            END IF;

            DROP TABLE IF EXISTS __inter_results_bayes;
            CREATE TEMP TABLE __inter_results_bayes AS (SELECT * FROM {keys_and_prob_values} AS key_and_nb_values);

            IF (SELECT count(*)>0 FROM __inter_results_bayes WHERE log_prob is NULL) THEN
                 RAISE EXCEPTION 'Smoothing factor cannot be 0 if test set has values not in training set!';
            END IF;
             
            SELECT
                {schema_madlib}.argmax(class, log_prob)
            FROM __inter_results_bayes INTO rst;

            SET client_min_messages = {oldMsgLevel};
                 
            RETURN rst;
        END;
        $$
        LANGUAGE plpgsql
        """.format(**kwargs))


def __init_prepared_data(kwargs):
    """
    Fill in values for optional parameters: Create subqueries instead of using
    a relation.

    """

    if not 'classPriorsSource' in kwargs:
        kwargs.update(dict(
                classPriorsSource = "(" + __get_class_priors_sql(**kwargs) + ")"
            ))
    if not 'featureProbsSource' in kwargs:
        kwargs.update(dict(
                featureProbsSource = "(" + __get_feature_probs_sql(**kwargs) + ")"
            ))
    if not 'smoothingFactor' in kwargs:
        kwargs.update(dict(
                smoothingFactor = 1
            ))

def __verify_attr_num(sourceTable, attrColumn, numAttr):
    """
    Verify the number of attributes for each record is as expected.
    """
    result = plpy.execute("""
        SELECT count(*) size FROM {0}
        WHERE array_upper({1}, 1) <> {2}
        """.format(sourceTable, attrColumn, numAttr)
        )

    dsize = result[0]['size']
    if (dsize <> 0):
        plpy.error('found %d records in "%s" where "%s" was not of expected length (%d)'\
            % (dsize, sourceTable, attrColumn, numAttr))
