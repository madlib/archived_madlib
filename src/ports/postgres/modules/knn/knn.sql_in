/* ----------------------------------------------------------------------- *//**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 *//* ----------------------------------------------------------------------- */


/* ----------------------------------------------------------------------- *//**
 *
 * @file knn.sql_in
 * @brief Set of functions for k-nearest neighbors.
 * @sa For a brief introduction to k-nearest neighbors algorithm for regression and classification,
 * see the module description \ref grp_knn.
 *
 *
 *//* ----------------------------------------------------------------------- */

m4_include(`SQLCommon.m4')


/**
@addtogroup grp_knn

<div class="toc"><b>Contents</b>
<ul>
<li class="level1"><a href="#knn">K-Nearest Neighbors</a></li>
<li class="level1"><a href="#usage">Usage</a></li>
<li class="level1"><a href="#output">Output Format</a></li>
<li class="level1"><a href="#examples">Examples</a></li>
<li class="level1"><a href="#background">Technical Background</a></li>
<li class="level1"><a href="#literature">Literature</a></li>
</ul>
</div>

@brief Finds k nearest data points to the given data point and outputs majority vote value of output classes for classification, and average value of target values for regression.

\warning <em> This MADlib method is still in early stage development. There may be some
issues that will be addressed in a future version. Interface and implementation
are subject to change. </em>

@anchor knn

K-nearest neighbors is a method for finding the k closest points to a
given data point in terms of a given metric. Its input consists of
data points as features from testing examples, and it
looks for k closest points in the training set for each of the data
points in test set.  The output of KNN depends on the type of task.
For classification, the output is the majority vote of the classes of
the k nearest data points. That is, the testing example gets assigned the
most popular class from the nearest neighbors.
For regression, the output is the average of the values of k nearest
neighbors of the given test point.

@anchor usage
@par Usage
<pre class="syntax">
knn( point_source,
     point_column_name,
     point_id,
     label_column_name,
     test_source,
     test_column_name,
     test_id,
     output_table,
     k,
     output_neighbors
   )
</pre>

\b Arguments
<dl class="arglist">
<dt>point_source</dt>
<dd>TEXT. Name of the table containing the training data points.

Training data points are expected to be stored row-wise
in a column of type <tt>DOUBLE PRECISION[]</tt>.
</dd>

<dt>point_column_name</dt>
<dd>TEXT. Name of the column with training data points.</dd>

<dt>point_id</dt>
<dd>TEXT. Name of the column in 'point_sourceâ€™ containing source data ids.
The ids are of type INTEGER with no duplicates. They do not need to be contiguous. 
This parameter must be used if the list of nearest neighbors are to be output, i.e., 
if the parameter 'output_neighbors' below is TRUE or if 'label_column_name' is NULL.

<dt>label_column_name</dt>
<dd>TEXT. Name of the column with labels/values of training data points.
If Boolean, integer or text types will run knn classification, else if 
double precision values will run knn regression.  
If you set this to NULL will return neighbors only without doing classification or regression.</dd>

<dt>test_source</dt>
<dd>TEXT. Name of the table containing the test data points.

Testing data points are expected to be stored row-wise
in a column of type <tt>DOUBLE PRECISION[]</tt>.
</dd>

<dt>test_column_name</dt>
<dd>TEXT. Name of the column with testing data points.</dd>

<dt>test_id</dt>
<dd>TEXT. Name of the column having ids of data points in test data table.</dd>

<dt>output_table</dt>
<dd>TEXT. Name of the table to store final results.</dd>

<dt>operation</dt>
<dd>TEXT. Type of task: 'r' for regression and 'c' for classification.</dd>

<dt>k (optional)</dt>
<dd>INTEGER. default: 1. Number of nearest neighbors to consider.
For classification, should be an odd number to break ties.
otherwise result may depend on ordering of the input data.</dd>

<dt>output_neighbors (optional) </dt>
<dd>BOOLEAN default: FALSE. Outputs the list of k-nearest 
neighbors that were used in the voting/averaging.</dd>

</dl>


@anchor output
@par Output Format

The output of the KNN module is a table with the following columns:
<table class="output">
    <tr>
        <th>id</th>
        <td>INTEGER. The ids of test data points.</td>
    </tr>
    <tr>
        <th>test_column_name</th>
        <td>DOUBLE PRECISION[]. The test data points.</td>
    </tr>
    <tr>
        <th>prediction</th>
        <td>INTEGER. Label in case of classification, average value in case of regression.</td>
    </tr>
</table>


@anchor examples
@examp

-#  Prepare some training data for classification:
<pre class="example">
DROP TABLE IF EXISTS knn_train_data;
CREATE TABLE knn_train_data (
                    id integer, 
                    data integer[], 
                    label integer
                    );
INSERT INTO knn_train_data VALUES
(1, '{1,1}', 1),
(2, '{2,2}', 1),
(3, '{3,3}', 1),
(4, '{4,4}', 1),
(5, '{4,5}', 1),
(6, '{20,50}', 0),
(7, '{10,31}', 0),
(8, '{81,13}', 0),
(9, '{1,111}', 0);
</pre>

-#  Prepare some training data for regression:
<pre class="example">
DROP TABLE IF EXISTS knn_train_data_reg;
CREATE TABLE knn_train_data_reg (
                    id integer, 
                    data integer[], 
                    label float
                    );
INSERT INTO knn_train_data_reg VALUES
(1, '{1,1}', 1.0),
(2, '{2,2}', 1.0),
(3, '{3,3}', 1.0),
(4, '{4,4}', 1.0),
(5, '{4,5}', 1.0),
(6, '{20,50}', 0.0),
(7, '{10,31}', 0.0),
(8, '{81,13}', 0.0),
(9, '{1,111}', 0.0);
</pre>

-#  Prepare some testing data:
<pre class="example">
DROP TABLE IF EXISTS knn_test_data;
CREATE TABLE knn_test_data (
                    id integer, 
                    data integer[]
                    );
INSERT INTO knn_test_data VALUES
(1, '{2,1}'),
(2, '{2,6}'),
(3, '{15,40}'),
(4, '{12,1}'),
(5, '{2,90}'),
(6, '{50,45}');
</pre>

-#  Run KNN for classification:
<pre class="example">
DROP TABLE IF EXISTS madlib_knn_result_classification;
SELECT * FROM madlib.knn( 
                'knn_train_data',      -- Table of training data
                'data',                -- Col name of training data
                'id',                  -- Col Name of id in train data 
                'label',               -- Training labels
                'knn_test_data',       -- Table of test data
                'data',                -- Col name of test data
                'id',                  -- Col name of id in test data 
                'madlib_knn_result_classification',  -- Output table
                 3                     -- Number of nearest neighbours
                 True                  -- True if you want to show Nearest-Neighbors, False otherwise
                );
SELECT * from madlib_knn_result_classification ORDER BY id;
</pre>
Result:
<pre class="result">
 id |  data   | prediction | k_nearest_neighbours 
----+---------+------------+----------------------
  1 | {2,1}   |          1 | {1,2,3}
  2 | {2,6}   |          1 | {5,4,3}
  3 | {15,40} |          0 | {7,6,5}
  4 | {12,1}  |          1 | {4,5,3}
  5 | {2,90}  |          0 | {9,6,7}
  6 | {50,45} |          0 | {6,7,8}
(6 rows)
</pre>

-#  Run KNN for regression:
<pre class="example">
DROP TABLE IF EXISTS madlib_knn_result_regression;
SELECT * FROM madlib.knn( 
                'knn_train_data_reg',  -- Table of training data
                'data',                -- Col name of training data
                'id',                  -- Col Name of id in train data 
                'label',               -- Training labels
                'knn_test_data',       -- Table of test data
                'data',                -- Col name of test data
                'id',                  -- Col name of id in test data 
                'madlib_knn_result_regression',  -- Output table
                 3,                    -- Number of nearest neighbours
                True                   -- True if you want to show Nearest-Neighbors, False otherwise
                );
SELECT * from madlib_knn_result_regression ORDER BY id;
</pre>
Result:
<pre class="result">
 id |  data   |    prediction     | k_nearest_neighbours 
----+---------+-------------------+----------------------
  1 | {2,1}   |                 1 | {1,2,3}
  2 | {2,6}   |                 1 | {5,4,3}
  3 | {15,40} | 0.333333333333333 | {7,6,5}
  4 | {12,1}  |                 1 | {4,5,3}
  5 | {2,90}  |                 0 | {9,6,7}
  6 | {50,45} |                 0 | {6,7,8}
(6 rows)
</pre>

@anchor background
@par Technical Background

The training data points are vectors in a multidimensional feature space,
each with a class label. The training phase of the algorithm consists
only of storing the feature vectors and class labels of the training points.

In the classification phase, k is a user-defined constant, and an unlabeled
vector (a test point) is classified by assigning the label which is most
frequent among the k training samples nearest to that test point.
In case of regression, average of the values of these k training samples
is assigned to the test point.
The only distance metric supported in this version is MADlib's squared_dist_norm2.
Other distance metrics will be added in a future release of this module.


@anchor literature
@literature

@anchor knn-lit-1
[1] Wikipedia, k-nearest neighbors algorithm,
    https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm

@anchor knn-lit-2
[2] N. S. Altman: An Introduction to Kernel and Nearest-Neighbor Nonparametric Regression
    http://www.stat.washington.edu/courses/stat527/s13/readings/Altman_AmStat_1992.pdf

@anchor knn-lit-3
[3] Gongde Guo1, Hui Wang, David Bell, Yaxin Bi, Kieran Greer: KNN Model-Based Approach in Classification,
    https://ai2-s2-pdfs.s3.amazonaws.com/a7e2/814ec5db800d2f8c4313fd436e9cf8273821.pdf

@internal
@sa namespace knn (documenting the implementation in Python)
@endinternal
*/

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__knn_validate_src(
    point_source VARCHAR,
    point_column_name VARCHAR,
    label_column_name VARCHAR,
    test_source VARCHAR,
    test_column_name VARCHAR,
    test_id VARCHAR,
    output_table VARCHAR,
    operation VARCHAR,
    k INTEGER
) RETURNS INTEGER AS $$
    PythonFunctionBodyOnly(`knn', `knn')
    return knn.knn_validate_src(
        schema_madlib,
        point_source,
        point_column_name,
        label_column_name,
        test_source,
        test_column_name,
        test_id,
        output_table,
        operation,
        k
    )
$$ LANGUAGE plpythonu VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA', `');


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.knn(
    arg1 VARCHAR
) RETURNS VOID AS $$
BEGIN
    IF arg1 = 'help' OR arg1 = 'usage' OR arg1 = '?' THEN
    RAISE NOTICE
'
-----------------------------------------------------------------------
                            USAGE
-----------------------------------------------------------------------
SELECT {schema_madlib}.knn(
    point_source,       -- Training data table having training features as vector column and labels
    point_column_name,  -- Name of column having feature vectors in training data table
    point_id,           -- Name of column having feature vector Ids in train data table
    label_column_name,  -- Name of column having actual label/vlaue for corresponding feature vector in training data table
    test_source,        -- Test data table having features as vector column. Id of features is mandatory
    test_column_name,   -- Name of column having feature vectors in test data table
    test_id,     -- Name of column having feature vector Ids in test data table
    output_table,       -- Name of output table
    k,                  -- value of k. Default will go as 1
    output_neighbors    -- Outputs the list of k-nearest neighbors that were used in the voting/averaging.
    );

-----------------------------------------------------------------------
                            OUTPUT
-----------------------------------------------------------------------
The output of the KNN module is a table with the following columns:

id                  The ids of test data points.
test_column_name    The test data points.
prediction          The output of KNN- label in case of classification, average value in case of regression.
k_nearest_neighbours The list of k-nearest neighbors that were used in the voting/averaging.
';
    END IF;
END;
$$ LANGUAGE plpgsql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `READS SQL DATA', `');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.knn(
) RETURNS VOID AS $$
BEGIN
    RAISE NOTICE '
k-Nearest Neighbors is a method for finding k closest points to a given data
point in terms of a given metric. Its input consist of data points as features
from testing examples. For a given k, it looks for k closest points in
training set for each of the data points in test set. Algorithm generates one
output per testing example. The output of KNN depends on the type of task:
For Classification, the output is majority vote of the classes of the k
nearest data points. The testing example gets assigned the most popular class
among nearest neighbors. For Regression, the output is average of the values
of k nearest neighbors of the given testing example.
    ';
END;
$$ LANGUAGE plpgsql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `READS SQL DATA', `');



CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.knn(
    point_source VARCHAR,
    point_column_name VARCHAR,
    point_id VARCHAR,
    label_column_name VARCHAR,
    test_source VARCHAR,
    test_column_name VARCHAR,
    test_id VARCHAR,
    output_table VARCHAR,
    k INTEGER,
    output_neighbors Boolean
) RETURNS VARCHAR AS $$
    PythonFunctionBodyOnly(`knn', `knn')
    return knn.knn(
        schema_madlib,
        point_source,
        point_column_name,
        point_id,
        label_column_name,
        test_source,
        test_column_name,
        test_id,
        output_table,
        k,
        output_neighbors
    )
$$ LANGUAGE plpythonu VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA', `');


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.knn(
    point_source VARCHAR,
    point_column_name VARCHAR,
    point_id VARCHAR,
    label_column_name VARCHAR,
    test_source VARCHAR,
    test_column_name VARCHAR,
    test_id VARCHAR,
    output_table VARCHAR,
    output_neighbors Boolean
) RETURNS VARCHAR AS $$
DECLARE
    returnstring VARCHAR;
BEGIN
    returnstring = MADLIB_SCHEMA.knn($1,$2,$3,$4,$5,$6,$7,$8,1,$9);
    RETURN returnstring;
END;
$$ LANGUAGE plpgsql VOLATILE
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `MODIFIES SQL DATA', `');
