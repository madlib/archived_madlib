/* ----------------------------------------------------------------------- *//** 
 *
 * @file logistic.sql_in
 *
 * @brief SQL functions for logistic regression
 * @date January 2011
 *
 * @sa For a brief introduction to logistic regression, see the
 *     module description \ref grp_logreg.
 *
 *//* ----------------------------------------------------------------------- */

m4_include(`SQLCommon.m4')

/**
@addtogroup grp_logreg

@about

Logistic regression is used to estimate probabilities of a dependent binary
variable, by fitting a stochastic model. It is one of the most commonly used
tools for applied statistics and data mining [1].

Logistic regression assumes a generalized linear model:
\f[
    E[Y] = g^{-1}(\boldsymbol c^T X)
\f]
where:
- $Y$ is the dependent variable
- \f$\boldsymbol c^T X\f$ is the linear predictor
- \f$g(x) = \ln\left( \frac{x}{1-x} \right)\f$ is the link function, with
  inverse \f$\sigma(x) := g^{-1}(x) = \frac{1}{1 + \exp(-x)} \f$

For each training data point \f$i\f$, we have a vector of
features \f$x_i\f$ and an observed class \f$y_i\f$. For ease of notation, let \f$Z\f$ be a
dependent random variable such that \f$Z = -1\f$ if \f$Y = 0\f$ and \f$Z = 1\f$ if \f$Y = 1\f$,
i.e., \f$Z := 2(Y - \frac 12)\f$. By definition,
\f$P[Z = z_i | X = x_i] = \sigma(z_i \cdot \boldsymbol c^T x_i)\f$.

Since logistic regression predicts probabilities, we can do maximum-likelihood
fitting: That is, we want the vector of regression coefficients
\f$\boldsymbol c\f$ to maximize
\f[
    \prod_{i=1}^n \sigma(z_i \cdot \boldsymbol c^T \boldsymbol x_i)
\f]
or, equivalently, to maximize the objective
\f[
    l(\boldsymbol c) =
        -\sum_{i=1}^n \ln(1 + \exp(-z_i \cdot \boldsymbol c^T \boldsymbol x_i))
\f]
By looking at the Hessian, we can verify that \f$l(\boldsymbol c)\f$ is convex.

There are many techniques for solving convex optimization problems. Currently,
logistic regression in MADlib can use one of two algorithms:
- Iteratively Reweighted Least Squares
- A conjugate-gradient approach, also known as Fletcher-Reeves method in the
  literature, where we use the Hestenes-Stiefel rule for calculating the step
  size.

@usage

- <strong>Syntax</strong>

<tt>MADlib.logregr(<em>'sourceName','dependentVariable','independentVariables',[optional] numIterations,[optional] 'optimizer',[optional] precision</em>)</tt>

- <strong>Input</strong>

The training data is expected to be of the following form:\n
<tt>{TABLE|VIEW} <em>sourceName</em> ([...] <em>dependentVariable</em>
BOOLEAN, <em>independentVariables</em> DOUBLE PRECISION[], [...])</tt>

- <strong>Output</strong>

The output is a custom type <em>logregr_result</em> with the following fields:
- <em>coef</em>: The estimated coefficients \f$\boldsymbol c\f$ of type float[].
- <em>logl_ikelihood</em>: The maximized log-likelihood \f$l(\boldsymbol c)\f$ of type float.
- <em>std_err</em>: The standard error of the fit for each coefficient of type float[].
- <em>z_stat</em>: The Z-score, which is computed as \f$Z_{score}=\frac{coef}{stderr}\f$ for each coef.
- <em>p_values</em>: The p-value of the significance of fit for each coef based on the Z-score. A low p-value suggests that the null hypothesis (i.e. the coef is actually 0) should be rejected.
- <em>odd_ratio</em>: An estimate of the conditional odds ratio: \f$OddsRatio=exp(coef)\f$ for each coef.

- <strong>Notes</strong>

In order to model an intercept, set one coordinate in the <tt>independentVariables</tt> array to 1.

The last three arguments are optional and can be omitted, in which case
   default values will be used. See logistic.sql_in.  
   
@examp

@verbatim 
# SELECT * FROM data;
                     r1                      | val 
---------------------------------------------+-----
 {1,-0.537208849564195,0.0507612079381943}   | t
 {1,-1.83960389345884,0.0875045731663704}    | t
 {1,-4.28730633109808,0.839790890924633}     | t
 {1,2.4249863717705,0.499282206874341}       | t
 {1,-4.41880372352898,0.883616517763585}     | t
 {1,4.16272420901805,0.598018785007298}      | t
[...]

# SELECT * FROM madlib.logregr('data','val','r1',100,'irls',0.001); 
 
                          coef                          |  log_likelihood   |                         std_err                          |                        z_stats                        |                           p_values                            |                      odd_ratios                      
--------------------------------------------------------+-------------------+----------------------------------------------------------+-------------------------------------------------------+---------------------------------------------------------------+------------------------------------------------------
 {4.93532185789865,0.469393180445221,0.432089528467682} | -173.514819452336 | {0.407205759965291,0.0881607217787946,0.591853189786052} | {12.1199706465825,5.32428921830951,0.730062008492135} | {8.27887853333452e-34,1.01348545192981e-07,0.465352282469966} | {139.117911525733,1.59902357997671,1.54047302517705}
(1 row)
 
@endverbatim

@sa file logistic.sql_in (documenting the SQL functions)

@internal
@sa namespace logistic (documenting the driver/outer loop implemented in
    Python), function float8_cg_update_final() (documenting the
    conjugate-gradient update/iteration steps, implemented in C), function
    float8_cg_update_accum() (documenting the 
    iteratively-reweighted-least-squares update/iteration steps, implemented in
    C)
@endinternal

@literature

A somewhat random selection of nice write-ups, with valuable pointers into
further literature:

[1] Cosma Shalizi: Statistics 36-350: Data Mining, Lecture Notes, 18 November
    2009, http://www.stat.cmu.edu/~cshalizi/350/lectures/26/lecture-26.pdf

[2] Thomas P. Minka: A comparison of numerical optimizers for logistic
    regression, 2003 (revised Mar 26, 2007),
    http://research.microsoft.com/en-us/um/people/minka/papers/logreg/minka-logreg.pdf

[3] Paul Komarek, Andrew W. Moore: Making Logistic Regression A Core Data Mining
    Tool With TR-IRLS, IEEE International Conference on Data Mining 2005,
    pp. 685-688, http://komarix.org/ac/papers/tr-irls.short.pdf
*/

DROP TYPE IF EXISTS MADLIB_SCHEMA.logregr_result;
CREATE TYPE MADLIB_SCHEMA.logregr_result AS (
    coef DOUBLE PRECISION[],
    log_likelihood DOUBLE PRECISION,
    std_err DOUBLE PRECISION[],
    z_stats DOUBLE PRECISION[],
    p_values DOUBLE PRECISION[],
    odd_ratios DOUBLE PRECISION[]
);

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.logregr_cg_step_transition(
    DOUBLE PRECISION[],
    BOOLEAN,
    DOUBLE PRECISION[],
    DOUBLE PRECISION[])
RETURNS DOUBLE PRECISION[]
AS 'MODULE_PATHNAME'
LANGUAGE C IMMUTABLE;

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.logregr_irls_step_transition(
    DOUBLE PRECISION[],
    BOOLEAN,
    DOUBLE PRECISION[],
    DOUBLE PRECISION[])
RETURNS DOUBLE PRECISION[]
AS 'MODULE_PATHNAME'
LANGUAGE C IMMUTABLE;

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.logregr_cg_step_merge_states(
    state1 DOUBLE PRECISION[],
    state2 DOUBLE PRECISION[])
RETURNS DOUBLE PRECISION[]
AS 'MODULE_PATHNAME'
LANGUAGE C IMMUTABLE STRICT;

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.logregr_irls_step_merge_states(
    state1 DOUBLE PRECISION[],
    state2 DOUBLE PRECISION[])
RETURNS DOUBLE PRECISION[]
AS 'MODULE_PATHNAME'
LANGUAGE C IMMUTABLE STRICT;

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.logregr_cg_step_final(
    state DOUBLE PRECISION[])
RETURNS DOUBLE PRECISION[]
AS 'MODULE_PATHNAME'
LANGUAGE C IMMUTABLE STRICT;

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.logregr_irls_step_final(
    state DOUBLE PRECISION[])
RETURNS DOUBLE PRECISION[]
AS 'MODULE_PATHNAME'
LANGUAGE C IMMUTABLE STRICT;

DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.logreg_cg_step(
    /*+ y */ BOOLEAN,
    /*+ x */ DOUBLE PRECISION[],
    /*+ previous_state" */ DOUBLE PRECISION[]);

/**
 * @internal
 * @brief Perform one iteration of the conjugate-gradient method for computing
 *        logistic regression
 */
CREATE AGGREGATE MADLIB_SCHEMA.logregr_cg_step(
    /*+ y */ BOOLEAN,
    /*+ x */ DOUBLE PRECISION[],
    /*+ previous_state */ DOUBLE PRECISION[]) (
    
    STYPE=DOUBLE PRECISION[],
    SFUNC=MADLIB_SCHEMA.logregr_cg_step_transition,
    PREFUNC=MADLIB_SCHEMA.logregr_cg_step_merge_states,
    FINALFUNC=MADLIB_SCHEMA.logregr_cg_step_final,
	INITCOND='{0,0,0,0,0,0}'
);

DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.logreg_irls_step(
    /*+ y */ BOOLEAN,
    /*+ x */ DOUBLE PRECISION[],
    /*+ previous_state */ DOUBLE PRECISION[]);

/**
 * @internal
 * @brief Perform one iteration of the iteratively-reweighted-least-squares
 *        method for computing linear regression
 */
CREATE AGGREGATE MADLIB_SCHEMA.logregr_irls_step(
    /*+ y */ BOOLEAN,
    /*+ x */ DOUBLE PRECISION[],
    /*+ previous_state */ DOUBLE PRECISION[]) (
    
    STYPE=DOUBLE PRECISION[],
    SFUNC=MADLIB_SCHEMA.logregr_irls_step_transition,
    PREFUNC=MADLIB_SCHEMA.logregr_irls_step_merge_states,
    FINALFUNC=MADLIB_SCHEMA.logregr_irls_step_final,
	INITCOND='{0,0,0}'
);

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.internal_logregr_cg_step_distance(
    /*+ state1 */ DOUBLE PRECISION[],
    /*+ state2 */ DOUBLE PRECISION[])
RETURNS DOUBLE PRECISION AS
'MODULE_PATHNAME'
LANGUAGE c IMMUTABLE STRICT;

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.internal_logregr_cg_result(
    /*+ state */ DOUBLE PRECISION[])
RETURNS MADLIB_SCHEMA.logregr_result AS
'MODULE_PATHNAME'
LANGUAGE c IMMUTABLE STRICT;

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.internal_logregr_irls_step_distance(
    /*+ state1 */ DOUBLE PRECISION[],
    /*+ state2 */ DOUBLE PRECISION[])
RETURNS DOUBLE PRECISION AS
'MODULE_PATHNAME'
LANGUAGE c IMMUTABLE STRICT;

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.internal_logregr_irls_result(
    /*+ state */ DOUBLE PRECISION[])
RETURNS MADLIB_SCHEMA.logregr_result AS
'MODULE_PATHNAME'
LANGUAGE c IMMUTABLE STRICT;


-- begin functions for logistic-regression coefficients
-- We only need to document the last one (unfortunately, in Greenplum we have to
-- use function overloading instead of default arguments).
CREATE FUNCTION MADLIB_SCHEMA.compute_logregr(
    "source" VARCHAR,
    "depColumn" VARCHAR,
    "indepColumn" VARCHAR,
    "numIterations" INTEGER /*+ DEFAULT 20 */,
    "optimizer" VARCHAR /*+ DEFAULT 'irls' */,
    "precision" DOUBLE PRECISION /*+ DEFAULT 0.0001 */)
RETURNS INTEGER
AS $$PythonFunction(regress, logistic, compute_logregr)$$
LANGUAGE plpythonu VOLATILE;

/**
 * @brief Compute logistic-regression coefficients and diagnostic statistics
 *
 * To include an intercept in the model, set one coordinate in the
 * <tt>independentVariables</tt> array to 1.
 * 
 * @param source Name of the source relation containing the training data
 * @param depColumn Name of the dependent column (of type BOOLEAN)
 * @param indepColumn Name of the independent column (of type DOUBLE
 *        PRECISION[])
 * @param numIterations The maximum number of iterations
 * @param optimizer The optimizer to use (either
 *        <tt>'ilrs'</tt>/<tt>'newton'</tt> for iteratively reweighted least
 *        squares or <tt>'cg'</tt> for conjugent gradient)
 * @param precision The difference between log-likelihood values in successive
 *        iterations that should indicate convergence, or 0 indicating that
 *        log-likelihood values should be ignored
 *
 * @note This function starts an iterative algorithm. It is not an aggregate
 *       function. Source and column names have to be passed as strings (due to
 *       limitations of the SQL syntax).
 *
 * @examp <tt>CREATE VIEW data AS SELECT y, array[1, x1, x2] AS x FROM source;<br>
 *        SELECT * FROM logregr('data', 'y', 'x', 20, 'cg', 0.001);</tt>
 *
 * @internal
 * @sa This function is a wrapper for logistic::compute_logregr(), which
 *     sets the default values.
 */
CREATE FUNCTION MADLIB_SCHEMA.logregr(
    "source" VARCHAR,
    "depColumn" VARCHAR,
    "indepColumn" VARCHAR,
    "numIterations" INTEGER /*+ DEFAULT 20 */,
    "optimizer" VARCHAR /*+ DEFAULT 'irls' */,
    "precision" DOUBLE PRECISION /*+ DEFAULT 0.0001 */)
RETURNS MADLIB_SCHEMA.logregr_result AS $$
DECLARE
    theIteration INTEGER;
    fnName VARCHAR;
    theResult MADLIB_SCHEMA.logregr_result;
BEGIN
    theIteration := (
        SELECT MADLIB_SCHEMA.compute_logregr($1, $2, $3, $4, $5, $6)
    );
    -- Because of Greenplum bug MPP-10050, we have to use dynamic SQL (using
    -- EXECUTE) in the following
    -- Because of Greenplum bug MPP-6731, we have to hide the tuple-returning
    -- function in a subquery
    IF optimizer = 'irls' OR optimizer = 'newton' THEN
        fnName := 'internal_logregr_irls_result';
    ELSE
        fnName := 'internal_logregr_cg_result';
    END IF;
    EXECUTE
        $sql$
        SELECT (result).*
        FROM (
            SELECT
                MADLIB_SCHEMA.$sql$ || fnName || $sql$(state) AS result
                FROM _madlib_iterative_alg
                WHERE iteration = $sql$ || theIteration || $sql$
            ) subq
        $sql$
        INTO theResult;
    RETURN theResult;
END;
$$ LANGUAGE plpgsql VOLATILE;

CREATE FUNCTION MADLIB_SCHEMA.logregr(
    "source" VARCHAR,
    "depColumn" VARCHAR,
    "indepColumn" VARCHAR)
RETURNS MADLIB_SCHEMA.logregr_result AS
$$SELECT MADLIB_SCHEMA.logregr($1, $2, $3, 20, 'irls', 0.0001);$$
LANGUAGE sql VOLATILE;

CREATE FUNCTION MADLIB_SCHEMA.logregr(
    "source" VARCHAR,
    "depColumn" VARCHAR,
    "indepColumn" VARCHAR,
    "numIterations" INTEGER)
RETURNS MADLIB_SCHEMA.logregr_result AS
$$SELECT MADLIB_SCHEMA.logregr($1, $2, $3, $4, 'irls', 0.0001);$$
LANGUAGE sql VOLATILE;

CREATE FUNCTION MADLIB_SCHEMA.logregr(
    "source" VARCHAR,
    "depColumn" VARCHAR,
    "indepColumn" VARCHAR,
    "numIterations" INTEGER,
    "optimizer" VARCHAR)
RETURNS MADLIB_SCHEMA.logregr_result AS
$$SELECT MADLIB_SCHEMA.logregr($1, $2, $3, $4, $5, 0.0001);$$
LANGUAGE sql VOLATILE;
