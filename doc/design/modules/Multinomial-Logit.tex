\documentclass[12pt]{article}
\bibliographystyle{plain}

\usepackage{geometry}
\usepackage{algpseudocode}
\usepackage{algorithm}


\geometry{letterpaper,
          left   = 1in,
          right  = 1in,
          top    = 1in,
          bottom = 1in}
\renewcommand{\baselinestretch}{1.1}
\renewcommand{\arraystretch}{1.1}
\usepackage{amsmath, amsthm, amssymb} % Latex Math Include


% Custom commands
% ----------------------------------------------------------------------------------------------------
\newcounter{count}
\newcommand{\bS}[1] {\boldsymbol  #1}
\newcommand{\aI}{\forall i \in \{1,2,\ldots, N\}}
\newcommand{\aJ}{\forall j \in \{1,2,\ldots, J\}}
\newcommand{\MLE}{\hat{\bS{\beta}}_{MLE}}

% Title
% ----------------------------------------------------------------------------------------------------

\author{Srikrishna Sridhar, Mark Wellons, Caleb Welton}
\title{Multi-Nominal Logistic Regression}
\begin{document}
\maketitle
\large
\begin{center}
{\sc \bf Design Document -- Not for Distribution}
\end{center}
\normalsize

% ---------------------------------------- Introduction ---------------------------------------
 \section{Introduction}\label{sec:intro}
Multinomial logistic regression is a widely used regression analysis tool that models the outcomes of categorical dependent random variables. {\it Generalized linear models} identify key ideas shared by a broad class of distributions thereby extending techniques used in linear regression, into the field of logistic regression. 

This document provides an overview of the theory of multinomial logistic regression models followed by a design specification of how the model can be estimated using maximum likelihood estimators. In the final section, we outline a specific implementation of the algorithm that estimates multinomial logistic regression models on large datasets.

\section{Problem Description}\label{sec:problem}
In this section, we setup the notation for a generic multinomial regression problem. Let us consider an $N$-dimensional multinomial random variable $\bS{Z}$ that can take values in $J$ different categories, where $J \geq 2$. As input to the problem, we are given an $N\times J$ matrix of observations $\bS{y}$. Here $y_{i,j}$ denotes the observed value for the $j^{th}$ category of the random variable $Z_i$.  Analogous to the observed values, we define a set of parameters $\bS{\pi}$  as an $N\times J$ matrix with each entry $\pi_{i,j}$ denoting the probability of observing the random variable $Z_i$ to fall in the $j^{th}$ category. In logistic regression, we assume that the random variables $\bS{Z}$ are explained by a {\it design matrix} of independent random variables $\bS{X}$ which contains $N$ rows and $(K+1)$ columns. We define a regression coefficient $\bS{\beta}$ as a matrix with $K+1$ rows and $J$ columns such that $\beta_{k,j}$ corresponds to the importance while predicting the $j^{th}$ value of the $k^{th}$ explanatory variable.
 
For the multinomial regression model, we assume the observations $\bS{y}$ are realizations of random variables $\bS{Z}$ which are explained using random variables $\bS{X}$ and parameters $\bS{\beta}$. More specifically, if we consider the $J^{th}$ category to be the `pivot' or `baseline' category, then the log of the odds of an observation compared to the  $J^{th}$ observation can be predicted using a linear function of variables $\bS{X}$ and parameters $\bS{\beta}$.
\begin{equation}
 \log \Big( \frac{\pi_{i,j}}{\pi_{i,J}}\Big) =  \log \Big( \frac{\pi_{i,j}}{\sum_{j=1}^{J-1}\pi_{i,j}}\Big) = \sum_{k=0}^{K} x_{i,k}\beta_{k,j}  \ \ \ \ \ \aI, \aJ
\end{equation}
Solving for $\pi_{i,j}$, we have
\begin{subequations}
\label{eq:pi_def}
\begin{align}
\pi_{i,j} &= \frac{\exp \big( {\sum_{k=0}^{K} x_{i,k}\beta_{k,j}}\big)}{1 + \sum_{j=1}^{J-1}\exp{\big( \sum_{k=0}^{K} x_{i,k}\beta_{k,j}\big)}}\ \ \ \  \forall j < J\\
\pi_{i,J} &= \frac{1}{1 + \sum_{j=1}^{J-1}\exp{\big( \sum_{k=0}^{K} x_{i,k}\beta_{k,j}\big)}} 
\end{align}
\end{subequations}
In a sentence, the goal of multinomial regression is to use observations $\bS{y}$ to estimate parameters $\bS{\beta}$ that can predict random variables $\bS{Z}$ using explanatory variables $\bS{X}$. 

\subsection{Parameter Estimation}\label{sec:problem}
We evaluate the parameters $\bS{\beta}$ using a maximum-likelihood estimator (MLE) which maximizes the likelihood that a certain set of parameters predict the given set of observations. For this, we define the following likelihood function:

\begin{align}
L(\bS{\beta}|\bS{y}) &\simeq \prod_{i=1}^{N}\prod_{j=1}^{J}{\pi_{i,j}}^{y_{i,j}} \\
\intertext{Substituting \eqref{eq:pi_def} in the above expression, we have}
&= \prod_{i=1}^{N}\prod_{j=1}^{J-1} \Big( 1 + \sum_{j=1}^{J-1} e^{\sum_{k=0}^{K} x_{i,k}\beta_{k,j}} \Big)  e^{y_{i,j} \sum_{k=0}^{K} x_{i,k}\beta_{k,j}} \\
\intertext{Taking the natural logarithm of $L(\bS{\beta}|\bS{y})$ defined above, we derive the following expression for the log-likelihood function, $l(\bS{\beta})$ as:}
l(\bS{\beta}) &= \sum_{i=1}^{N}\sum_{j=1}^{N} \Big( y_{i,j} \sum_{k=0}^{K} x_{i,k}\beta_{k,j} \Big) - \log \Big( 1 + \sum_{j=1}^{J-1} \sum_{k=0}^{K} x_{i,k}\beta_{k,j}\Big) \label{eq:log_lik}
\end{align}

The maximum likelihood estimator tries maximize the log-likelihood function as defined in Equation \eqref{eq:log_lik}. Unlike linear regression, the MLE has to be obtained numerically. Since we plan to implement derivative based algorithms to solve $\max_{\bS{\beta}} l(\bS{\beta})$, we first derive expressions for the first and second derivatives of the log-likelihood function. 

We differentiate \eqref{eq:log_lik} with respect to each parameter $\beta_{k,j}$
\begin{equation}\label{eq:first_derivative}
\frac{\partial l(\bS{\beta})}{\partial \beta_{k,j}} = \sum_{i=1}^{N} y_{i,j}x_{i,k} - \pi_{i,j}x_{i,k} \ \ \ \ \forall k \  \forall j
\end{equation}
We  evaluate the extreme point of the function $l(\bS{\beta})$ by setting each equation of \eqref{eq:first_derivative} to zero. We proceed on similar lines to derive the second order derivative of the $l(\bS{\beta})$ with respect to two parameters $\beta_{k_1,j_1}$ and $\beta_{k_2,j_2}$
\begin{subequations}
\begin{align}\label{eq:second_derivative}
\frac{\partial^2 l(\bS{\beta})}{\partial \beta_{k_2,j_2} \partial \beta_{k_1,j_1}} 
&= \sum_{i=1}^{N} -\pi_{i,j_2}x_{i,k_2}(1-\pi_{i,j_1})x_{i,k_1} &&j_1 = j_2 \\
&= \sum_{i=1}^{N} \pi_{i,j_2}x_{i,k_2}\pi_{i,j_1}x_{i,k_1} &&j_1 \neq j_2 
\end{align}
\end{subequations}


\subsection{Algorithms}\label{sec:algorithms}


%\subsection*{Iterative Reweighted Least Squares}\label{sec:IRLS}
%
%The Newton-Raphson method is a numerical recipe for root finding of non-linear functions. We apply this method to solve all nonlinear equations produced by setting  \eqref{eq:first_derivative} to zero. Newton's method begins with an initial guess  for the solution after which it uses the Taylor series approximation of function at the current iterate to produce another estimate that might be closer to the true solution. This iterative procedure has one of the fastest theoretical rates of convergence in terms of number of iterations, but requires second derivative information to be computed at each iteration which is a lot more work than other light-weight derivative based methods.
%
%Newton method can be compactly described using the update step. Let us assume $\bS{\beta^0}$ to be the initial guess for the MLE (denoted by $\MLE$). If the $\bS{\beta^I}$ is the `guess' for $\MLE$ at the $I^{th}$ iteration, then we can evaluate $\bS{\beta^{I+1}}$ using
%\begin{align}\label{eq:update}
%\bS{\beta^{I+1}} &= \bS{\beta^{I}} - [l''(\bS{\beta^{I}})]^{-1}  \ l'(\bS{\beta^{I}}) \\
%&= \bS{\beta^{I}} - [l''(\bS{\beta^{I}})]^{-1}  \bS{X}^{T} (\bS{y} - \bS{\pi}) 
%\end{align}
%where $\bS{X}^{T} (\bS{y} - \bS{\pi})$ is matrix notation for the first order derivative.
%The newton method might have proven advantage in terms of number of iterations on small problem sizes but it might not scale well to larger sizes because it requires an expensive step for the matrix inversion of the second order derivative. 
%
%As an aside, we observe that Equation \eqref{eq:first_derivative} and \eqref{eq:second_derivative} refers to the first derivative in matrix form and the second derivative in tensor form respectively. In the implementation phase, we work with `vectorized' versions of $\bS{\beta},\bS{X},\bS{y}, \bS{\pi}$ denoted by $\vec{\beta},\vec{X},\vec{y}, \vec{\pi}$ respectively where the matrix are stacked up together in row major format.  
%
%Using this notation, we can rewrite the first derivative in \eqref{eq:first_derivative} as:
%\begin{equation}\label{eq:first_derivative_vectorized}
%\frac{\partial l(\bS{\beta})}{\partial \vec{\beta}} = \vec{X}^{T} (\vec{y} - \vec{\pi}) 
%\end{equation}
%Similarly, we can rewrite the second derivative in  \eqref{eq:second_derivative} as:
%\begin{equation}\label{eq:second_derivative_vectorized}
%\frac{\partial^2 l(\bS{\beta})}{\partial^2 \vec{\beta}} = \vec{X}^{T} W \vec{X}
%\end{equation}
%where $W$ is a diagonal matrix of dimension $(K+1) \times J $ where the diagonal elements are set $\pi_{i,j_1}\pi_{i,j_2}$ if $j_1 \neq j_2$ or $\pi_{i,j_1}(1-\pi_{i,j_2})$ otherwise. Note that \eqref{eq:second_derivative_vectorized} is merely a compact way to write \eqref{eq:second_derivative}. 
%
%Using the above notational simplifications, we interpret the update formula in \eqref{eq:update} as a weighted linear regression of $[\vec{X}^T\vec{\beta^{I}} + W^{-1}(\bS{y} - \bS{\pi}) ]$ on $\vec{X}$ using weights $W$. The parameters of this linear regression produces the coefficients $\vec{\beta^{I+1}}$ which are used in the next iteration. The iterative reweighed least squares procedure is illustrated in Algorithm \ref{alg:IRLS}.
%
%\begin{algorithm}
%\caption{Iterative Reweighted Least Squares (IRLS)}
%	\textbf{Input:} $\vec{X},\vec{Y}$ and an initial guess for parameters $\vec{\beta^{0}}$ \\
%	\textbf{Output:} The maximum likelihood estimator $\beta_{MLE}$
%\begin{algorithmic}               
%\State $I \leftarrow 0$
%\Repeat 
%\State Diagonal Weight matrix $W$: $w_{j_1,j_2} \leftarrow \pi_{i,j_1}\pi_{i,j_2}$ if $j_1 \neq j_2$ or $\pi_{i,j_1}(1-\pi_{i,j_2})$ otherwise
%\State Define $\vec{\alpha_j} \leftarrow \vec{\beta^{I}_j}\vec{X}_j + \frac{\bS{y}_j - \bS{\pi}_j}{W_{j,j}} $
%\State Compute $\vec{\beta}^{I+1}$  via weighted least squares using:
%\State $(\vec{X}^{T} W \vec{X})\vec{\beta}^{I+1} = \vec{X}^{T} W \vec{\alpha}$
%\Until{$\vec{\beta}^{I+1}$ converges}
%\end{algorithmic}
%\label{alg:IRLS}
%\end{algorithm}

\subsection*{Newton's Method}\label{sec:Newton}
Newton's method is a numerical recipe for root finding of non-linear functions. We apply this method to solve all nonlinear equations produced by setting  \eqref{eq:first_derivative} to zero. Newton's method begins with an initial guess  for the solution after which it uses the Taylor series approximation of function at the current iterate to produce another estimate that might be closer to the true solution. This iterative procedure has one of the fastest theoretical rates of convergence in terms of number of iterations, but requires second derivative information to be computed at each iteration which is a lot more work than other light-weight derivative based methods.

Newton method can be compactly described using the update step. Let us assume $\bS{\beta^0}$ to be the initial guess for the MLE (denoted by $\MLE$). If the $\bS{\beta^I}$ is the `guess' for $\MLE$ at the $I^{th}$ iteration, then we can evaluate $\bS{\beta^{I+1}}$ using
\begin{align}\label{eq:update}
\bS{\beta^{I+1}} &= \bS{\beta^{I}} - [l''(\bS{\beta^{I}})]^{-1}  \ l'(\bS{\beta^{I}}) \\
&= \bS{\beta^{I}} - [l''(\bS{\beta^{I}})]^{-1}  \bS{X}^{T} (\bS{y} - \bS{\pi}) 
\end{align}
where $\bS{X}^{T} (\bS{y} - \bS{\pi})$ is matrix notation for the first order derivative.
The newton method might have proven advantage in terms of number of iterations on small problem sizes but it might not scale well to larger sizes because it requires an expensive step for the matrix inversion of the second order derivative. 

As an aside, we observe that Equation \eqref{eq:first_derivative} and \eqref{eq:second_derivative} refers to the first derivative in matrix form and the second derivative in tensor form respectively. In the implementation phase, we work with `vectorized' versions of $\bS{\beta},\bS{X},\bS{y}, \bS{\pi}$ denoted by $\vec{\beta},\vec{X},\vec{y}, \vec{\pi}$ respectively where the matrix are stacked up together in row major format.  

Using this notation, we can rewrite the first derivative in \eqref{eq:first_derivative} as:
\begin{equation}\label{eq:first_derivative_vectorized}
\frac{\partial l(\bS{\beta})}{\partial \vec{\beta}} = \vec{X}^{T} (\vec{y} - \vec{\pi}) 
\end{equation}
Similarly, we can rewrite the second derivative in  \eqref{eq:second_derivative} as:
\begin{equation}\label{eq:second_derivative_vectorized}
\frac{\partial^2 l(\bS{\beta})}{\partial^2 \vec{\beta}} = \vec{X}^{T} W \vec{X}
\end{equation}
where $W$ is a diagonal matrix of dimension $(K+1) \times J $ where the diagonal elements are set $\pi_{i,j_1}\pi_{i,j_2}$ if $j_1 \neq j_2$ or $\pi_{i,j_1}(1-\pi_{i,j_2})$ otherwise. Note that \eqref{eq:second_derivative_vectorized} is merely a compact way to write \eqref{eq:second_derivative}. 

%Using the above notational simplifications, we interpret the update formula in \eqref{eq:update} as a weighted linear regression of $[\vec{X}^T\vec{\beta^{I}} + W^{-1}(\bS{y} - \bS{\pi}) ]$ on $\vec{X}$ using weights $W$. The parameters of this linear regression produces the coefficients $\vec{\beta^{I+1}}$ which are used in the next iteration. 
The Newton method procedure is illustrated in Algorithm \ref{alg:Newton}, and this is the algorithm that is implemented.  

\begin{algorithm}
\caption{Newton's Method}
	\textbf{Input:} $\vec{X},\vec{Y}$ and an initial guess for parameters $\vec{\beta^{0}}$ \\
	\textbf{Output:} The maximum likelihood estimator $\beta_{MLE}$
\begin{algorithmic}               
\State $I \leftarrow 0$
\Repeat 
\State Diagonal Weight matrix $W$: $w_{j_1,j_2} \leftarrow \pi_{i,j_1}\pi_{i,j_2}$ if $j_1 \neq j_2$ or $\pi_{i,j_1}(1-\pi_{i,j_2})$ otherwise
\State Compute $\vec{\beta}^{I+1}$  using:
\State $\vec{\beta}^{I+1} =\vec{\beta}^{I} - (\vec{X}^{T} W \vec{X})^{-1} \vec{X}^{T} (\vec{y} - \vec{\pi}) $
\Until{$\vec{\beta}^{I+1}$ converges}
\end{algorithmic}
\label{alg:Newton}
\end{algorithm}

%\subsection*{Conjugate Gradient}\label{sec:CG}
%The Conjugate Gradient method (CG)is a numerical method to solve systems of linear equations containing positive semidefinite matrices. This is specially useful in large sparse systems that are too large to be handled by direct methods such as the Cholesky decomposition. CG is a popular choice for solving linear regression problems. We refer the reader to Chapter 5 of \cite{Wright2006} for a more detailed discussion on the Conjugate Gradient Method. 
%
%CG falls into the {\it iterative reweighed least squares} framework of Algorithm \ref{alg:IRLS} by solving the problem:
%\begin{align}
%(\vec{X}^{T} W \vec{X})\vec{\beta^{I+1}} = \vec{X}^{T} W [\vec{X}^T\vec{\beta^{I}} + W^{-1}(\bS{y} - \bS{\pi})]   \nonumber
%\end{align}
%We propose the use of the Fletcher-Reeves method (Algorithm 5.4 of \cite{Wright2006}) with the Hestenes-Stiefel rule for calculating the step size because it was used in Madlib0.3 while solving binary logistic regression.

\subsection*{Other Solvers}\label{sec:others}
Since logistic regression is a workhorse of data mining there has been a lot of recent work developing and benchmarking gradient based solvers. We refer the reader to \cite{Minka2003} for a study benchmarking 8 modern logistic regression solvers including the ones discussed in this report. Recently, there has been work related to Incremental Gradient Descent algorithms \cite{Bertsekas2010}. There have been similar data mining problems solved successfully using incremental gradient algorithms. That could be one solver worth exploring in case we hit bottlenecks with CG.

\subsection*{Common Statistics}\label{sec:stats}
Irrespective of the solver that we choose to implement, we would need to calculate the standard errors and p-value. 

Asymptotics tells that the MLE is an asymptotically efficient estimator. This means that  it reaches the following Cramer-Rao lower bound:
\begin{equation}
\sqrt{n}(\beta_{MLE} - \beta) \rightarrow \mathcal{N}(0,I^{-1})
\end{equation} 
where $I^{-1}$ is the Fisher information matrix. Hence, we evaluate the standard errors using  the asymptotic variance of $i^{th}$ parameter (in vector form) of the MLE as:
\begin{equation}
se(\vec{\beta}_i) = (\vec{X}^{T} W \vec{X})^{-1}_{i}
\end{equation}

The Wald statistic is used to assess the significance of coefficients $\beta$. In the Wald test, the MLE is compared with the null hypothesis while assuming that the difference between them is approximately normally distributed. The Wald p-value for a coefficient provides us with the probability of seeing a value as extreme as the one observed, when null hypothesis is true. We evaluate the Wald p-value as:
\begin{equation}
p_i = \mbox{Pr}\Big(|Z| \geq \big|\frac{\vec{\beta}_i}{se(\vec{\beta}_i)} \big| \Big) = 2 \Big[1 - F(se(\vec{\beta}_i)) \Big]
\end{equation}
where $Z$ is a normally distributed random variable and $F$ represents the cdf of $Z$.

\subsection{Robust Variance}
%Given a regression setup of $n$ data points, each defined by a feature vector $x_i$ and a category $y_i$, we assume that $y_i$ is controlled by a $k$-dimensional parameter vector $\theta$.  Generally, we are interested in finding the values of $\theta$ that best predict $y_i$ from $x_i$, with \textit{best} being defined as the values that maximize some likelihood function $L(y,x,\theta)$.  The maximization is typically solved using the derivative of the likelihood $\psi$  and the Hessian $H$.  More formally, $\psi$ is defined as 
%\begin{align}
%\psi(y,x, \theta) = \frac{\partial L(x,y,\theta)}{\partial \theta}
%\end{align} 
%and $H$ is defined as
%\begin{align}
%H(y,x, \theta) = \frac{\partial^2 L(x,y,\theta)}{\partial \theta^2}.
%\end{align} 

In addition to the standard errors, we may also be interested in what are called \textit{robust errors}, derived from the robust variance-covariance matrix.  The robust variance-covariance matrix is computed with the sandwich operator\cite{robust_var}, of the form
\begin{align}
S( \vec{\beta}) = B(\vec{\beta}) M( \vec{\beta}) B( \vec{\beta}).  
\end{align}
and the robust errors are
\begin{align}
re(\vec{\beta}_i) = S_{ii}. 
\end{align}
The $B( \vec{\beta})$ matrix is commonly called the \textit{bread}, whereas the $M( \vec{\beta})$ matrix is the \textit{meat}.  

\subsubsection{The Bread}
Computing $B$ is relatively straightforward, 
\begin{align}
B(\vec{\beta}) = n\left(\sum_i^n -l''( \vec{\beta}) \right)^{-1}
\end{align}

\subsubsection{The Meat}
There are several choices for the $M$ matrix, each with different robustness properties.  The estimators we are interested in for this implementation are the Huber-White estimator.  

In the Huber-White estimator, the matrix $M$ is defined as
\begin{align}
M_{H} =\frac{1}{n} \sum_i^n l'( \vec{\beta})^T l'( \vec{\beta}) .
\end{align}


\section{Implementation}\label{sec:implem}

For the implementation, we plan to mimic the framework used for the current implementation of the logistic regression.  In this framework, the regression is broken up into 3 steps, denoted as the  \textit{transition} step, the \textit{merge states} step, and the \textit{final} step.  Much of the computation is done in parallel, and each transition step operates on a small number of rows in the data matrix to compute a portion of the calculation for $\vec{X}^T W\vec{X}$, as well as the calculation for $\vec{X}^TW\vec{\alpha}$. This is used in the Hessian calculation in equation \ref{eq:second_derivative_vectorized}.  

The merge step aggregates these transition steps together.  This consists of summing the $X^T WX$ calculations, summing the $\vec{X}^TW\vec{\alpha}$ calculations, and updating the bookkeeping.   The final step computes the current solution $\vec{\beta}$, and returns it and its associated statistics.  

The three steps in the framework communicate by passing around \textit{state} objects.  Each state object carries with it several fields:  
\begin{description}
\item [coef] This is the column vector containing the current solution.  This is the $\beta$ part of $\beta^TX$ with the exception of $\beta_0$.
\item [beta] The scalar constant term $\beta_0$ in $\beta^TX$.
\item [widthOfX] The number of features in the data matrix.  
\item [numRows] The number of data points being operated on.  
\item [dir] The direction of the gradient it the $k$th step.  Not sure about this one.  
\item [grad] The gradient vector in the $k$th step.
\item [gradNew] The gradient vector in the $x+1$th step.  
\item [X\_transp\_AX] The current sum of $\vec{X}^T W\vec{X}$.   
\item [X\_transp\_Az] The current sum of $\vec{X}^TW\vec{\alpha}$.
\item [logLikelihood] The log likelihood  of the solution.  
\end{description}

Each transition step is given an initial state, and returns a state with the updated X\_transp\_AX and X\_transp\_Az fields.  These states are iteratively passed to a merge step, which combines them two at a time.  The final product is then passed to the final step.  We expect to use the same system, or something similar.  

We can formalize the API's for these three steps as:
\begin{verbatim}
multilogregr_irls_step_transition(AnyType *Args)
multilogregr_irls_step_merge(AnyType *Args)
multilogregr_irls_step_final(AnyType *Args)
\end{verbatim}
The \texttt{AnyType} object is a generic type used in  madlib to retrieve and return values from the backend.  Among other things, an  \texttt{AnyType} object can be a \texttt{NULL} value, a scalar, or a state object.  

The first step, \texttt{multilogregr\_cg\_step\_transition}, will expect \texttt{*Args} to contains the following items in the following order: a state object for the current state, a vector $x$ containing a row of the design matrix, a vector $y$ containing the values of $\bS{Z}$ for this row, and a state object for the previous state.  The return value for this function will be another state object.  

The second step \texttt{multilogregr\_cg\_step\_merge} will expect \texttt{*Args} to contain two state objects, and will return a state object expressing the merging of the two input objects.  The final step \texttt{multilogregr\_cg\_step\_final} expects a single state object, and returns an \texttt{AnyType} object containing the solution's coefficients, the standard error, and the solution's p values.   



\bibliography{multinomial-logit}

\end{document}


