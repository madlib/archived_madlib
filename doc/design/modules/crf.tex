% When using TeXShop on the Mac, let it know the root document.
% The following must be one of the first 20 lines.
% !TEX root = ../design.tex

\chapter[Linear-chain conditional random field]{Linear-chain conditional random field}
% Motivation. Why do we want to have this abstract layer?
Conditional random fields(CRFs) is a type of discriminative undirected probabilistic graphical model.
CRFs have achieved the start of art accuracy in some real work natural language processing tasks such
as part of speech tagging(POS) and named entity resolution(NER). POS is the process of assigning a part
of speech to each token in a sentence. POS is often used in information retrieval, text to speech.

\section{Introduction}
% Problem definition. What are the problems that we can solve, formally and example applications?
% linearly separable, unconstrained, continuous, deterministic, convex, minimization problems.
This section is to first explain, formally, the type of problems that we consider in the MADlib convex programming framework, and then give a few example modules.

\subsection{Formulation}
Conditional random fields(CRFs) is a type of discriminative undirected probabilistic graphical model.
A linear-chain CRF is a distribution
    p(\boldsymbol Y | \boldsymbol X) = \frac{\exp{\sum_{m=1}^M \lambda_m f_m(y_n,y_{n-1},x_n)}}{Z(X)}

Where Z(X) is an instance specific normalizer
Z(X) = \sum_{y} \exp{\sum_{m=1}^M \lambda_m f_m(y_n,y_{n-1},x_n)}.

Train a CRF by maximizing the log-likelihood of a giving training set \f$ T=\{(x_k,y_k)\}_{k=1}^N \f$.
Seek the zero of the gradient.\\
    \ell_{\lambda}=\sum_k \log p_\lambda(y_k|x_k) =\sum_k[\lambda F(y_k,x_k)-\log Z_\lambda(x_k)]\\
    \nabla \ell_{\lambda}=\sum_k[\lambda F(y_k,x_k)-E_{p\lambda(Y|x_k)}F(Y,x_k)]

To avoid overfitting, we penalize the likelihood with a spherical Gaussian weight prior:\\
    \ell_{\lambda}^\prime=\sum_k[\lambda F(y_k,x_k)-\log Z_\lambda(x_k)]-\frac{\lVert \lambda \rVert^2}{2\sigma ^2}\\
    \nabla \ell_{\lambda}^\prime=\sum_k[\lambda F(y_k,x_k)-E_{p\lambda(Y|x_k)}F(Y,x_k)]-\frac{\lambda}{\sigma ^2}

\section{Part of Speech Tagging}

\subsection{Tag Set}
\begin{tabular}{lll||lll}
  Tag & Description         & Example       & Tag & Description         & Example\\
  \hline                        
  CC  & Coordin,Conjunction & and,but,or    & SYM & Symbol              & +,\%,\&\\
  CD  & Cardinal number     & one,two,three & TO  & 'to'                & to\\
  DT  & Determiner          & a,the         & UH  & Interjection        & ah,oops\\
  EX  & Existential         & there         & VB  & Verb,base form      & eat\\
  ... & ...                 & ...           & ... & ...                 & ...\\
  RBR & Adverb,comparative  & faster        & .   & Sentence-final      & (.!?)\\
  RBS & Adverb,superlative  & fastest       & :   & Mid-sentence punc   & (:;...-)\\
  RP  & Particle            & up,off        &     &                     &\\
  \hline  
\end{tabular}

\section{Feature Extraction}
The Feature Extraction module provides functionality for basic text-analysis
tasks such as part-of-speech (POS) tagging and named-entity resolution.
At present, six feature types are implemented.
    \begin{itemize}
    \item Edge Feature: transition feature that encodes the transition feature weight from current label to next label.
    \item Start Feature: fired when the current token is the first token in a sentence.
    \item End Feature: fired when the current token is the last token in a sentence.
    \item Word Feature: fired when the current token is observed in the trained dictionary.
    \item Unknown Feature: fired when the current token is not observed in the trained dictionary for at least certain times.
    \item Regex Feature: fired when the current token can be matched by the regular expression.
    \end{itemize}

\subsection{Training Data Feature Extraction}

\subsection{Testing Data Feature Extraction}
  This feature extraction function will produce two factor tables, "m table"
  $viterbi\_mtbl$ and "r table" $viterbi\_rtbl$. The $viterbi\_mtbl$
  table and a $viterbi\_rtbl$ table are used to calculate the best label
  sequence for each sentence.
 
  $viterbi\_mtbl$ table
  encodes the edge features which are solely dependent on upon current label and
  previous y value. The m table has three columns which are prev\_label, label,
  and value respectively.
  If the number of labels in \f$ n \f$, then the m factor table will \f$ n^2 \f$
  rows. Each row encodes the transition feature weight value from the previous label
  to the current label.
 
  startFeature is considered as a special edge feature which is from the
  beginning to the first token. Likewise, endFeature can be considered
  as a special edge feature which is from the last token to the very end.
  So m table encodes the edgeFeature, startFeature, and endFeature.
  If the total number of labels in the label space is 45 from 0 to 44,
  then the m factor array is as follows:
  \begin{tabular}{l*{6}{c}r}
   token             & 0   & 1   & 2   & 3   & ... & 43 &  44 \\
   \hline
  -1                 & 2.1 & 1.1 & 1.0 & 1.1 & 1.1 & 2.1 & 1.1  \\
   0                 & 1.1 & 3.9 & 1.2 & 2.1 & 2.8 & 1.8 & 0.8  \\
   1                 & 0.7 & 1.7 & 2.9 & 3.8 & 0.6 & 3.2 & 0.2  \\
   2                 & 0.2 & 3.2 & 3.8 & 2.9 & 0.2 & 0.1 & 0.2  \\
   3                 & 1.2 & 6.9 & 7.8 & 8.0 & 0.1 & 1.9 & 1.7  \\
   ...               & ... & ... & ... & ... & ... & ... & ...  \\
   44                & 8.2 & 1.8 & 3.7 & 2.1 & 7.2 & 1.3 & 7.2  \\
   45                & 1.8 & 7.8 & 5.6 & 9.8 & 2.3 & 9.4 & 1.1  \\
  \end{tabular}
 
  $viterbi\_r$ table
  is related to specific tokens. It encodes the single state features,
  e.g., wordFeature, RegexFeature for all tokens. The r table is represented
  in the following way.\\
  \begin{tabular}{l*{6}{c}r}
   token             & 0   & 1   & 2   & 3   & ... & 43 &  44 \\
   \hline
   madlib            & 0.2 & 4.1 & 0.0 & 2.1 & 0.1 & 2.5 & 1.2  \\
   is                & 1.3 & 3.0 & 0.2 & 3.1 & 0.8 & 1.9 & 0.9  \\
   an                & 0.9 & 1.1 & 1.9 & 3.8 & 0.7 & 3.8 & 0.7  \\
   open-source       & 0.8 & 0.2 & 1.8 & 2.7 & 0.5 & 0.8 & 0.1  \\
   library           & 1.8 & 1.9 & 1.8 & 8.7 & 0.2 & 1.8 & 1.1  \\
   ...               & ... & ... & ... & ... & ... & ... & ...  \\
  \end{tabular}

\section{Linear-chain CRF Learning}

\subsection{Forward-backward Algorithm}
$E_{p\lambda(Y|x)}F(Y,x)$ is computed using a variant of the forward-backward algorithm:

    E_{p\lambda(Y|x)}F(Y,x) = \sum_y p\lambda(y|x)F(y,x)
                            = \sum_i\frac{\alpha_{i-1}(f_i*M_i)\beta_i^T}{Z_\lambda(x)}\\
    $Z_\lambda(x) = \alpha_n.1^T$ \\
    where $\alpha_i$ and $\lambda_i$ the forward and backward state cost vectors defined by\\
    \alpha_i = 
    \begin{cases}
    \alpha_{i-1}M_i, & 0<i<=n\\
    1, & i=0
    \end{cases}
    ,
    \beta_i^T = 
    \begin{cases}
    M_{i+1}\lambda_{i+1}^T, & 1<=i<n\\
    1, & i=n
    \end{cases}
\subsection{L-BFGS Convex Solver}

\section{Viterbi Inference}
