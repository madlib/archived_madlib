/* ----------------------------------------------------------------------- *//** 
 *
 * @file online_sv.sql_in
 *
 * @brief SQL functions for support vector machines
 * @sa For an introduction to Support vector machines (SVMs) and related kernel
 *     methods, see the module description \ref grp_kernmach.
 *
 *//* ------------------------------------------------------------------------*/

/**
@addtogroup grp_kernmach

@about

Support vector machines (SVMs) and related kernel methods have been one of 
the most popular and well-studied machine learning techniques of the 
past 15 years, with an amazing number of innovations and applications.

In a nutshell, an SVM model $f(x)$ takes the form of
\f[
    f(x) = \sum_i \alpha_i k(x_i,x),
\f]
where each \f$ \alpha_i \f$ is a real number, each \f$ \boldsymbol x_i \f$ is a
data point from the training set (called a support vector), and
\f$ k(\cdot, \cdot) \f$ is a kernel function that measures how "similar" two
objects are. In regression, \f$ f(\boldsymbol x) \f$ is the regression function
we seek. In classification, \f$ f(\boldsymbol x) \f$ serves as
the decision boundary; so for example in binary classification, the predictor 
can output class 1 for object $x$ if \f$ f(\boldsymbol x) \geq 0 \f$, and class
2 otherwise.

In the case when the kernel function \f$ k(\cdot, \cdot) \f$ is the standard
inner product on vectors, \f$ f(\boldsymbol x) \f$ is just an alternative way of
writing a linear function
\f[
    f'(\boldsymbol x) = \langle \boldsymbol w, \boldsymbol x \rangle,
\f]
where \f$ \boldsymbol w \f$ is a weight vector having the same dimension as
\f$ \boldsymbol x \f$. One of the key points of SVMs is that we can use more
fancy kernel functions to efficiently learn linear models in high-dimensional
feature spaces, since \f$ k(\boldsymbol x_i, \boldsymbol x_j) \f$ can be
understood as an efficient way of computing an inner product in the feature
space:
\f[
    k(\boldsymbol x_i, \boldsymbol x_j)
    =   \langle \phi(\boldsymbol x_i), \phi(\boldsymbol x_j) \rangle,
\f]
where \f$ \phi(\boldsymbol x) \f$ projects \f$ \boldsymbol x \f$ into a
(possibly infinite-dimensional) feature space.

There are many algorithms for learning kernel machines. This module 
implements the class of online learning with kernels algorithms 
described in Kivinen et al. [1]. See also the book Scholkopf and Smola [2]
for much more details.

The implementation follows the original description in the Kivinen et al 
paper faithfully, except that we only update the support vector model 
when we make a significant error. The original algorithms update the 
support vector model at every step, even when no error was made, in the 
name of regularisation. For practical purposes, and this is verified 
empirically to a certain degree, updating only when necessary is both 
faster and better from a learning-theoretic point of view, at least in 
the i.i.d. setting.

Methods for classification, regression and novelty detection are 
available. Multiple instances of the algorithms can be executed 
in parallel on different subsets of the training data. The resultant
support vector models can then be combined using standard techniques
like averaging or majority voting.

Training data points are accessed via a table or a view. The support
vector models can also be stored in tables for fast execution.

@prereq

The \link grp_svec sparse vector SVEC datatype\endlink.

@usage

Here are the main learning functions.

-  Regression learning is achieved through the following function
   \code
   madlib.svm_regression(input_table text, model_name text, parallel bool, eta float8 DEFAULT 0.1, nu float8 DEFAULT 0.005, slambda float8 DEFAULT 0.2)
   \endcode

-  Classification learning is achieved through the following function
   \code
   madlib.svm_classification(input_table text, model_name text, parallel bool, eta float8 DEFAULT 0.1, nu float8 DEFAULT 0.005)
   \endcode

-  Novelty detection is achieved through the following function
   \code
   madlib.svm_novelty_detection(input_table text, model_name text, parallel bool, eta float8 DEFAULT 0.1, nu float8 DEFAULT 0.01)
   \endcode

In each case, input_table is the name of the table/view with the training data. 
In the case of classification and regression, mandatory fields for input_table are
\code
	( id INT,       -- point ID
	  ind FLOAT8[], -- data point
	  label FLOAT8  -- label of data point
        ).
\endcode
For novelty detection, the label field is not required.

The model_name parameter is the name under which we want to store the resultant learned model. 
The parallel parameter is a flag indicating whether the system
should learn multiple models in parallel. (The multiple models can be
combined to make predictions; more on that shortly.) 
The eta, nu, and slambda parameters are optional. (The default values are shown.) 
The eta parameter is the learning rate, a number in (0,1]. 
The nu parameter is a compression parameter with a value in (0,1] that corresponds approximately 
to the fraction of the training data that will become support vectors. 
Finally, the slambda parameter is a regularisation parameter.

Here are the functions that can be used to make predictions on new
data points.

- To make predictions on new data points using a single model
  learned previously, we use the function
  \code
  madlib.svm_predict(model_name text, x float8[]),
  \endcode
  where model_name is the name of the model stored and x is a data point.

- To make predictions on new data points using multiple models
  learned in parallel, we use the function
  \code
  madlib.svm_predict_combo(model_name text, x float8[]),
  \endcode
  where model_name is the name under which the models are stored, and x 
  is a data point.

- Note that, at the moment, we cannot use madlib.svm_predict() and madlib.svm_predict_combo()
  on multiple data points. For example, something like the following
  \code
  select madlib.svm_predict('mymodel', ind) from a_table;
  \endcode
  will fail.

- To make predictions on new data points stored in a table using
  previously learned models, we use the function
  \code
  madlib.svm_predict(input_table text, col_name text, model_name text, output_table text, parallel bool),
  \endcode
  where the data points are stored under column col_name in input_table,
  output_table is the table into which we will store the results,
  model_name is the name of the model previously learned, and parallel is 
  true if the model was learned in parallel, false otherwise. 
  The output_table is created during the function call; an existing table with 
  the same name will be dropped.

Models that have been stored can be deleted using the function
\code
   madlib.svm_drop_model(modelname text).
\endcode

@examp

As a general first step, we need to prepare and populate an input 
table/view with the following structure:
\code   
        CREATE TABLE my_schema.my_input_table 
        (       
                id    INT,       -- point ID
                ind   FLOAT8[],  -- data point
                label FLOAT8     -- label of data point
    	);
\endcode    
     Note: The label field is not required for novelty detection.
    

Example usage for regression:
     -# We can randomly generate 1000 5-dimensional data labelled by the simple target function 
        \code
        t(x) = if x[5] = 10 then 50 else if x[5] = -10 then 50 else 0;
        \endcode
        and store that in the madlib.svm_train_data table as follows:
        \code
        testdb=# select madlib.svm_generate_reg_data(1000, 5);
        \endcode
     -# We can now learn a regression model and store the resultant model
        under the name  'myexp'.
        \code
        testdb=# select madlib.svm_regression('madlib.svm_train_data', 'myexp', false);
        \endcode
     -# We can now start using it to predict the labels of new data points 
        like as follows:
        \code
        testdb=# select madlib.svm_predict('myexp', '{1,2,4,20,10}');
        testdb=# select madlib.svm_predict('myexp', '{1,2,4,20,-10}');
        \endcode
     -# To learn multiple support vector models, we replace the learning step above by 
        \code
        testdb=# select madlib.svm_regression('madlib.svm_train_data', 'myexp', true);
        \endcode
        The resultant models can be used for prediction as follows:
        \code
        testdb=# select * from madlib.svm_predict_combo('myexp', '{1,2,4,20,10}');
        \endcode
     -# We can also predict the labels of all the data points stored in a table.
        For example, we can execute the following:
        \code
        testdb=# create table madlib.svm_reg_test ( id int, ind float8[] );
        testdb=# insert into madlib.svm_reg_test (select id, ind from madlib.svm_train_data limit 20);
        testdb=# select madlib.svm_predict('madlib.svm_reg_test', 'ind', 'myexp', 'madlib.svm_reg_output1', false); 
        testdb=# select * from madlib.svm_reg_output1;
        testdb=# select madlib.svm_predict('madlib.svm_reg_test', 'ind', 'myexp', 'madlib.svm_reg_output2', true);
        testdb=# select * from madlib.svm_reg_output2;
        \endcode 

Example usage for classification:
     -# We can randomly generate 2000 5-dimensional data labelled by the simple
        target function 
        \code
        t(x) = if x[1] > 0 and  x[2] < 0 then 1 else -1;
        \endcode
        and store that in the madlib.svm_train_data table as follows:
        \code 
        testdb=# select madlib.svm_generate_cls_data(2000, 5);
        \endcode
     -# We can now learn a classification model and store the resultant model
        under the name  'myexpc'.
        \code
        testdb=# select madlib.svm_classification('madlib.svm_train_data', 'myexpc', false);
        \endcode
     -# We can now start using it to predict the labels of new data points 
        like as follows:
        \code
        testdb=# select madlib.svm_predict('myexpc', '{10,-2,4,20,10}');
        \endcode 
     -# To learn multiple support vector models, replace the model-building and prediction steps above by 
        \code
        testdb=# select madlib.svm_classification('madlib.svm_train_data', 'myexpc', true);
        testdb=# select * from madlib.svm_predict_combo('myexpc', '{10,-2,4,20,10}');
        \endcode

Example usage for novelty detection:
     -# We can randomly generate 100 2-dimensional data (the normal cases)
        and store that in the madlib.svm_train_data table as follows:
        \code
        testdb=# select madlib.svm_generate_nd_data(100, 2);
        \endcode
     -# Learning and predicting using a single novelty detection model can be done as follows:
        \code
        testdb=# select madlib.svm_novelty_detection('madlib.svm_train_data', 'myexpnd', false);
        testdb=# select madlib.svm_predict('myexpnd', '{10,-10}');  
        testdb=# select madlib.svm_predict('myexpnd', '{-1,-1}');  
        \endcode
     -# Learning and predicting using multiple models can be done as follows:
        \code
        testdb=# select madlib.svm_novelty_detection('madlib.svm_train_data', 'myexpnd', true);
        testdb=# select * from madlib.svm_predict_combo('myexpnd', '{10,-10}');  
        testdb=# select * from madlib.svm_predict_combo('myexpnd', '{-1,-1}');  
        \endcode

@sa file online_sv.sql_in (documenting the SQL functions)

@internal
@sa namespace online_sv (documenting the implementation in Python)
@endinternal

@literature

[1] Jyrki Kivinen, Alexander J. Smola, and Robert C. Williamson: <em>Online
    Learning with Kernels</em>, IEEE Transactions on Signal Processing, 52(8),
    2165-2176, 2004.

[2] Bernhard Scholkopf and Alexander J. Smola: <em>Learning with Kernels:
    Support Vector Machines, Regularization, Optimization, and Beyond</em>, 
    MIT Press, 2002.
*/

-- The following is the structure to record the results of a learning process.
-- We work with arrays of float8 for now; we'll extend the code to work with sparse vectors next.
--
CREATE TYPE MADLIB_SCHEMA.svm_model_rec AS (
       inds int,        -- number of individuals processed 
       cum_err float8,  -- cumulative error
       epsilon float8,  -- the size of the epsilon tube around the hyperplane, adaptively adjusted by algorithm
       rho float8,      -- classification margin
       b   float8,      -- classifier offset
       nsvs int,        -- number of support vectors
       ind_dim int,     -- the dimension of the individuals
       weights float8[],       -- the weight of the support vectors
       individuals float8[]    -- the array of support vectors, represented as a 1-D array
);

CREATE TABLE MADLIB_SCHEMA.svm_reg_result (
       model_table text, -- table where the model is stored
       model_name text,  -- model name
       inds int,         -- number of individuals processed 
       cum_err float8,   -- cumulative error
       epsilon float8,   -- the size of the epsilon tube around the hyperplane, adaptively adjusted by algorithm
       b float8,         -- classifier offset
       nsvs int          -- number of support vectors
);

-- The table for storing training data 
CREATE TABLE MADLIB_SCHEMA.svm_train_data ( id int, ind float8[], label float8 ) 
ifdef(`GREENPLUM', `DISTRIBUTED BY (id)')
;

-- The table for storing learned support vector models, as one composite object
CREATE TABLE MADLIB_SCHEMA.svm_results ( id text, model MADLIB_SCHEMA.svm_model_rec ) 
ifdef(`GREENPLUM', `DISTRIBUTED BY (id)')
;

-- The table for storing learned support vector models, one row per support vector
CREATE TABLE MADLIB_SCHEMA.svm_model ( id text, weight float8, sv float8[] ) 
ifdef(`GREENPLUM', `DISTRIBUTED BY (weight)')   -- this shouldn't be distributed by id; otherwise won't get parallelism
;

-- Kernel functions are a generalisation of inner products. 
-- They provide the means by which we can extend linear machines to work in non-linear transformed feature spaces.
-- Here we specify the dot product as the kernel; it can be replaced with any other kernel, including the polynomial
-- and Gaussian kernels defined below.
--
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_kernel(float8[], float8[]) RETURNS float8 AS $$
	SELECT MADLIB_SCHEMA.svec_dot($1, $2);
--	SELECT MADLIB_SCHEMA.svm_polynomial_kernel($1, $2, 2);
-- 	SELECT MADLIB_SCHEMA.svm_gaussian_kernel($1, $2, 0.5);
$$ LANGUAGE SQL IMMUTABLE;


-- Here are two other standard kernels.
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_polynomial_kernel(x float8[], y float8[], degree int) 
RETURNS float8 AS $$
	SELECT MADLIB_SCHEMA.svec_dot($1,$2) ^ $3;
$$ LANGUAGE SQL IMMUTABLE;

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_gaussian_kernel(x float8[], y float8[], gamma float) 
RETURNS float8 AS $$
	SELECT exp(-1.0 * $3 * MADLIB_SCHEMA.svec_dot(MADLIB_SCHEMA.float8arr_minus_float8arr($1,$2),
						      MADLIB_SCHEMA.float8arr_minus_float8arr($1,$2)));
$$ LANGUAGE SQL IMMUTABLE;

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_predict_sub(int,int,float8[],float8[],float8[]) RETURNS float8
AS 'MODULE_PATHNAME', 'svm_predict_sub' LANGUAGE C IMMUTABLE STRICT;

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_predict(svs MADLIB_SCHEMA.svm_model_rec, ind float8[]) 
RETURNS float8 AS $$
	SELECT MADLIB_SCHEMA.svm_predict_sub($1.nsvs, $1.ind_dim, $1.weights, $1.individuals, $2);
$$ LANGUAGE SQL;

-- This is the main online support vector regression learning algorithm. 
-- The function updates the support vector model as it processes each new training example.
-- This function is wrapped in an aggregate function to process all the training examples stored in a table.  
--
CREATE OR REPLACE FUNCTION 
MADLIB_SCHEMA.svm_reg_update(svs MADLIB_SCHEMA.svm_model_rec, ind FLOAT8[], label FLOAT8, eta FLOAT8, nu FLOAT8, slambda FLOAT8)
RETURNS MADLIB_SCHEMA.svm_model_rec AS 'MODULE_PATHNAME', 'svm_reg_update' LANGUAGE C STRICT;   

CREATE AGGREGATE MADLIB_SCHEMA.svm_reg_agg(float8[], float8, float8, float8, float8) (
       sfunc = MADLIB_SCHEMA.svm_reg_update,
       stype = MADLIB_SCHEMA.svm_model_rec,
       initcond = '(0,0,0,0,1,0,0,{},{})'
);

-- This is the main online support vector classification learning algorithm. 
-- The function updates the support vector model as it processes each new training example.
-- This function is wrapped in an aggregate function to process all the training examples stored in a table.  
--
CREATE OR REPLACE FUNCTION 
MADLIB_SCHEMA.svm_cls_update(svs MADLIB_SCHEMA.svm_model_rec, ind FLOAT8[], label FLOAT8, eta FLOAT8, nu FLOAT8)
RETURNS MADLIB_SCHEMA.svm_model_rec AS 'MODULE_PATHNAME', 'svm_cls_update' LANGUAGE C STRICT;   

CREATE AGGREGATE MADLIB_SCHEMA.svm_cls_agg(float8[], float8, float8, float8) (
       sfunc = MADLIB_SCHEMA.svm_cls_update,
       stype = MADLIB_SCHEMA.svm_model_rec,
       initcond = '(0,0,0,0,1,0,0,{},{})'
);

-- This is the main online support vector novelty detection algorithm. 
-- The function updates the support vector model as it processes each new training example.
-- In contrast to classification and regression, the training data points have no labels.
-- This function is wrapped in an aggregate function to process all the training examples stored in a table.  
--
CREATE OR REPLACE FUNCTION 
MADLIB_SCHEMA.svm_nd_update(svs MADLIB_SCHEMA.svm_model_rec, ind FLOAT8[], eta FLOAT8, nu FLOAT8)
RETURNS MADLIB_SCHEMA.svm_model_rec AS 'MODULE_PATHNAME', 'svm_nd_update' LANGUAGE C STRICT;   

CREATE AGGREGATE MADLIB_SCHEMA.svm_nd_agg(float8[], float8, float8) (
       sfunc = MADLIB_SCHEMA.svm_nd_update,
       stype = MADLIB_SCHEMA.svm_model_rec,
       initcond = '(0,0,0,0,0,0,0,{},{})'
);

-- This function transforms a MADLIB_SCHEMA.svm_model_rec into a set of (weight, support_vector) values for the purpose of storage in a table.
--
CREATE OR REPLACE FUNCTION 
MADLIB_SCHEMA.__svm_transform_rec(modelname text, ind_dim int, weights float8[], individuals float8[]) 
RETURNS SETOF MADLIB_SCHEMA.svm_model AS $$
DECLARE
	idx INT := 0;
	nsvs INT;
	sv MADLIB_SCHEMA.svm_model;
BEGIN
	nsvs = array_upper(weights,1);
	FOR i IN 1..nsvs LOOP 
	    sv.id := modelname;
       	    sv.weight := weights[i];
	    idx := ind_dim * (i-1);
	    sv.sv := individuals[(idx+1):(idx+ind_dim)]; 
	    RETURN NEXT sv;
     	END LOOP;
END;
$$ LANGUAGE plpgsql;

-- This function stores a MADLIB_SCHEMA.svm_model_rec stored with modelname in the MADLIB_SCHEMA.svm_results table into the MADLIB_SCHEMA.svm_model table.
--
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_storeModel(modelname TEXT) RETURNS VOID AS $$
DECLARE
	temp INT;
	myind_dim INT;
	mynsvs INT;
	myweights float8[];
	myindividuals float8[];
--	mysvs MADLIB_SCHEMA.svm_model_rec;
BEGIN
	SELECT INTO temp COUNT(*) FROM MADLIB_SCHEMA.svm_results WHERE id = modelname;
	IF temp = 0 THEN
	   	RAISE EXCEPTION 'No support vector model with name ''%'' found. (Be careful of white spaces.)', modelname;
	   	EXIT;
	END IF;	
--	SELECT INTO mysvs model FROM MADLIB_SCHEMA.svm_results WHERE id = modelname; -- for some strange reason this line doesn't work....
	SELECT INTO myind_dim (model).ind_dim FROM MADLIB_SCHEMA.svm_results WHERE id = modelname;
	SELECT INTO mynsvs (model).nsvs FROM MADLIB_SCHEMA.svm_results WHERE id = modelname;
	SELECT INTO myweights (model).weights FROM MADLIB_SCHEMA.svm_results WHERE id = modelname;
	SELECT INTO myindividuals (model).individuals FROM MADLIB_SCHEMA.svm_results WHERE id = modelname;
	IF mynsvs = 0 THEN
	   	RAISE NOTICE 'Model ''%'' has no support vectors and therefore not processed', modelname;
	ELSE
		INSERT INTO MADLIB_SCHEMA.svm_model (SELECT * FROM MADLIB_SCHEMA.__svm_transform_rec(modelname, myind_dim, myweights[1:mynsvs], myindividuals));
	END IF;	
END;
$$ LANGUAGE plpgsql;

-- This function stores a collection of models learned in parallel into the MADLIB_SCHEMA.svm_model table.
-- The different models are assumed to be named modelname1, modelname2, ....
--
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_storeModel(modelname TEXT, n INT) RETURNS VOID AS $$
DECLARE
BEGIN
	FOR i IN 0..n-1 LOOP
	    PERFORM MADLIB_SCHEMA.svm_storeModel(modelname || i);
        END LOOP;
END;
$$ LANGUAGE plpgsql;


-- DROP TYPE IF EXISTS MADLIB_SCHEMA.svm_model_pr CASCADE;
CREATE TYPE MADLIB_SCHEMA.svm_model_pr AS ( model text, prediction float8 );

/**
 * @brief Evaluates a support-vector model on a given data point
 *
 * @param modelname The table storing learned model \f$ f \f$ to be used
 * @param ind The data point \f$ \boldsymbol x \f$
 * @return This function returns \f$ f(\boldsymbol x) \f$
 */
CREATE OR REPLACE FUNCTION 
MADLIB_SCHEMA.svm_predict(modelname text, ind float8[]) RETURNS FLOAT8 AS $$
    nmodels_rec = plpy.execute("select count(distinct id) from " + modelname);

    if (nmodels_rec[0]['count'] <> 1):
        plpy.error("Error in MADLIB_SCHEMA.svm_predict(): the table contains an ensemble of models");

    ret = plpy.execute("SELECT sum(weight * MADLIB_SCHEMA.svm_kernel(sv, \'" + str(ind) + "\')) FROM " + modelname);
    if (ret.nrows() == 0):
       plpy.error("Error executing svm_predict()");    
    return ret[0]['sum'];
$$ LANGUAGE plpythonu;

/**
 * @brief Evaluates multiple support-vector models on a data point
 *
 * @param modelname The table storing the learned models to be used.
 * @param ind The data point \f$ \boldsymbol x \f$
 * @return This function returns a table, a row for each model.
 *      Moreover, the last row contains the average value, over all models.
 *
 * The different models are assumed to be named <tt><em>modelname</em>1</tt>,
 * <tt><em>modelname</em>2</tt>, ....
 */
CREATE OR REPLACE FUNCTION
MADLIB_SCHEMA.svm_predict_combo(modelname text, ind float8[]) RETURNS SETOF MADLIB_SCHEMA.svm_model_pr AS $$
    nmodels_rec = plpy.execute("select count(distinct id) from " + modelname);
    nmodels = nmodels_rec[0]['count'];

    if (nmodels <= 1):
        plpy.error("Error in MADLIB_SCHEMA.svm_predict_combo(): not an ensemble of models");

    sumpr = 0.0;
    ret = [];

    for i in range(0,nmodels):
        pred = plpy.execute("select sum(weight * MADLIB_SCHEMA.svm_kernel(sv, \'" + str(ind) + "\')) from " + modelname + " where id = '" + modelname + str(i) + "'");
        if (pred.nrows() == 0):
            plpy.error("Error in MADLIB_SCHEMA.svm_predict_combo(): failed to compute prediction for model '" + modelname + str(i) + "'.");

        prediction = pred[0]['sum'];
        sumpr = sumpr + prediction;
        ret = ret + [(modelname + str(i), prediction)];

    ret = ret + [('avg', sumpr/nmodels)];	
    return ret;
$$ LANGUAGE plpythonu;


-- This function removes all the models whose id is prefixed with modelname in the svm_results table
--
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_drop_model(modelname text) RETURNS VOID
AS $$
   DELETE FROM MADLIB_SCHEMA.svm_results WHERE position($1 in id) > 0;
$$ LANGUAGE SQL;

/**
 * @brief This is the support vector regression function
 *
 * @param input_table The name of the table/view with the training data
 * @param modelname The name under which we want to store the resultant learned model
 * @param parallel A flag indicating whether the system should learn multiple models in parallel
 * @return Textual summary of the algorithm run
 *
 * @internal 
 * @sa This function is a wrapper for online_sv::svm_regression().
 */
CREATE OR REPLACE FUNCTION 
MADLIB_SCHEMA.svm_regression(input_table text, modelname text, parallel bool)
RETURNS SETOF MADLIB_SCHEMA.svm_reg_result
AS $$
    import sys
    try:
        from madlib import online_sv
    except:
        sys.path.append("PLPYTHON_LIBDIR")
        from madlib import online_sv

    return online_sv.svm_regression(input_table, modelname, parallel);	
$$ LANGUAGE 'plpythonu';

/**
 * @brief This is the support vector regression function
 *
 * @param input_table The name of the table/view with the training data
 * @param modelname The name under which we want to store the resultant learned model
 * @param parallel A flag indicating whether the system should learn multiple models in parallel
 * @param eta Learning rate in (0,1] 
 * @param nu  Compression parameter in (0,1] associated with the fraction of training data that will become support vectors 
 * @param slambda Regularisation parameter
 * @return Textual summary of the algorithm run
 *
 * @internal 
 * @sa This function is a wrapper for online_sv::svm_regression().
 */
CREATE OR REPLACE FUNCTION 
MADLIB_SCHEMA.svm_regression(input_table text, modelname text, parallel bool, eta float8, nu float8, slambda float8)
RETURNS TEXT
AS $$
    import sys
    try:
        from madlib import online_sv
    except:
        sys.path.append("PLPYTHON_LIBDIR")
        from madlib import online_sv

    return online_sv.svm_regression(input_table, modelname, parallel, eta, nu, slambda);
$$ LANGUAGE 'plpythonu';

/**
 * @brief This is the support vector classification function
 *
 * @param input_table The name of the table/view with the training data
 * @param modelname The name under which we want to store the resultant learned model
 * @param parallel A flag indicating whether the system should learn multiple models in parallel
 * @return Textual summary of the algorithm run
 *
 * @internal 
 * @sa This function is a wrapper for online_sv::svm_classification().
 */
CREATE OR REPLACE FUNCTION 
MADLIB_SCHEMA.svm_classification(input_table text, modelname text, parallel bool)
RETURNS TEXT
AS $$
    import sys
    try:
        from madlib import online_sv
    except:
        sys.path.append("PLPYTHON_LIBDIR")
        from madlib import online_sv

    return online_sv.svm_classification(input_table, modelname, parallel);
$$ LANGUAGE 'plpythonu';

/**
 * @brief This is the support vector classification function
 *
 * @param input_table The name of the table/view with the training data
 * @param modelname The name under which we want to store the resultant learned model
 * @param parallel A flag indicating whether the system should learn multiple models in parallel
 * @param eta Learning rate in (0,1]
 * @param nu Compression parameter in (0,1] associated with the fraction of training data that will become support vectors
 * @return Textual summary of the algorithm run
 *
 * @internal 
 * @sa This function is a wrapper for online_sv::svm_classification().
 */
CREATE OR REPLACE FUNCTION 
MADLIB_SCHEMA.svm_classification(input_table text, modelname text, parallel bool, eta float8, nu float8)
RETURNS TEXT
AS $$
    import sys
    try:
        from madlib import online_sv
    except:
        sys.path.append("PLPYTHON_LIBDIR")
        from madlib import online_sv

    return online_sv.svm_classification(input_table, modelname, parallel, eta, nu);
$$ LANGUAGE 'plpythonu';

/**
 * @brief This is the support vector novelty detection function.
 * 
 * @param input_table The name of the table/view with the training data
 * @param modelname The name under which we want to store the resultant learned model
 * @param parallel A flag indicating whether the system should learn multiple models in parallel
 * @return Textual summary of the algorithm run
 *
 * @internal 
 * @sa This function is a wrapper for online_sv::svm_novelty_detection().
 */
CREATE OR REPLACE FUNCTION 
MADLIB_SCHEMA.svm_novelty_detection(input_table text, modelname text, parallel bool)
RETURNS TEXT
AS $$
    import sys
    try:
        from madlib import online_sv
    except:
        sys.path.append("PLPYTHON_LIBDIR")
        from madlib import online_sv

    return online_sv.svm_novelty_detection(input_table, modelname, parallel);
$$ LANGUAGE 'plpythonu';

/**
 * @brief This is the support vector novelty detection function.
 * 
 * @param input_table The name of the table/view with the training data
 * @param modelname The name under which we want to store the resultant learned model
 * @param parallel A flag indicating whether the system should learn multiple models in parallel
 * @param eta Learning rate in (0,1]
 * @param nu Compression parameter in (0,1] associated with the fraction of training data that will become support vectors
 * @return Textual summary of the algorithm run
 *
 * @internal 
 * @sa This function is a wrapper for online_sv::svm_novelty_detection().
 */
CREATE OR REPLACE FUNCTION 
MADLIB_SCHEMA.svm_novelty_detection(input_table text, modelname text, parallel bool, eta float8, nu float8)
RETURNS TEXT
AS $$
    import sys
    try:
        from madlib import online_sv
    except:
        sys.path.append("PLPYTHON_LIBDIR")
        from madlib import online_sv

    return online_sv.svm_novelty_detection(input_table, modelname, parallel, eta, nu);
$$ LANGUAGE 'plpythonu';


/**
 * @brief Scores the data points stored in a table using a learned support-vector model
 *
 * @param input_table Name of table/view containing the data points to be scored
 * @param col_name Name of column in input_table containing the data points
 * @param modelname Name of the learned model to be used for scoring 
 * @param output_table Name of table to store the results 
 * @param parallel A flag indicating whether the model to be used was learned in parallel
 * @return Textual summary of the algorithm run
 *
 * @internal 
 * @sa This function is a wrapper for online_sv::svm_predict().
 */
CREATE OR REPLACE FUNCTION
MADLIB_SCHEMA.svm_predict(input_table text, col_name text, modelname text, output_table text, parallel bool)
RETURNS TEXT
AS $$
    import sys
    try:
        from madlib import online_sv
    except:
        sys.path.append("PLPYTHON_LIBDIR")
        from madlib import online_sv

    return online_sv.svm_predict(input_table, col_name, modelname, output_table, parallel);
$$ LANGUAGE 'plpythonu';

-- Generate artificial training data 
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__svm_random_ind(d INT) RETURNS float8[] AS $$
DECLARE
    ret float8[];
BEGIN
    FOR i IN 1..(d-1) LOOP
        ret[i] = RANDOM() * 40 - 20;
    END LOOP;
    IF (RANDOM() > 0.5) THEN
        ret[d] = 10;
    ELSE 
        ret[d] = -10;
    END IF;
    RETURN ret;
END
$$ LANGUAGE plpgsql;

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__svm_random_ind2(d INT) RETURNS float8[] AS $$
DECLARE
    ret float8[];
BEGIN
    FOR i IN 1..d LOOP
        ret[i] = RANDOM() * 5 + 10;
	IF (RANDOM() > 0.5) THEN ret[i] = -ret[i]; END IF;
    END LOOP;
    RETURN ret;
END
$$ LANGUAGE plpgsql;


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_generate_reg_data(num int, dim int) RETURNS VOID AS $$
    plpy.execute("DELETE FROM MADLIB_SCHEMA.svm_train_data")
    plpy.execute("INSERT INTO MADLIB_SCHEMA.svm_train_data SELECT a.val, MADLIB_SCHEMA.__svm_random_ind(" + str(dim) + "), 0 FROM (SELECT generate_series(1," + str(num) + ") AS val) AS a")
    plpy.execute("UPDATE MADLIB_SCHEMA.svm_train_data SET label = MADLIB_SCHEMA.__svm_target_reg_func(ind)")
$$ LANGUAGE 'plpythonu';

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__svm_target_reg_func(ind float8[]) RETURNS float8 AS $$
DECLARE
    dim int;
BEGIN
    dim = array_upper(ind,1);
    IF (ind[dim] = 10) THEN RETURN 50; END IF;
    RETURN -50;
END
$$ LANGUAGE plpgsql;

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_generate_cls_data(num int, dim int) RETURNS VOID AS $$
    plpy.execute("DELETE FROM MADLIB_SCHEMA.svm_train_data")
    plpy.execute("INSERT INTO MADLIB_SCHEMA.svm_train_data SELECT a.val, MADLIB_SCHEMA.__svm_random_ind(" + str(dim) + "), 0 FROM (SELECT generate_series(1," + str(num) + ") AS val) AS a")
    plpy.execute("UPDATE MADLIB_SCHEMA.svm_train_data SET label = MADLIB_SCHEMA.__svm_target_cl_func(ind)")
$$ LANGUAGE 'plpythonu';

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__svm_target_cl_func(ind float8[]) RETURNS float8 AS $$
BEGIN
    IF (ind[1] > 0 AND ind[2] < 0) THEN RETURN 1; END IF;
    RETURN -1;
END
$$ LANGUAGE plpgsql;

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.svm_generate_nd_data(num int, dim int) RETURNS VOID AS $$
    plpy.execute("DELETE FROM MADLIB_SCHEMA.svm_train_data")
    plpy.execute("INSERT INTO MADLIB_SCHEMA.svm_train_data SELECT a.val, MADLIB_SCHEMA.__svm_random_ind2(" + str(dim) + "), 0 FROM (SELECT generate_series(1," + str(num) + ") AS val) AS a")
$$ LANGUAGE 'plpythonu';

