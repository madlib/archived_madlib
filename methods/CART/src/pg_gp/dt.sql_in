/* ----------------------------------------------------------------------- *//** 
 *
 * @file decision_tree.sql_in
 *
 * @brief decision tree APIs and main controller written in PL/PGSQL
 * @date Dec. 22 2011
 *
 * @sa For a brief introduction to decision trees, see the
 *     module description \ref grp_dectree.
 *
 *//* ----------------------------------------------------------------------- */

m4_include(`SQLCommon.m4')

/* Own macro definitions */
m4_ifelse(
    m4_eval(
        m4_ifdef(`__GREENPLUM__', 1, 0) &&
        __DBMS_VERSION_MAJOR__ * 100 + __DBMS_VERSION_MINOR__ < 401
    ), 1,
    `m4_define(`__GREENPLUM_PRE_4_1__')'
)
m4_ifelse(
    m4_eval(
        m4_ifdef(`__POSTGRESQL__', 1, 0) &&
        __DBMS_VERSION_MAJOR__ < 9
    ), 1,
    `m4_define(`__POSTGRESQL_PRE_9_0__')'
)

/**
@addtogroup grp_dectree

@about

This module provides an implementation of the C4.5 implementation to grow decision trees.

The implementation supports:
- Tree Growth
- Multiple split critera, including:
  - Information Gain, also known as Kullback-Leibeler divergence:
  - Gini Coefficient:
  - Gain Ratio
- Tree Pruning
- Tree Scoring
- Tree Display
- Continuous and Discrete features
- Null handling

@input

The <b>training data</b> is expected to be of 
the following form:
<pre>{TABLE|VIEW} <em>trainingSource</em> (
    ...
    <em>id</em> INTEGER,
    <em>feature1</em> ANYTYPE,
    <em>feature2</em> ANYTYPE,
    <em>feature3</em> ANYTYPE,
    ....................
    <em>featureN</em> ANYTYPE,
    <em>class</em> INTEGER,
    ...
)</pre>

The <b>data to classify</b> is expected to be 
of the following form:
<pre>{TABLE|VIEW} <em>classifySource</em> (
    ...
    <em>id</em> INTEGER,
    <em>feature1</em> ANYTYPE,
    <em>feature2</em> ANYTYPE,
    <em>feature3</em> ANYTYPE,
    ....................
    <em>featureN</em> ANYTYPE,
    ...
)</pre>

@usage

- Run the training algorithm on the source data:
  <pre>SELECT * FROM \ref c45_train(
    '<em>split_criterion_name</em>',
    '<em>training_table_name</em>', 
    '<em>result_tree_table_name</em>', 
    '<em>validation_table_name</em>',
    '<em>continuous_feature_names</em>',
    '<em>feature_col_names</em>',
    '<em>id_col_name</em>', 
    '<em>class_col_name</em>',
    '<em>confidence_level</em>',
    '<em>how2handle_missing_value</em>'
    '<em>max_num_iter</em>',
    '<em>max_tree_depth</em>',
    '<em>min_percent_mode</em>',
    '<em>min_percent_split</em>'
    '<em>verbosity</em>');
  </pre>
  This will create the decision tree output table storing an abstract object
  (representing the model) used for further classification. Column names:
  <pre>    
 id | tree_location | feature |    probability    |    ebp_coeff     | maxclass |    split_gain     | live | cat_size | parent_id |     jump      | is_feature_cont | split_value 
----+---------------+---------+-------------+-----------+----------+------------+------+----------+-----------+------+-----------------+-------------
                                                     ...</pre>    
    
- Run the classification function using the learned model: 
  <pre>SELECT * FROM \ref c45_classify(
    '<em>tree_table_name</em>', 
    '<em>classification_table_name</em>', 
    '<em>result_table_name</em>');</pre>
  This will create the result_table with the 
  classification results. 
  <pre> </pre> 

- Run the scorinf function to score the learned model against a validation data set:
  <pre>SELECT * FROM \ref c45_score(
    '<em>tree_table_name</em>',
    '<em>validation_table_name</em>',
	'<em>verbosity</em>');</pre>
  This will give a ratio of correctly classified items in the validation set.
  <pre> </pre>

- Run the display tree function using the learned model: 
  <pre>SELECT * FROM \ref c45_display(
    '<em>tree_table_name</em>');</pre>
  This will display the trained tree in human readable format. 
  <pre> </pre> 

- Run the clean tree function as below: 
  <pre>SELECT * FROM \ref c45_clean(
    '<em>tree_table_name</em>');</pre>
  This will clean up the learned model and all metadata.
  <pre> </pre> 

@examp

-# Prepare an input table/view, e.g.:
\verbatim
sql> select * from golf_data order by id;
 id | outlook  | temperature | humidity | windy  |    class     
----+----------+-------------+----------+--------+--------------
  1 | sunny    |          85 |       85 |  false |  Do not Play
  2 | sunny    |          80 |       90 |  true  |  Do not Play
  3 | overcast |          83 |       78 |  false |  Play
  4 | rain     |          70 |       96 |  false |  Play
  5 | rain     |          68 |       80 |  false |  Play
  6 | rain     |          65 |       70 |  true  |  Do not Play
  7 | overcast |          64 |       65 |  true  |  Play
  8 | sunny    |          72 |       95 |  false |  Do not Play
  9 | sunny    |          69 |       70 |  false |  Play
 10 | rain     |          75 |       80 |  false |  Play
 11 | sunny    |          75 |       70 |  true  |  Play
 12 | overcast |          72 |       90 |  true  |  Play
 13 | overcast |          81 |       75 |  false |  Play
 14 | rain     |          71 |       80 |  true  |  Do not Play
(14 rows)

\endverbatim
-# Train the decision tree model, e.g.:
\verbatim
sql> SELECT * FROM madlib.c45_clean('trained_tree_infogain');
sql> SELECT * FROM MADLIB.c45_train(
	   'infogain',                       -- split criterion_name
	   'golf_data',                      -- input table name
	   'trained_tree_infogain',          -- result tree name
	   null,                             -- validation table name
	   'temperature,humidity',           -- continuous feature names
	   'outlook,temperature,humidity,windy', -- feature column names
	   'id',                             -- id column name
	   'class',                          -- class column name
	   100,                              -- confidence level
	   'explicit',                       -- missing value preparation
	   100,                              -- max iterations
	   5,                                -- max tree depth
	   0.001,                            -- min percent mode
	   0.001,                            -- min percent split
	   0);                               -- verbosity
 training_set_size | tree_nodes | tree_depth |    cost_time    | split_criterion 
-------------------+------------+------------+-----------------+-----------------
                14 |          8 |          3 | 00:00:00.871805 | infogain
(1 row)
\endverbatim
-# Check few rows from the tree model table:
\verbatim
sql> select * from trained_tree_infogain order by id;
 id | tree_location | feature |    probability    |    ebp_coeff     | maxclass |    split_gain     | live | cat_size | parent_id |     jump      | is_feature_cont | split_value 
----+---------------+---------+-------------------+-----------+----------+-------------------+------+----------+-----------+---------------+-----------------+-------------
  1 | {0}           |       3 | 0.642857142857143 |         1 |        2 | 0.171033941880327 |    0 |       14 |         0 | [2:4]={2,3,4} | f               |            
  2 | {0,1}         |       4 |                 1 |         1 |        2 |                 0 |    0 |        4 |         1 |               | f               |            
  3 | {0,2}         |       4 |               0.6 |         1 |        2 | 0.673011667009257 |    0 |        5 |         1 | [2:3]={5,6}   | f               |            
  4 | {0,3}         |       2 |               0.6 |         1 |        1 | 0.673011667009257 |    0 |        5 |         1 | [2:3]={7,8}   | t               |          70
  5 | {0,2,1}       |       4 |                 1 |         1 |        2 |                 0 |    0 |        3 |         3 |               | f               |            
  6 | {0,2,2}       |       4 |                 1 |         1 |        1 |                 0 |    0 |        2 |         3 |               | f               |            
  7 | {0,3,1}       |       4 |                 1 |         1 |        2 |                 0 |    0 |        2 |         4 |               | f               |            
  8 | {0,3,2}       |       4 |                 1 |         1 |        1 |                 0 |    0 |        3 |         4 |               | f               |            
(8 rows)

\endverbatim
-# To display the tree with human readable format:
\verbatim
sql> select madlib.c45_display('trained_tree_infogain');
                                      c45_display                                      
---------------------------------------------------------------------------------------
     Root Node  : class(  Play)   num_elements(14)  predict_prob(0.642857142857143)          
         outlook:  = overcast : class( Play)   num_elements(4)  predict_prob(1)        
         outlook:  = rain : class( Play)   num_elements(5)  predict_prob(0.6)          
             windy:  =  false : class( Play)   num_elements(3)  predict_prob(1)        
             windy:  = true  : class(  Do not Play)   num_elements(2)  predict_prob(1)       
         outlook:  =  sunny      : class(  Do not Play)   num_elements(5)  predict_prob(0.6) 
             humidity:  <= 70 : class( Play)   num_elements(2)  predict_prob(1)      
             humidity:  > 70  : class(  Do not Play)   num_elements(3)  predict_prob(1)      
(1 row)

\endverbatim
-# To classify data with the learned model:
\verbatim
sql> select * from madlib.c45_classify
         'trained_tree_infogain',  -- name of the trained model
         'golf_data',              -- name of the table containing data to classify
         'classification_result'); -- name of the output table
 input_set_size |    cost_time    
----------------+-----------------
             14 | 00:00:00.247713
(1 row)
\endverbatim
-# Check classification results: 
\verbatim
sql> select t.id,t.outlook,t.temperature,t.humidity,t.windy,c.class from
    madlib.classification_result c,golf_data t where t.id=c.id order by id;
 id | outlook  | temperature | humidity | windy  | class 
----+------------+-------------+----------+-------+-------
  1 |  sunny     |          85 |       85 | f     |     1
  2 |  sunny     |          80 |       90 | t     |     1
  3 |  overcast  |          83 |       78 | f     |     2
  4 |  rain      |          70 |       96 | f     |     2
  5 |  rain      |          68 |       80 | f     |     2
  6 |  rain      |          65 |       70 | t     |     1
  7 |  overcast  |          64 |       65 | t     |     2
  8 |  sunny     |          72 |       95 | f     |     1
  9 |  sunny     |          69 |       70 | f     |     2
 10 |  rain      |          75 |       80 | f     |     2
 11 |  sunny     |          75 |       70 | t     |     2
 12 |  overcast  |          72 |       90 | t     |     2
 13 |  overcast  |          81 |       75 | f     |     2
 14 |  rain      |          71 |       80 | t     |     1
(14 rows)
(notes: The class value of 2 refers to 'do not play'. The class value of 1
refers to 'Play'. We plan to add a view to translate the numeric value to original
value soon.
\endverbatim

-# Score the data against a validation set:
\verbatim
sql> select * from madlib.c45_score(
        'trained_tree_infogain',
        'golf_data_validation',
        false);
 c45_score 
-----------
      0.98
(1 row)
\endverbatim


-# clean up the tree and metadata: 
\verbatim
testdb=# select madlib.c45_clean('trained_tree_infogain');
 c45_clean 
-----------
 
(1 row)
\endverbatim

@literature

[1] http://en.wikipedia.org/wiki/C4.5_algorithm

@sa File decision_tree.sql_in documenting the SQL functions.
*/

/*
 * This structure is used to store the result for the function of c45_train.
 *      training_set_size: It means how many records there exists in 
 *                         the training set.
 *      tree_nodes:        It is the number of total tree nodes.
 *      tree_depth:        It is the depth of the trained tree.
 *      cost_time:         It is the time consumed during training the tree.
 *      split_criterion:   It is the split criterion used to train the tree.
 */
DROP TYPE IF EXISTS MADLIB_SCHEMA.c45_train_result;
CREATE TYPE MADLIB_SCHEMA.c45_train_result AS 
    (   
    training_set_size        BIGINT,   
    tree_nodes               BIGINT,
    tree_depth               INT,
    cost_time                INTERVAL,
    split_criterion          TEXT
    );

/*
 * This structure is used to store the result for the function of c45_classify.
 *      input_set_size:    It means how many records there exists in 
 *                         the classification set.
 *      cost_time:         It is the time consumed during classifying the tree.
 */
DROP TYPE IF EXISTS MADLIB_SCHEMA.c45_classify_result;
CREATE TYPE MADLIB_SCHEMA.c45_classify_result AS 
    (   
    input_set_size        BIGINT,   
    cost_time             INTERVAL
    );

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__scv_aggr_sfunc
    (
    result              FLOAT8[],
    split_criterion     INT,
    feature_value       FLOAT8,
    class               FLOAT8,
    is_cont             boolean,
    le                  FLOAT8,
    gt                  FLOAT8,
    true_total_count    FLOAT8
    )CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__scv_aggr_sfunc
    (
    result              FLOAT8[],
    split_criterion     INT,
    feature_value       FLOAT8,
    class               FLOAT8,
    is_cont             boolean,
    le                  FLOAT8,
    gt                  FLOAT8,
    true_total_count    FLOAT8
    ) 
RETURNS FLOAT8[]  
AS 'MODULE_PATHNAME', 'scv_aggr_sfunc'
LANGUAGE C IMMUTABLE;

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__scv_aggr_prefunc
    (
    FLOAT8[], 
    FLOAT8[]
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__scv_aggr_prefunc
    (
    sfunc1_result     FLOAT8[],
    sfunc2_result     FLOAT8[]
    ) 
RETURNS FLOAT8[]
AS 'MODULE_PATHNAME', 'scv_aggr_prefunc'
LANGUAGE C STRICT IMMUTABLE;

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__scv_aggr_ffunc
    (
    FLOAT8[]
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__scv_aggr_ffunc
    (
    internal_result     FLOAT8[]
    ) 
RETURNS FLOAT8[]
AS 'MODULE_PATHNAME', 'scv_aggr_ffunc'
LANGUAGE C STRICT IMMUTABLE;

DROP TYPE IF EXISTS MADLIB_SCHEMA.__scv_aggr_result CASCADE;
CREATE TYPE MADLIB_SCHEMA.__scv_aggr_result AS
    (
    info_impurity       FLOAT8,
    class_prob          FLOAT8,
    class_id            INT,
    total_size          FLOAT8,
    is_cont             BOOLEAN
    );

DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.__scv_aggr
    (
    INT,
    FLOAT8,
    FLOAT8,
    boolean,
    FLOAT8,
    FLOAT8,
    FLOAT8
    ) CASCADE;
CREATE
m4_ifdef(`__GREENPLUM__', m4_ifdef(`__HAS_ORDERED_AGGREGATES__', `ORDERED'))
AGGREGATE MADLIB_SCHEMA.__scv_aggr
    (
    INT,
    FLOAT8,
    FLOAT8,
    boolean,
    FLOAT8,
    FLOAT8,
    FLOAT8
    ) 
(
  SFUNC=MADLIB_SCHEMA.__scv_aggr_sfunc,
  m4_ifdef(`__GREENPLUM__', m4_ifdef(`__HAS_ORDERED_AGGREGATES__', `', ``prefunc=MADLIB_SCHEMA.__scv_aggr_prefunc,''))
  FINALFUNC=MADLIB_SCHEMA.__scv_aggr_ffunc,
  STYPE=FLOAT8[],
  initcond = '{0,0,0,0,0,0,0,0,0,0,0,0,0,0}'
);

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__scv_aggr_wrapper
    (
    internal_result FLOAT8[]
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__scv_aggr_wrapper(
    internal_result FLOAT8[]
    )
RETURNS MADLIB_SCHEMA.__scv_aggr_result AS $$
DECLARE
    result              MADLIB_SCHEMA.__scv_aggr_result;
    split_criterion     INT;
    calc_pre_split      INT;
    is_cont             INT;
BEGIN
    split_criterion = internal_result[4];
    is_cont = internal_result[8];

    IF ( split_criterion = 1 ) THEN
        result.info_impurity = internal_result[5];
    ELSIF( split_criterion = 2 ) THEN
        result.info_impurity = internal_result[6];
    ELSE
        result.info_impurity = internal_result[7];
    END IF;
    
    result.class_id   = internal_result[9];
    result.class_prob = internal_result[10];
    result.total_size = internal_result[11];
    
    IF ((NOT result.total_size>0) OR
            result.class_prob<0 OR 
            result.class_prob>1) THEN
        RAISE EXCEPTION 'total_size is %,class_prob is %', 
            result.total_size,result.class_prob;
    END IF;

    IF (is_cont>0) THEN
        result.is_cont = 't';
    ELSE
        result.is_cont = 'f';
    END IF;
    RETURN result;
END 
$$ LANGUAGE PLPGSQL;


/*
 * Attribute info type
 * Parameters:
 *      fid:    feature index
 *      fval:   feature value
 *      is_cont: is continuous feature or not
 */
DROP TYPE IF EXISTS MADLIB_SCHEMA.__attr_info;
CREATE TYPE MADLIB_SCHEMA.__attr_info AS
    (
    fid     INT,
    fval    FLOAT8,
    is_cont  BOOLEAN
    );

/*
 * Customized coalesce function
 * Parameters:
 *      lhs:    the first attribute info type
 *      rhs:    the second attribute info type
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA__tfc
    (
    lhs     MADLIB_SCHEMA.__attr_info,
    rhs     MADLIB_SCHEMA.__attr_info
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__tfc
    (
    lhs     MADLIB_SCHEMA.__attr_info,
    rhs     MADLIB_SCHEMA.__attr_info
    )  
RETURNS MADLIB_SCHEMA.__attr_info AS $$
DECLARE
BEGIN
    IF (lhs.fval IS NULL) THEN
        RETURN rhs;
    END IF;
    
    RETURN lhs;
END
$$ LANGUAGE PLPGSQL;


/*
 * generate the training instances for current leaf nodes
 * Parameters:
 *      cycle                   If the cycle is 1, then we will add the (), (assigned_nid), (class, assigned_nid) into the grouping sets
 *      feature_stmt            The feature statement, used to construct the grouping set statement
 *      bracket_stmt            The bracket statement, used to construct the grouping set statement
 *      group_stmt              The group stmt statement, used to construct the grouping set statement
 *      input_table_name:       The name of the original table containing all the records in training set.
 *      selection_table_name:   It has two columns: id and assigned_nid. The id column is the same with that in input table.
 *                              Each record will be assigned to a node.
 *      verbosity               ture if print the debug info  
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__execute_grouping_stmt
    (
    cycle                INT,
    feature_stmt         TEXT,
    bracket_stmt         TEXT,
    group_stmt           TEXT,    
    input_table_name     TEXT, 
    selection_table_name TEXT,
    group_table_name     TEXT,
    verbosity            INT
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__execute_grouping_stmt
    (
    cycle                INT,
    feature_stmt         TEXT,
    bracket_stmt         TEXT,
    group_stmt           TEXT,    
    input_table_name     TEXT, 
    selection_table_name TEXT,
    group_table_name     TEXT,
    verbosity            INT
    )
RETURNS void AS $$
DECLARE
    group_result_stmt   TEXT := '';
    curstmt             TEXT;
BEGIN
    -- Due to the limitation of grouping sets (currently, the number of grouping sets  
    -- components can not exceed 70) we will use 'UNION ALL' to combine all the columns' 
    -- result together. Once the limitation is removed, we should only use one 
    -- SQL statement to get the grouping sets result. 
    IF (cycle = 1) THEN
        SELECT MADLIB_SCHEMA.__format
            (
            '(SELECT ((%, null%)::MADLIB_SCHEMA.__attr_info).*, class, COUNT(*), assigned_nid
            FROM 
                (SELECT t1.*, assigned_nid FROM % t1 LEFT JOIN % t2 ON t1.id = t2.id) s
            WHERE assigned_nid IS NOT NULL 
            GROUP BY GROUPING SETS(assigned_nid, (class, assigned_nid),%))',
            ARRAY[
            feature_stmt,
            bracket_stmt,
            input_table_name,
            selection_table_name,
            group_stmt
            ]
        ) INTO group_result_stmt;    
    ELSE
        SELECT MADLIB_SCHEMA.__format
            (
            '(SELECT ((%, null%)::MADLIB_SCHEMA.__attr_info).*, class, COUNT(*), assigned_nid
            FROM 
                (SELECT t1.*, assigned_nid FROM % t1, % t2 WHERE t1.id = t2.id) s
            WHERE assigned_nid IS NOT NULL 
            GROUP BY GROUPING SETS(%))',
            ARRAY[
            feature_stmt,
            bracket_stmt,
            input_table_name,
            selection_table_name,
            group_stmt
            ]
        ) INTO group_result_stmt;    
    END IF;        
    
    IF(verbosity > 0) THEN
        RAISE INFO 'group stmt:%', group_result_stmt;
    END IF;
        
    SELECT MADLIB_SCHEMA.__format
        (
            'INSERT INTO % %',
            ARRAY[
            group_table_name,
            group_result_stmt
            ]
        ) INTO curstmt;
    
    IF(verbosity > 0) THEN
        RAISE INFO 'Inser into auxiliary table stmt: %', curstmt;
    END IF;
            
    EXECUTE curstmt;
        
END
$$ LANGUAGE PLPGSQL;

/*
 * generate the training instances for current leaf nodes by grouping sets for greenplumn
 * Parameters:
 *      instance_table_name:    The output table name, which contains all the training set for current leaf nodes.
 *      class_table_name:       Name of class table
 *      group_table_name        The name of table containing grouping data info
 *      verbosity               ture if print the debug info  
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__create_training_instance
    (
    instance_table_name   TEXT,
    class_table_name      TEXT,
    group_table_name      TEXT,
    verbosity             INT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__create_training_instance
    (
    instance_table_name   TEXT,
    class_table_name      TEXT,
    group_table_name      TEXT,
    verbosity             INT
    )
RETURNS void AS $$
DECLARE
    curstmt             TEXT := '';
    window_func_stmt    TEXT := '';
BEGIN
m4_changequote(`>>>', `<<<')
m4_ifdef(>>>__POSTGRESQL_PRE_9_0__<<<, >>>
    -- old db version does not support starting from 1 following/curr row.
    window_func_stmt = 
            'CASE WHEN (is_cont) THEN 
                    sum(count) OVER 
                        (
                        PARTITION BY assigned_nid,class,fid ORDER BY fval 
                        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
                        ) 
                    ELSE 
                        count 
                    END AS le, 
            CASE WHEN (is_cont) THEN
            -- Any operation with NULL returns NULL.
            -- We need convert NULL to 0 before minus operation.
                    coalesce(
                      sum(count) OVER
                        ( 
                        PARTITION BY assigned_nid,class,fid ORDER BY fval 
                        ROWS BETWEEN UNBOUNDED PRECEDING AND  UNBOUNDED FOLLOWING),
                      0)
                    -
                    coalesce(
                      sum(count) OVER 
                        (
                        PARTITION BY assigned_nid,class,fid ORDER BY fval 
                        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
                        ),
                      0)
                    ELSE 
                        NULL 
                    END AS gt,';
<<<, >>>
    window_func_stmt = 
            'CASE WHEN (is_cont) THEN 
                    sum(count) OVER 
                        (
                        PARTITION BY assigned_nid,class,fid ORDER BY fval 
                        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
                        ) 
                    ELSE 
                        count 
                    END AS le, 
            CASE WHEN (is_cont) THEN 
                    sum(count) OVER
                        ( 
                        PARTITION BY assigned_nid,class,fid ORDER BY fval 
                        ROWS BETWEEN 1 FOLLOWING AND  UNBOUNDED FOLLOWING) 
                    ELSE 
                        NULL 
                    END AS gt,';
<<<)
m4_changequote(>>>`<<<, >>>'<<<)

    SELECT MADLIB_SCHEMA.__format
        (
            'INSERT INTO %(fid, fval, class, is_cont, split_value, le, gt, assigned_nid)  
            SELECT fid, fval, class, is_cont,
                CASE WHEN (is_cont) THEN 
                        fval::float8 
                    ELSE 
                        NULL 
                    END AS split_value, 
                %
                assigned_nid
            FROM (
                SELECT DISTINCT n1.fid, n1.fval, n1.is_cont, n1.class, n2.count, n1.assigned_nid 
                FROM 
                    (
                    SELECT fid, fval, is_cont, key as class, count, assigned_nid 
                    FROM 
                        (
                        SELECT DISTINCT fval, count, fid, is_cont, assigned_nid 
                        FROM %
                        ) AS t1
                    CROSS JOIN
                    (
                        SELECT key FROM MADLIB_SCHEMA.%
                    ) as t2
                ) AS n1 
                LEFT JOIN % n2   
                ON coalesce(n2.class,0) = coalesce(n1.class,0) AND
                    coalesce(n2.assigned_nid,0) = coalesce(n1.assigned_nid,0) AND
                    coalesce(n2.fid,0)= coalesce (n1.fid,0) AND
                    coalesce(n2.fval,0) = coalesce(n1.fval,0)  
            ) t;',
            ARRAY[
                instance_table_name,
                window_func_stmt,
                group_table_name,
                class_table_name,
                group_table_name
            ]
        ) INTO curstmt;

    IF(verbosity > 0) THEN
        RAISE INFO 'window function stmt: %', curstmt;
    END IF;
            
    EXECUTE curstmt;

    SELECT MADLIB_SCHEMA.__format
        (
            'INSERT INTO %(fid, fval, class, le, gt, assigned_nid)  
             SELECT fid, null, class, sum(le), null, assigned_nid from
             % where not is_cont group by fid,class,assigned_nid;',
            instance_table_name,
            instance_table_name
        ) INTO curstmt;

    EXECUTE curstmt;                    
END
$$ LANGUAGE PLPGSQL;


/*
 * generate the training instances for current leaf nodes by grouping sets for greenplumn
 * Parameters:
 *      input_table_name:       The name of the original table containing all the records in training set.
 *      metatable_name:         Contains the relevant information of the input table
 *      instance_table_name:    The output table name, which contains all the training set for current leaf nodes.
 *      selection_table_name:   It has two columns: id and assigned_nid. The id column is the same with that in input table.
 *                              Each record will be assigned to a node.
 *      verbosity               ture if print the debug info 
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__generate_training_instance_greenplum
    (
    input_table_name     TEXT, 
    metatable_name       TEXT,
    instance_table_name  TEXT,
    selection_table_name TEXT,
    verbosity            INT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__generate_training_instance_greenplum
    (
    input_table_name      TEXT, 
    metatable_name        TEXT,
    instance_table_name   TEXT,
    selection_table_name  TEXT,
    verbosity             INT
    )
RETURNS void AS $$
DECLARE
    curstmt             TEXT := '';
    result_rec          RECORD;
    feature_stmt        TEXT := '';
    bracket_stmt        TEXT := '';
    group_stmt          TEXT := '';
    group_result_stmt   TEXT := '';
    class_table_name    TEXT := '';
    group_table_name    TEXT := 'training_instance_aux';
    max_num_col_group   INT  := 30;
    index               INT  := 1;
    
BEGIN
    EXECUTE 'TRUNCATE ' || instance_table_name;
    EXECUTE 'TRUNCATE ' || group_table_name;
   
    SELECT MADLIB_SCHEMA.__format
        (
        'SELECT table_name 
        FROM MADLIB_SCHEMA.% 
        WHERE column_type = ''c''',
        metatable_name
        )
    INTO curstmt;
    
    EXECUTE curstmt INTO class_table_name;
              
    SELECT MADLIB_SCHEMA.__format
        (
            'SELECT id, column_name, is_cont 
            FROM MADLIB_SCHEMA.% 
            WHERE column_type = ''f'' ORDER BY id',
            metatable_name
        )
        INTO curstmt;
    
    FOR result_rec IN EXECUTE (curstmt) LOOP
        
        feature_stmt = feature_stmt || 
                        'MADLIB_SCHEMA.__tfc((' || 
                        result_rec.id || 
                        ',' ||
                        result_rec.column_name || 
                        '::FLOAT8,''' || 
                        MADLIB_SCHEMA.__to_char(result_rec.is_cont) ||
                        ''')::MADLIB_SCHEMA.__attr_info,';
        
        group_stmt = group_stmt || 
                    '(' ||
                    result_rec.column_name || 
                    ', assigned_nid)' || 
                    ',(' || 
                    result_rec.column_name || 
                    ',class, assigned_nid),';
                    
        bracket_stmt = bracket_stmt || ')';
        
        IF ((index % max_num_col_group) = 0) THEN
                PERFORM MADLIB_SCHEMA.__execute_grouping_stmt
                    (
                    (index / max_num_col_group)::INT4,
                    rtrim(feature_stmt, ','),
                    bracket_stmt,
                    rtrim(group_stmt, ','),                 
                    input_table_name,
                    selection_table_name,
                    group_table_name,
                    verbosity
                    );
                    
            feature_stmt = '';
            bracket_stmt = '';
            group_stmt   = '';
            
        END IF;
        
        index = index + 1;
    END LOOP;
    
    index = index - 1;
    
    IF ((index % max_num_col_group) != 0) THEN
        PERFORM MADLIB_SCHEMA.__execute_grouping_stmt
                (
                (index / max_num_col_group)::INT4 + 1,
                rtrim(feature_stmt, ','),
                bracket_stmt,
                rtrim(group_stmt, ','),                 
                input_table_name,
                selection_table_name,
                group_table_name,
                verbosity
                );  
    END IF;    
    
    PERFORM MADLIB_SCHEMA.__create_training_instance
        (
        instance_table_name,
        class_table_name,
        group_table_name,
        verbosity
        );
END
$$ LANGUAGE PLPGSQL;
	

/*
 * generate the training instances for current leaf nodes by union all statement for postgres
 * Parameters:
 *      input_table_name:       The name of the original table containing all the records in training set.
 *      metatable_name:         Contains the relevant information of the input table
 *      instance_table_name:    The output table name, which contains all the training set for current leaf nodes.
 *      selection_table_name:   It has two columns: id and assigned_nid. The id column is the same with that in input table.
 *                              Each record will be assigned to a node.
 *      verbosity               ture if print the debug info 
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__generate_training_instance_postgres
    (
    input_table_name     TEXT, 
    metatable_name       TEXT,
    instance_table_name  TEXT,
    selection_table_name TEXT,
    verbosity            INT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__generate_training_instance_postgres
    (
    input_table_name      TEXT, 
    metatable_name        TEXT,
    instance_table_name   TEXT,
    selection_table_name  TEXT,
    verbosity             INT
    )
RETURNS void AS $$
DECLARE
    curstmt             TEXT := '';
    result_rec          RECORD;
    feature_stmt        TEXT := '';
    bracket_stmt        TEXT := '';
    group_stmt          TEXT := '';
    group_result_stmt   TEXT := '';
    class_table_name    TEXT := '';
    name                TEXT;
    null_stmt           TEXT := '';    
    group_table_name    TEXT := 'training_instance_aux';
    max_num_col_group   INT  := 30;
    index               INT  := 1;
BEGIN
    EXECUTE 'TRUNCATE ' || instance_table_name;
    EXECUTE 'TRUNCATE ' || group_table_name;
    
    SELECT MADLIB_SCHEMA.__format
        (
        'SELECT table_name 
        FROM MADLIB_SCHEMA.% 
        WHERE column_type = ''c''',
        metatable_name
        )
    INTO curstmt;
    
    EXECUTE curstmt INTO class_table_name;

    EXECUTE 'DROP TABLE IF EXISTS training_instace_with_nid';
    SELECT MADLIB_SCHEMA.__format
        (
        'CREATE TEMP TABLE training_instace_with_nid 
         AS SELECT %, class, assigned_nid 
         FROM % t1 
         LEFT JOIN % t2 
         ON t1.id = t2.id
         m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY(assigned_nid)')',
        ARRAY[
            MADLIB_SCHEMA.__get_feature_name_in_selectstmt(metatable_name),
            input_table_name,
            selection_table_name
            ]
        )
    INTO curstmt;
    
    IF(verbosity > 0) THEN
        RAISE INFO 'training_instace_with_nid creation stmt: %', curstmt;
    END IF;
            
    EXECUTE curStmt;

    -- get all the feature names, and default value is null
    FOR name IN EXECUTE 
        ('SELECT column_name 
          FROM   MADLIB_SCHEMA.' || metatable_name || ' ' ||
         'WHERE  column_type = ''f'' ORDER BY id;'
        )
        LOOP
        null_stmt = null_stmt || 'null as ' || name || ', ';
    END LOOP;
    
    
    EXECUTE 'DROP TABLE IF EXISTS training_instace_count';
    
    SELECT MADLIB_SCHEMA.__format
        (
        'CREATE TEMP TABLE training_instace_count 
         AS 
         (
         SELECT % null as class, COUNT(*) as count, assigned_nid
         FROM training_instace_with_nid
         GROUP BY assigned_nid
         UNION ALL
         SELECT % class, COUNT(*), assigned_nid
         FROM training_instace_with_nid
         GROUP BY assigned_nid, class
         )
         m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY(assigned_nid)')',
        ARRAY[
            null_stmt,
            null_stmt
            ]
        )
    INTO curstmt;

    IF(verbosity > 0) THEN
        RAISE INFO 'training_instace_count creation stmt: %', curstmt;
    END IF;
        
    EXECUTE curStmt;
    
    SELECT MADLIB_SCHEMA.__format
        (
            'SELECT id, column_name, is_cont 
            FROM MADLIB_SCHEMA.% 
            WHERE column_type = ''f'' ORDER BY id',
            metatable_name
        )
    INTO curstmt;
    
    FOR result_rec IN EXECUTE (curstmt) LOOP
        
        feature_stmt = feature_stmt || 
                        'MADLIB_SCHEMA.__tfc((' || 
                        result_rec.id || 
                        ',' ||
                        result_rec.column_name || 
                        '::FLOAT8,''' || 
                        MADLIB_SCHEMA.__to_char(result_rec.is_cont) ||
                        ''')::MADLIB_SCHEMA.__attr_info,';
        
        bracket_stmt = bracket_stmt || ')';

       SELECT MADLIB_SCHEMA.__format
            (
            'INSERT INTO training_instace_count(%, class, count, assigned_nid) 
             (
             SELECT %, null, COUNT(*), assigned_nid
             FROM training_instace_with_nid
             GROUP BY assigned_nid, %
             UNION ALL
             SELECT %, class, COUNT(*), assigned_nid
             FROM training_instace_with_nid
             GROUP BY assigned_nid, %, class
             )',
            ARRAY[
                result_rec.column_name,
                result_rec.column_name,
                result_rec.column_name,
                result_rec.column_name,
                result_rec.column_name
                ]
            )
        INTO curstmt;
        
        IF(verbosity > 0) THEN
            RAISE INFO 'training_instace_count insertion for each feature stmt: %', curstmt;
        END IF;
    
        EXECUTE curStmt;
    END LOOP;
    
    SELECT MADLIB_SCHEMA.__format
        (
        'INSERT INTO %
        SELECT ((% null%)::MADLIB_SCHEMA.__attr_info).*, class, count, assigned_nid
        FROM training_instace_count
        WHERE assigned_nid IS NOT NULL',
        ARRAY[
        group_table_name,
        feature_stmt,
        bracket_stmt
        ]
    ) INTO curstmt;    
    
    IF(verbosity > 0) THEN
        RAISE INFO 'Inser into auxiliary table stmt: %', curstmt;
    END IF;
            
    EXECUTE curstmt;
      
    PERFORM MADLIB_SCHEMA.__create_training_instance
        (
        instance_table_name,
        class_table_name,
        group_table_name,
        verbosity
        );
                         
END
$$ LANGUAGE PLPGSQL;

DROP TYPE IF EXISTS MADLIB_SCHEMA.__rep_type CASCADE;
CREATE TYPE MADLIB_SCHEMA.__rep_type AS
    (
    numOfOrgClasses BIGINT[]
    );

DROP TYPE IF EXISTS MADLIB_SCHEMA.__rep_result CASCADE;
CREATE TYPE MADLIB_SCHEMA.__rep_result AS
    (
    maxclass  BIGINT,
    isreplace BIGINT
    );

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__rep_aggr_class_count_sfunc
    (
    BIGINT[],
    INT, 
    INT, 
    INT
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__rep_aggr_class_count_sfunc
    (
    class_count_array       BIGINT[],        
    classified_class        INT,
    original_class          INT,
    max_num_of_classes      INT
    ) 
RETURNS BIGINT[]
AS 'MODULE_PATHNAME', 'rep_aggr_class_count_sfunc'
LANGUAGE C IMMUTABLE;

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__rep_aggr_class_count_prefunc
    (
    BIGINT[],
    BIGINT[]
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__rep_aggr_class_count_prefunc
    (
    BIGINT[],
    BIGINT[]
    ) 
RETURNS BIGINT[]
AS 'MODULE_PATHNAME', 'rep_aggr_class_count_prefunc'
LANGUAGE C IMMUTABLE;

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__rep_aggr_class_count_ffunc
    (
    BIGINT[]
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__rep_aggr_class_count_ffunc
    (
    class_count_array       BIGINT[]        
    ) 
RETURNS BIGINT[]
AS 'MODULE_PATHNAME', 'rep_aggr_class_count_ffunc'
LANGUAGE C STRICT IMMUTABLE;

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__rep_aggr_class_count_wrapper
    (
    BIGINT[]
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__rep_aggr_class_count_wrapper
    (
    internal_result BIGINT[]
    ) 
RETURNS MADLIB_SCHEMA.__rep_result AS $$
DECLARE
    result MADLIB_SCHEMA.__rep_result;
BEGIN
    IF(internal_result IS NOT NULL) THEN
       result.maxclass = internal_result[1];
       result.isreplace = internal_result[2];        
    ELSE
       result.maxclass = -1;
       result.isreplace = -1;
    END IF;
    RETURN result;
END
$$ LANGUAGE PLPGSQL;

DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.__rep_aggr_class_count
    (
    INT,
    INT,
    INT
    );
CREATE AGGREGATE MADLIB_SCHEMA.__rep_aggr_class_count
    (
    INT,
    INT,
    INT
    ) 
(
  SFUNC=MADLIB_SCHEMA.__rep_aggr_class_count_sfunc,
  m4_ifdef(`__GREENPLUM__', `prefunc=MADLIB_SCHEMA.__rep_aggr_class_count_prefunc,')
  FINALFUNC=MADLIB_SCHEMA.__rep_aggr_class_count_ffunc,
  STYPE=BIGINT[]
);

/*
 * This type is used to store information for the calculated best split 
 *
 * Parameters:
 *      feature:              The ID of the selected feature.
 *      probability:          The predicted probability of our chosen class.
 *      maxclass:             The ID of the class chosen by the algorithm
 *      infoGain:             The information gain.
 *      live:                 1- For the chosen split, we should split further.
 *                            0- For the chosen split, we shouldn't split further.
 *      ebp_coeff:            total error for error-based pruning.
 *      is_cont_feature:      whether the selected feature is continuous.
 *      split_value:          If the selected feature is continuous, it specifies
 *                            the split value. Otherwise, it is of no use.
 *      distinct_features:    The number of distinct values for the selected feature.
 */
DROP TYPE IF EXISTS MADLIB_SCHEMA.__best_split_result CASCADE;
CREATE TYPE MADLIB_SCHEMA.__best_split_result AS
    (
    node_id             INT,
	feature             INT,
	probability         FLOAT,
	maxclass            INTEGER,
	infogain            FLOAT,
	live                INT,
	ebp_coeff           FLOAT,
    is_cont_feature     BOOLEAN,
    split_value         FLOAT,
    distinct_features   INT,
    total_size          INT
    );

/*
 * This function find the best split and return the information.
 *
 * Parameters:
 *  feature_dimensions:     The total number of different features
 *  featureValCountStr:     A string in csv format. Each element is 
 *                          a numeric value equal to the count of 
 *                          distinct features for each feature.
 *  distinct_classes:       Total number of different classes.
 *  assigned_nid:              It specifies which part of records should 
 *                          be used to calculate the best split.
 *  table_name:             The name of the table containing the training
 *                          set.
 *  confidence_level:       This parameter is used by the 'Error-Based Pruning'.
 *                          Please refer to the paper for detailed definition.
 *                          The paper's name is 'Error-Based Pruning of Decision  
 *                          Trees Grown on Very Large Data Sets Can Work!'.
 *  feature_table_name:     Is is the name of one internal table, which contains
 *                          meta data for each feature.
 *  sp_criterion:           It defines the split criterion to be used.
 *                          (1- information gain. 2- gain ratio. 3- gini)
 *  continue_gow:           It specifies whether we should still grow the tree
 *                          on the selected branch.
 * Return:
 *  The return is of the type of MADLIB_SCHEMA.__best_split_result, which contains the information
 *  for best split. Please refer to that structure for detailed definition. 
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__find_best_split
    (
    feature_dimensions      INT, 
    distinct_classes        INT,  
    selection_begin         INT, 
    selection_cnt           INT,
    table_name              TEXT, 
    confidence_level        FLOAT,
    feature_table_name      TEXT, 
    sp_criterion            INT, 
    continue_gow            INT
    );
    
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__find_best_split
    (
    feature_dimensions      INT, 
    distinct_classes        INT,  
    selection_begin         INT, 
    selection_cnt           INT, 
    table_name              TEXT, 
    confidence_level        FLOAT,
    feature_table_name      TEXT, 
    sp_criterion            INT, 
    continue_gow            INT
    ) 
RETURNS SETOF MADLIB_SCHEMA.__best_split_result AS $$
DECLARE
	total_size         INT;
	result             MADLIB_SCHEMA.__best_split_result;
	has_cont_text      TEXT := 't';
	curstmt            TEXT := '';
	result_rec         RECORD;
	exec_begin         TIMESTAMP;
    best_answer        FLOAT8[];
BEGIN	 
	exec_begin = clock_timestamp();
	
	TRUNCATE info_impurity;
	
	SELECT MADLIB_SCHEMA.__format
        (
        'INSERT INTO info_impurity 
        SELECT assigned_nid, 
             MAX(le) 
        FROM %
        WHERE fid IS NULL 
        AND assigned_nid IS NOT NULL 
        AND class IS NULL
        GROUP BY assigned_nid;',
        table_name
        )
    INTO curstmt;
    
    EXECUTE curstmt;
    
m4_changequote(`>>>', `<<<')
m4_ifdef(>>>__HAS_ORDERED_AGGREGATES__<<<, >>>
    -- With ordered aggregate support, we can use this facility.
    SELECT MADLIB_SCHEMA.__format
        (
        'SELECT s1.assigned_nid, 
            MAX(ARRAY[s1.info_impurity, s1.fid, coalesce(s1.split_value, ''NaN''::FLOAT8), s1.class_prob, s1.class_id, s1.total_size]::FLOAT8[]) as info
        FROM (
            SELECT assigned_nid,fid,split_value,
                (MADLIB_SCHEMA.__scv_aggr_wrapper(
                    MADLIB_SCHEMA.__scv_aggr
                        (%,fval,class,is_cont,le,gt, total_size
                        ORDER BY fval desc,class desc)
                    )::MADLIB_SCHEMA.__scv_aggr_result).*
            FROM (
                SELECT t1.*, t2.total_size FROM % t1, info_impurity t2
                WHERE t1.assigned_nid = t2.assigned_nid AND 
                        (t1.fid IS NOT NULL) AND (t1.assigned_nid IS NOT NULL AND t2.assigned_nid IS NOT NULL)
                ) y 
            GROUP BY assigned_nid,fid,split_value 
            ) s1  
        GROUP BY s1.assigned_nid
        ORDER BY s1.assigned_nid;',
        MADLIB_SCHEMA.__to_char(sp_criterion),
        table_name
        )
    INTO curstmt;   
<<<, >>>
    -- Without ordered aggregate support, we use window function as a workaround.
    -- The performance is much worse than ordered aggregate.
    SELECT MADLIB_SCHEMA.__format
        (
        'SELECT s1.assigned_nid, 
            MAX(ARRAY[s1.info_impurity, s1.fid, coalesce(s1.split_value, ''NaN''::FLOAT8), s1.class_prob, s1.class_id, s1.total_size]::FLOAT8[]) as info
        FROM (
            SELECT * FROM (
              SELECT assigned_nid,fid,split_value,
                row_number() over
                        (PARTITION BY assigned_nid,fid,split_value 
                         ORDER BY fval asc,class asc) as row_num,
                (MADLIB_SCHEMA.__scv_aggr_wrapper(
                    MADLIB_SCHEMA.__scv_aggr
                        (%,fval,class,is_cont,le,gt, total_size) over
                        (PARTITION BY assigned_nid,fid,split_value 
                         ORDER BY fval desc,class desc)
                    )::MADLIB_SCHEMA.__scv_aggr_result).*
              FROM (
                SELECT t1.*, t2.total_size FROM % t1, info_impurity t2
                WHERE t1.assigned_nid = t2.assigned_nid AND 
                        (t1.fid IS NOT NULL) AND (t1.assigned_nid IS NOT NULL AND t2.assigned_nid IS NOT NULL)
                ) y 
              ) k where row_num =1 
            ) s1    
        GROUP BY s1.assigned_nid
        ORDER BY s1.assigned_nid;',
        MADLIB_SCHEMA.__to_char(sp_criterion),
        table_name
        )
    INTO curstmt;   
<<<)
m4_changequote(>>>`<<<, >>>'<<<)
    
    -- s1.info_impurity[1], s1.fid[2], s1.split_value[3], 
    -- s2.class_prob[4], s2.class_id[5], s1.total_size[6]
    FOR result_rec IN EXECUTE (curstmt) LOOP
        result.node_id              = result_rec.assigned_nid;
        best_answer                 = result_rec.info;
        result.feature              = best_answer[2];
        result.maxclass             = best_answer[5];
        result.probability          = best_answer[4];
        result.infogain             = best_answer[1];
        result.total_size           = best_answer[6];
        result.distinct_features    = MADLIB_SCHEMA.__distinct_feature_value(feature_table_name, result.feature);
        
        IF (result.probability > 0.999999999 OR float8le(result.infogain, 0)) THEN
            result.live = 0;
        ELSE
            result.live = 1;
        END IF;
        
        result.ebp_coeff = MADLIB_SCHEMA.__ebp_calc_errors(result.total_size, result.probability, confidence_level); 
        
        IF (best_answer[3] = 'NaN'::FLOAT8) THEN
            result.split_value      = NULL;
        ELSE
            result.split_value      = best_answer[3];
        END IF;
        result.is_cont_feature  = (result.split_value IS NOT NULL);
        
        RETURN next result;
    END LOOP;
    	
	RETURN;
END
$$ LANGUAGE PLPGSQL;

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__jump_aggr_sfunc
    (
    INT[], 
    INT, 
    INT
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__jump_aggr_sfunc
    (
    INT[], 
    INT, 
    INT
    ) 
RETURNS INT[] AS $$
DECLARE
	temp INT[];
BEGIN
	temp = $1;
	temp[$2+1] = $3;
	
	RETURN temp;
END
$$ LANGUAGE PLPGSQL;

DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.__jump_aggr
    (
    INT, 
    INT
    );
CREATE AGGREGATE MADLIB_SCHEMA.__jump_aggr
    (
    INT, 
    INT
    ) 
(
  SFUNC=MADLIB_SCHEMA.__jump_aggr_sfunc,
  STYPE=INT[]
);


/*
 *   For training one decision tree, we need some internal tables
 *   to store intermediate results. This function creates those
 *   tables. Moreover, this function also creates the tree table
 *   specified by user.
 *
 *   Parameters:
 *      result_tree_table_name: The name of the tree specified by user.      
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__create_tree_tables
    (
    TEXT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__create_tree_tables
    (
    result_tree_table_name TEXT
    ) 
RETURNS void AS $$ 
BEGIN
    -- The training algorithm starts by eliminating all redundant points, 
    -- by producing a smaller subset of unique, weighted points,
    -- which was stored by the two tables below.
    --  Columns:
    --      id:             It is used to uniquely identify one record.
    --      feature:        It is used to store the value of one unique record.
    --      class:          The class of that record.
    --      weight:         The count of such a record.
    --      assigned_nid:   This field is not used while removing redundant records.
    --                      It is used to train a decision tree.


	DROP TABLE IF EXISTS training_instance CASCADE;
	CREATE TEMP TABLE training_instance
	(
    	fid            INTEGER,
    	fval           FLOAT8,
    	class          INTEGER,
    	is_cont        BOOLEAN,
    	split_value    FLOAT8,
    	le             BIGINT,
    	gt             BIGINT,
    	assigned_nid   BIGINT
	) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (fid, fval)');
	
    DROP TABLE IF EXISTS training_instance_aux CASCADE;
    CREATE TEMP TABLE training_instance_aux
    (
        fid             INTEGER,
        fval            FLOAT8,
        is_cont         BOOLEAN,
        class           INTEGER,
        count           INTEGER,
        assigned_nid    BIGINT
    ) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (fid, fval)');

    DROP TABLE IF EXISTS info_impurity CASCADE;
    CREATE TEMP TABLE info_impurity
    (
        assigned_nid         BIGINT,
        total_size           BIGINT
    )m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (assigned_nid)');
    
    -- The table below stores the decision tree information just constructed.
    -- It is an internal table, which contains some redundant nodes. 
    -- In the last step, we will remove those redundant nodes and move the
    -- useful records to the table specified by user.
    -- Columns:
    --      id:             Tree node id
    --      tree_location:  Set of values that lead to this branch. 
    --                      0 is the initial point (no value). But this path 
    --                      does not specify which feature was used
    --                      for the branching.
    --      feature:        Which element of the feature vector was used for 
    --                      branching at this node. Notice that this feature is not 
    --                      used in the current tree_location. It will be added 
    --                      in the next step.
    --      probability:    If forced to make a call for a dominant class 
    --                      at a given point this would be the confidence of the 
    --                      call (this is only an estimated value).
    --      maxclass:       If forced to make a call for a dominant class 
    --                      at a given point this is the selected class.
    --      split_gain:     Information gain computed using entropy (at this 
    --                      node), also used to determine termination of the branch.
    --      live:           Indication that the branch is still growing. 1 means "live". 
    --      cat_size:       Number of data point at this node.
    --      parent_id:      Id of the parent branch.
    --      jump:           Location of children for each feature value. 
    --                      Result such as [2:3]={2,3}, should be read: 
    --                      jump['feature value'+1], so in this case there were no 
    --                      0-value points for this feature. For value 1 jump to 2; 
    --                      for value 2 jump to 3;
    --      is_feature_cont: It specifies whether the selected feature is a continuous feature.
    --      split_value:    For continuous feature, it specifies the split value. Otherwise, 
    --                      it is of no meaning and fixed to 0.    
    --
	DROP TABLE IF EXISTS tree_internal CASCADE;
	CREATE TEMP TABLE tree_internal
	(
    	id              SERIAL,
    	tree_location   INT[],
    	feature         INT,
    	probability     FLOAT,
    	ebp_coeff       FLOAT,
    	maxclass        INTEGER,
    	split_gain      FLOAT,
    	live            INT,
    	cat_size        INT,
    	parent_id       INT,
    	jump            INT[],
        is_feature_cont BOOLEAN,
        split_value     FLOAT
	) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (id)');

    -- The table below stores the final decision tree information.
    -- It is an the table specified by users. 
    -- Please refer the table above for detailed column definition.
	EXECUTE 'DROP TABLE IF EXISTS '||result_tree_table_name||' CASCADE;';
	EXECUTE 'CREATE TABLE '||result_tree_table_name||E'
	(
    	id              SERIAL,
    	tree_location   INT[],
    	feature         INT,
    	probability     FLOAT,
    	ebp_coeff       FLOAT,
    	maxclass        INTEGER,
    	split_gain      FLOAT,
    	live            INT,
    	cat_size        INT,
    	parent_id       INT,
    	jump            INT[],
        is_feature_cont BOOLEAN,
        split_value     FLOAT    
	) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (id)');';

    -- These two auxiliary internal tables help to
    -- remove redundant tree nodes and move the useful
    -- node to the final tree table.
	DROP TABLE IF EXISTS auxiliary_tree_info CASCADE;
	CREATE TEMP TABLE auxiliary_tree_info
	(
		id          INT,
		new_id      INT,
		parent_id   INT
	) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (id)');

	DROP TABLE IF EXISTS auxiliary_tree_info2 CASCADE;
	CREATE TEMP TABLE auxiliary_tree_info2
	(
		id          INT,
		new_id      INT,
		parent_id   INT
	) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (id)');

END 
$$ LANGUAGE PLPGSQL;

/*
 * Prune the trained tree with "Reduced Error Pruning" algorithm
 *  
 * Parameters:
 *      tree_table_name:    The name of the table containing the tree.
 *      validation_table:   The name of the table containing validation set.
 *      max_num_classes:    The count of different classes.
 */
DROP FUNCTION IF EXISTS  MADLIB_SCHEMA.__rep_prune_tree
    (
    tree_table_name     TEXT, 
    validation_table    TEXT, 
    max_num_classes     INT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__rep_prune_tree
    (
    tree_table_name     TEXT, 
    validation_table    TEXT, 
    max_num_classes     INT
    ) 
RETURNS void AS $$
DECLARE
    num_parent_ids          INTEGER;
    cf_table_name           TEXT := '_ct';
    encoded_table_name      TEXT;
    metatable_name          TEXT ;
    curstmt                 TEXT;
    id_col_name             TEXT;
    class_col_name          TEXT;
    classify_result         TEXT;
    temp_text               TEXT;
    n                       INT;
BEGIN
    metatable_name  = MADLIB_SCHEMA.__get_metatable_name(tree_table_name);
    id_col_name     = MADLIB_SCHEMA.__get_id_column_name(metatable_name);
    class_col_name  = MADLIB_SCHEMA.__get_class_column_name(metatable_name);
    
    -- the value of class column in validation table must in the KV table
    SELECT MADLIB_SCHEMA.__format
        (
        'SELECT COUNT(*) 
         FROM %
         WHERE % NOT IN
            (SELECT % FROM MADLIB_SCHEMA.% WHERE % IS NOT NULL)',
        ARRAY[
            validation_table,
            class_col_name,
            class_col_name,
            MADLIB_SCHEMA.__get_classtable_name(metatable_name),
            class_col_name
        ]
        )
    INTO curstmt;
    
    EXECUTE curstmt INTO n;
    
    PERFORM MADLIB_SCHEMA.__assert
            (
                n = 0,
                'the value of class column in validation table must in training table'
            ); 

    cf_table_name   = MADLIB_SCHEMA.__get_schema_name(tree_table_name)    ||
                      MADLIB_SCHEMA.__strip_schema_name(tree_table_name)  || 
                      cf_table_name;
    classify_result = cf_table_name;   
                        
    SELECT MADLIB_SCHEMA.__c45_classify_internal
    (
        validation_table, 
        tree_table_name, 
        classify_result, 
        't',
        'f'
    ) INTO encoded_table_name;

    -- after encoding in classification, class_col_name is fixed to class
    class_col_name  = 'class';

m4_changequote(`>>>', `<<<')
m4_ifdef(>>>__GREENPLUM_PRE_4_1__<<<, >>>
    EXECUTE 'DROP TABLE IF EXISTS c45_rep_pong CASCADE';
    EXECUTE 'CREATE TEMP TABLE c45_rep_pong AS SELECT * FROM ' || classify_result || ' LIMIT 0 DISTRIBUTED BY(id)';
<<<)
m4_changequote(>>>`<<<, >>>'<<<)
    
    LOOP
        DROP TABLE IF EXISTS selected_parent_ids_rep;
        CREATE TEMP TABLE selected_parent_ids_rep
        (
            parent_id BIGINT,
            maxclass  INT
        ) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (parent_id)');
       
        SELECT MADLIB_SCHEMA.__format
            (
                'INSERT INTO selected_parent_ids_rep 
                SELECT parent_id, (t.g).maxclass as maxclass 
                FROM 
                (
                    SELECT parent_id, 
                           MADLIB_SCHEMA.__rep_aggr_class_count_wrapper(MADLIB_SCHEMA.__rep_aggr_class_count
                                (c.class, s.%, % )) as g 
                    FROM % c, % s 
                    WHERE c.id=s.% 
                    GROUP BY parent_id
                ) t 
                WHERE (t.g).isreplace >= 0 AND 
                      t.parent_id IN 
                      (
                          Select parent_id FROM % 
                          WHERE parent_id NOT IN
                              (
                                  Select parent_id  
                                  FROM % 
                                  WHERE jump IS NOT NULL
                              ) and id <> 1
                      );',
                  ARRAY[
                      class_col_name,
                      MADLIB_SCHEMA.__to_char(max_num_classes),
                      classify_result,
                      encoded_table_name,
                      id_col_name,
                      tree_table_name,
                      tree_table_name
                  ]
              )
              INTO curstmt;
            
        EXECUTE curstmt;
                        
        EXECUTE 'SELECT parent_id FROM selected_parent_ids_rep limit 1;' INTO num_parent_ids;
        IF (num_parent_ids IS NULL)  THEN
            EXIT;
        END IF;

m4_changequote(`>>>', `<<<')
m4_ifdef(`__GREENPLUM_PRE_4_1__', >>>
        -- for GPDB4.0, update operation can't distribute data across segments
        -- we use two tables to update the data
        IF (classify_result = 'c45_rep_pong') THEN
            temp_text = cf_table_name;
        ELSE
            temp_text =  'c45_rep_pong';
        END IF;
        
        EXECUTE 'TRUNCATE ' ||  temp_text;
        SELECT MADLIB_SCHEMA.__format
            (
            'INSERT INTO %(id, class, parent_id, leaf_id)
             SELECT m.id,  t.maxclass, t.parent_id, t.id
             FROM % m, % t
             WHERE t.id IN (SELECT parent_id FROM selected_parent_ids_rep) AND
             m.parent_id = t.id',
            ARRAY[
                temp_text,
                classify_result,
                tree_table_name
            ]
            )
        INTO curstmt;
        
        EXECUTE curstmt;
        
        classify_result = temp_text;
<<<, >>>
        SELECT MADLIB_SCHEMA.__format
            (
                'UPDATE % m set class = t.maxclass, parent_id = t.parent_id,leaf_id = t.id  FROM % t
                 WHERE t.id IN (SELECT parent_id FROM selected_parent_ids_rep) AND
                 m.parent_id=t.id',
                classify_result,
                tree_table_name
            )
        INTO curstmt;
        EXECUTE curstmt;        
<<<)
m4_changequote(>>>`<<<, >>>'<<<)
            
        SELECT MADLIB_SCHEMA.__format
            (
                'DELETE FROM % WHERE parent_id IN (SELECT parent_id FROM selected_parent_ids_rep)',
                tree_table_name
            )
            INTO curstmt;
        
        EXECUTE curstmt;

        SELECT MADLIB_SCHEMA.__format
            (
                'UPDATE % t1 SET jump = NULL, maxclass = t2.maxclass 
                 FROM selected_parent_ids_rep t2
                 WHERE t1.id = t2.parent_id;',
                tree_table_name
            )
            INTO curstmt;
        
        EXECUTE curstmt;
        
    END LOOP;
    
    EXECUTE 'DROP TABLE IF EXISTS ' || encoded_table_name || ' CASCADE;';
END
$$ LANGUAGE PLPGSQL;


/*
 * Calculates the total errors used by Error Based Pruning (EBP).
 *
 * Parameters:
 *      total:              the number of total cases represented by the node being processed.
 *      prob:               the probability to mis-classify cases represented by the child nodes
 *                          if they are pruned with EBP.
 *      confidence_level:   a certainty factor to calculate the confidence limits
 *                          for the probability of error using the binomial theorem.
 * Return:
 *      The computed total error
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__ebp_calc_errors
    (
    total            FLOAT8,
    prob             FLOAT8,
    confidence_level FLOAT8
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__ebp_calc_errors
    (
    total               FLOAT8,
    prob                FLOAT8,
    confidence_level    FLOAT8
    ) RETURNS FLOAT8 
AS 'MODULE_PATHNAME', 'ebp_calc_errors'
LANGUAGE C STRICT IMMUTABLE;

/*
 * Prune the trained tree with " Error based Pruning" algorithm
 *  
 * Parameters:
 *      tree_table_name:    The name of the table containing the tree.
 */
DROP FUNCTION IF EXISTS  MADLIB_SCHEMA.__ebp_prune_tree
    (
    tree_table_name TEXT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__ebp_prune_tree
    (
    tree_table_name TEXT
    ) 
RETURNS void AS $$
DECLARE
    num_parent_ids INTEGER;
    curstmt TEXT;
BEGIN
    LOOP
        DROP TABLE IF EXISTS selected_parent_ids_ebp;
        CREATE TEMP TABLE selected_parent_ids_ebp(parent_id BIGINT) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY(parent_id)');
        
        SELECT MADLIB_SCHEMA.__format
            (
                'INSERT INTO selected_parent_ids_ebp 
                SELECT s.parent_id as parent_id 
                FROM  
                (
                    Select parent_id, sum(ebp_coeff) as ebp_coeff 
                    FROM 
                    (
                        Select parent_id, ebp_coeff 
                        FROM % 
                        WHERE parent_id NOT IN 
                            (
                            Select parent_id  FROM % WHERE jump IS NOT NULL
                            )  and id <> 1
                    ) m 
                    GROUP BY m.parent_id
                 ) s 
                 LEFT JOIN  % p 
                    ON p.id = s.parent_id 
                 WHERE  p.ebp_coeff < s.ebp_coeff;',
                 tree_table_name,
                 tree_table_name,
                 tree_table_name
            )
            INTO curstmt;
         
        EXECUTE curstmt;
                 
        EXECUTE 'SELECT parent_id FROM selected_parent_ids_ebp LIMIT 1;' INTO num_parent_ids;

        IF (num_parent_ids IS NULL)  THEN
            EXIT;
        END IF;
        
        SELECT MADLIB_SCHEMA.__format
            (
                'DELETE FROM % 
                WHERE parent_id IN 
                    (SELECT parent_id FROM selected_parent_ids_ebp)',
                tree_table_name
            )
            INTO curstmt;
            
        EXECUTE curstmt;
        
        SELECT MADLIB_SCHEMA.__format
            (
                'UPDATE %  
                SET jump = NULL 
                WHERE id IN 
                    (SELECT parent_id FROM selected_parent_ids_ebp)',
                tree_table_name
            )
            INTO curstmt;
            
        EXECUTE curstmt;
        
    END LOOP;
END
$$ LANGUAGE PLPGSQL;

/*
 * Generate the final trained tree
 *  
 * Parameters:
 *      result_tree_table_name:    The name of the table containing the tree.
 */
DROP FUNCTION IF EXISTS  MADLIB_SCHEMA.__generate_final_tree(TEXT);
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__generate_final_tree
    (
    result_tree_table_name TEXT
    ) 
RETURNS void AS $$
DECLARE
    tree_size INTEGER;
BEGIN
    TRUNCATE auxiliary_tree_info;
    TRUNCATE auxiliary_tree_info2;
    
    EXECUTE 'DELETE FROM tree_internal WHERE COALESCE(cat_size,0) = 0';
    
    EXECUTE 'SELECT count(*) FROM tree_internal' INTO tree_size;
    EXECUTE 'INSERT INTO auxiliary_tree_info (id, parent_id, new_id) SELECT id, 
            MAX(parent_id), ('||tree_size||'+1) - count(1) 
            OVER(ORDER BY id DESC ROWS UNBOUNDED PRECEDING) FROM tree_internal GROUP BY id';
            
    EXECUTE 'INSERT INTO auxiliary_tree_info2 (id, parent_id, new_id) 
            SELECT  g2.id,g.new_id,g2.new_id FROM auxiliary_tree_info g, 
            auxiliary_tree_info g2  WHERE g.id = g2.parent_id';
    
    TRUNCATE auxiliary_tree_info;
    EXECUTE 'TRUNCATE '||result_tree_table_name||';';
    
    EXECUTE 'INSERT INTO '|| result_tree_table_name||' SELECT n.new_id, 
            g.tree_location, g.feature, g.probability, 
            g.ebp_coeff,g.maxclass, g.split_gain, g.live, g.cat_size, 
            n.parent_id, g.jump, g.is_feature_cont, g.split_value 
            FROM tree_internal g, auxiliary_tree_info2 n WHERE n.id = g.id';
            
    EXECUTE 'INSERT INTO '||result_tree_table_name
            ||' SELECT * FROM tree_internal WHERE id = 1';
            
    TRUNCATE tree_internal;
    EXECUTE 'INSERT INTO tree_internal (id, jump) SELECT parent_id, 
            MADLIB_SCHEMA.__jump_aggr(tree_location[array_upper(tree_location,1)], id) FROM '
            ||result_tree_table_name||' GROUP BY parent_id';
            
    TRUNCATE auxiliary_tree_info2;
    EXECUTE 'UPDATE '||result_tree_table_name||' k 
            SET jump = g.jump FROM tree_internal g WHERE g.id = k.id';
    TRUNCATE tree_internal;
END
$$ LANGUAGE PLPGSQL;

/*
 * This function trains a tree 
 *  
 * Parameters:
 *      split_criterion:            This parameter specifies which split criterion 
 *                                  should be used for tree construction and pruning. The  
 *                                  valid values are infogain, gainratio, and gini.
 *      training_table_name:        Name of the table/view with the source data
 *      result_tree_table_name:     The name of the table where the resulting DT will be stored.
 *      validation_table_name:      The validation table used for pruning tree. 
 *      id_col_name:                Name of the column containing id of each point.
 *      class_col_name:             Name of the column containing correct class of each point.
 *      confidence_level:           A statistical confidence interval of the resubstitution error.
 *      max_num_iter:               Max number of branches to follow (e.g. 2000)
 *      max_tree_depth:             Maximum decision tree depth 
 *      min_percent_mode:           Specifies the minimum number of cases required in a child node
 *      min_percent_split:          specifies the minimum number of cases required in a node  
 *                                  in order for a further split to be possible.
 *      verbosity:                  If True (or 1) will run in verbose mode
 *
 * Return:
 *      One summary result for training tree. Please refer to the structure of
 *      'MADLIB_SCHEMA.c45_train_result' for detailed definition.
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__train_tree
    (
    split_criterion         TEXT,
    training_table_name     TEXT, 
    training_table_meta     TEXT,
    result_tree_table_name  TEXT,
    validation_table_name   TEXT,
    id_col_name             TEXT, 
    class_col_name          TEXT, 
    confidence_level        FLOAT, 
    max_num_iter            INT, 
    max_tree_depth          INT,
    min_percent_mode        FLOAT,
    min_percent_split       FLOAT,
    verbosity               INT
    ) 
RETURNS MADLIB_SCHEMA.c45_train_result AS $$
DECLARE
    feature_dimension           INT;
    num_live_nodes              INT;
    assigned_nid                INT;
    location                    INT[];
    temp_location               INT[];
    num_classes                 INT;
    max_iter                    INT := max_num_iter;
    max_depth                   INT := max_tree_depth;
    answer                      MADLIB_SCHEMA.__best_split_result;
    location_size               INT;
    max_id                      INT;
    exec_begin                  TIMESTAMP;
    find_best_time              INTERVAL;
    begin_find_best_time        TIMESTAMP;
    data_transfer_time          INTERVAL;
    begin_data_transfer         TIMESTAMP;
    total_size                  FLOAT;
    sp_crit                     INT := 1;
    curstmt                     TEXT := '';
    grow_tree                   INT := 0;
    ret                         MADLIB_SCHEMA.c45_train_result;
    selection_visit             BOOLEAN[];
    selection_index             INT := 1;
    select_max_id               INT := 0;
    best_time_cmp               BOOLEAN := 'f';
BEGIN   
    exec_begin = clock_timestamp();
    find_best_time = exec_begin - exec_begin;

    ret.split_criterion = split_criterion;   
                  
    IF(split_criterion = 'infogain') THEN
        sp_crit = 1;
    ELSIF (split_criterion = 'gainratio') THEN
        sp_crit = 2;
    ELSIF (split_criterion = 'gini') THEN
        sp_crit = 3;
    ELSE
        RAISE EXCEPTION '%', 'Invalid split criterion!';
    END IF;

    PERFORM MADLIB_SCHEMA.__create_tree_tables(result_tree_table_name);
    
    EXECUTE 'SELECT count(*) FROM '|| training_table_name ||';' INTO total_size;
    
    IF(verbosity > 0) THEN
        RAISE INFO 'INPUT TABLE SIZE: %', total_size;
    END IF;
    
    ret.training_set_size = total_size;
    
    EXECUTE 'DROP TABLE IF EXISTS instance_selection CASCADE';
    EXECUTE 'CREATE TEMP TABLE instance_selection AS SELECT id, 1::BIGINT as assigned_nid FROM ' || 
	     training_table_name || 
            ' m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (id)')';
    
    EXECUTE 'CREATE INDEX instance_selection_selection_index ON instance_selection (assigned_nid)';


    feature_dimension = MADLIB_SCHEMA.__num_of_feature(training_table_meta);

    num_classes = MADLIB_SCHEMA.__num_of_class(training_table_meta); 
    
    IF(verbosity > 0) THEN
        RAISE INFO 'NUMBER OF CLASSES IN THE TRAINING SET %', num_classes;
    END IF;
    
    IF(num_classes < 2 OR num_classes > 8000000)THEN
        RAISE EXCEPTION 'The number of classes must be in range 2 to 8,000,000!';
    END IF;
    
    EXECUTE 'INSERT INTO tree_internal (tree_location, feature, probability, maxclass, 
        split_gain, live, cat_size, parent_id) 
        VALUES(ARRAY[0], 0, 1, 1, 1, 1, 0, 0);';
               
    location_size = 0;
    
    begin_data_transfer = clock_timestamp();
    
    LOOP
        EXECUTE 'SELECT COUNT(id) FROM tree_internal WHERE live = 1' INTO num_live_nodes;
        
        IF ((max_depth < 0) OR (num_live_nodes < 1)) THEN
            IF(verbosity > 0) THEN
                RAISE INFO 'EXIT: LIMIT tree depth: % OR LIMIT iteration: % OR NO NODES LEFT', max_depth, max_iter;
            END IF;
            
            EXIT;
        END IF;
        
        max_depth = max_depth - 1;
        IF(verbosity > 0) THEN
            RAISE INFO 'current level: %', max_tree_depth - max_depth;
        END IF;

        EXECUTE 'SELECT id FROM tree_internal WHERE live = 1 ORDER BY id LIMIT 1;' INTO assigned_nid;
        EXECUTE 'SELECT id FROM tree_internal WHERE live = 1 ORDER BY id DESC LIMIT 1;' INTO max_id;
        
        FOR selection_index IN 1..(max_id - assigned_nid + 1) LOOP
            selection_visit[selection_index] = 'f';
        END LOOP;
        
        select_max_id = max_id;
        
m4_changequote(`>>>', `<<<')
m4_ifdef(>>>__GREENPLUM__<<<, >>>
        PERFORM MADLIB_SCHEMA.__generate_training_instance_greenplum
            (
            training_table_name,
            training_table_meta,
            'training_instance',
            'instance_selection',
            verbosity
            );
<<<, >>>
        PERFORM MADLIB_SCHEMA.__generate_training_instance_postgres
            (
            training_table_name,
            training_table_meta,
            'training_instance',
            'instance_selection',
            verbosity
            );
<<<)
m4_changequote(>>>`<<<, >>>'<<<)
        
        begin_find_best_time = clock_timestamp();
        
        best_time_cmp = 't';
        
        FOR answer IN 
                    (
                    SELECT * FROM MADLIB_SCHEMA.__find_best_split
                        (
                        feature_dimension, 
                        num_classes, 
                        assigned_nid, 
                        max_id - assigned_nid + 1, 
                        'training_instance',
                        confidence_level,
                        training_table_meta,
                        sp_crit,
                        grow_tree
                        )
                    ) 
                    LOOP
            IF (0 = answer.feature) THEN
                CONTINUE;
            END IF;
            
            -- mark this node id was visited
            selection_visit[answer.node_id - assigned_nid + 1] = 't';
            
            IF (answer.is_cont_feature) THEN
                IF(verbosity > 0) THEN
                    RAISE INFO 'selected feature is continuous';
                    RAISE INFO 'answer:%', answer;  
                END IF;
                
                EXECUTE 'UPDATE tree_internal 
                        SET feature = '||answer.feature||',
                            probability = '|| answer.probability||',
                            maxclass = '||answer.maxclass||',
                            split_gain = '||answer.infogain||',
                            ebp_coeff = '||answer.ebp_coeff||',
                            cat_size = '||answer.total_size|| E',
                            live = 0,
                            is_feature_cont = ''t'',
                            split_value = '|| answer.split_value ||
                         ' WHERE id =' || answer.node_id || ';';
            ELSE
                IF (verbosity > 0) THEN
                    RAISE INFO 'selected feature is discrete';
                    RAISE INFO 'answer:%', answer;       
                END IF;   
                  
                EXECUTE 'UPDATE tree_internal 
                        SET feature = '||answer.feature||',
                            probability = '|| answer.probability||',
                            maxclass = '||answer.maxclass||',
                            split_gain = '||answer.infogain||',
                            ebp_coeff = '||answer.ebp_coeff||',
                            cat_size = '||answer.total_size|| E',
                            live = 0,
                            is_feature_cont = ''f'',
                            split_value = null
                        WHERE id =' || answer.node_id || ';';
            END IF;   
            
            -- no need to grow tree with the attribute 
            -- if comes up the maximum number;
            -- if its percent is lower than minimum split value
            IF (answer.node_id >= max_num_iter) THEN
                max_iter = 0;
                CONTINUE;
            END IF;

            IF (answer.total_size < min_percent_split * total_size) THEN
                CONTINUE;
            END IF;
                        
            -- grow the tree
            EXECUTE 'SELECT gt.tree_location FROM tree_internal gt 
                    WHERE gt.id =' || answer.node_id ||';' 
                    INTO location;
            
            --here insert live determination function 
            IF (answer.live > 0 and answer.is_cont_feature = 'f') THEN 
                IF(verbosity > 0) THEN
                    RAISE INFO 'determine live for discrete';
                END IF;  
                            
                FOR i IN 1..answer.distinct_features LOOP
                    temp_location = location;
                    temp_location[array_upper(temp_location,1)+1] = i;
                    EXECUTE 'INSERT INTO tree_internal (tree_location, feature, probability, 
                        maxclass, split_gain, live, parent_id) 
                        VALUES(ARRAY['                          || 
                            array_to_string(temp_location, ',') ||
                            '], 0, 1, 1, 1, 1, '                || 
                            answer.node_id                      ||
                          ');';
                END LOOP;
                
                SELECT MADLIB_SCHEMA.__format
                    (
                    'UPDATE instance_selection s
                    SET assigned_nid = % + %
                    FROM % t
                    WHERE assigned_nid = % and t.id = s.id;',
                    ARRAY[
                        MADLIB_SCHEMA.__get_feature_name(answer.feature,training_table_meta),
                        MADLIB_SCHEMA.__to_char(select_max_id),
                        training_table_name,
                        MADLIB_SCHEMA.__to_char(answer.node_id)
                    ]
                    ) 
                    INTO curstmt;
                    
                EXECUTE curstmt;
                
                select_max_id = select_max_id + answer.distinct_features;
                
            ELSIF (answer.live > 0 and answer.is_cont_feature = 't') THEN
                IF(verbosity > 0) THEN
                    RAISE INFO 'determine live for continuous';
                END IF;  
                FOR i IN 1..2 LOOP
                    temp_location = location;
                    temp_location[array_upper(temp_location,1)+1] = i;
                    EXECUTE 'INSERT INTO tree_internal (tree_location, feature, probability, 
                        maxclass, split_gain, live, parent_id) 
                        VALUES(ARRAY['                          ||
                            array_to_string(temp_location, ',') ||
                            '], 0, 1, 1, 1, 1, '                || 
                            answer.node_id                      ||
                         ');';
                    
                END LOOP;

                SELECT MADLIB_SCHEMA.__format
                    (
                    'UPDATE instance_selection s
                    SET assigned_nid = (
                                    CASE WHEN (% < %) THEN
                                        2
                                    ELSE
                                        1
                                    END
                                    )
                                    + %
                    FROM % t
                    WHERE assigned_nid = % and t.id = s.id;',
                    ARRAY[
                        MADLIB_SCHEMA.__to_char(answer.split_value),
                        MADLIB_SCHEMA.__get_feature_name(answer.feature, training_table_meta),
                        MADLIB_SCHEMA.__to_char(select_max_id),
                        training_table_name,
                        MADLIB_SCHEMA.__to_char(answer.node_id)
                    ]
                    ) 
                    INTO curstmt;
                    
                EXECUTE curstmt;
                               
                select_max_id = select_max_id + 2;
            ELSE
                -- answer.live = 0
                -- process the min_percent_mode
                IF (total_size * min_percent_mode > answer.total_size) THEN
                    --RAISE INFO '%', 'min_percent_mode';
                    EXECUTE 'DELETE FROM tree_internal WHERE id = ' || answer.node_id || ';';
                END IF;
            END IF;                      
        END LOOP;
        
        -- Remove the nodes which contains no data
        FOR selection_index IN assigned_nid..max_id  LOOP
            IF (NOT selection_visit[selection_index - assigned_nid + 1]) THEN
                IF (verbosity > 0) THEN
                    RAISE NOTICE '%, %', 'unneccesary node!', selection_index;
                END IF;
                
                EXECUTE 'DELETE FROM tree_internal WHERE id = '|| selection_index ||';';
            END IF;
        END LOOP;
               
        IF(verbosity > 0) THEN
            RAISE INFO 'computation time in this level:%',(clock_timestamp() - begin_find_best_time);
        END IF;
        IF (best_time_cmp) THEN
            find_best_time = find_best_time + (clock_timestamp() - begin_find_best_time);
            best_time_cmp = 'f';
        END IF;
    END LOOP;
    
    data_transfer_time =  (clock_timestamp() - begin_data_transfer) - find_best_time;
    
    PERFORM MADLIB_SCHEMA.__generate_final_tree( result_tree_table_name );
    
    IF (confidence_level < 100.0) THEN
       EXECUTE 'SELECT MADLIB_SCHEMA.__ebp_prune_tree(' || quote_literal(result_tree_table_name) || ');';
    END IF;
    
    IF (validation_table_name IS NOT NULL) THEN
       PERFORM MADLIB_SCHEMA.__rep_prune_tree(result_tree_table_name, validation_table_name , num_classes);
    END IF;
    
    EXECUTE 'select count(*) from '||result_tree_table_name||';' into ret.tree_nodes;
    EXECUTE 'select max(array_upper(tree_location,1)) from '||result_tree_table_name||';' into ret.tree_depth;
    IF(verbosity > 0) THEN
        /*
         * We measure the time with the dataset of kddcup1999, which can be found at
         * http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html.
         * We test the training with that dataset on two machines with two X5570@2.93GHz CPUs
         * and 48GB memory running x86_64 GNU/Linux and GPDB 4.2. 
         * 
         * We use three different configurations, 4 segments, 8 segments and 16 segments.
         * The 16 segments configuration spans over two machines connected with 10GB Ethernet HUB
         * The test results are as follows:
         * 
         * 4 segments results:
         * Data conversion:            5 minutes 43 seconds
         * Find best time:             30 minutes 10 seconds
         * olap query/windows func:    43 minutes 5 seconds
         * Total train:                79 minutes 06 seconds
         * 
         * 8 segments results:
         * Data conversion:            4 minutes 10 seconds
         * Find best time:             19 minutes 14 seconds
         * olap query/windows func:    29 minutes 58 seconds
         * Total train:                53 minutes 40 seconds
    
         * 16 segments results:
         * Data conversion:            2 minutes 24 seconds
         * Find best time:             10 minutes 29 seconds
         * olap query/windows func:    16 minutes 17 seconds
         * Total train:                29 minutes 13 seconds
         */

        RAISE INFO 'total of find best time: %', find_best_time;
        RAISE INFO 'total of olap query/windows func time: %', data_transfer_time;
        RAISE INFO 'total of pruning time: %', 
            (clock_timestamp() - begin_data_transfer -find_best_time - data_transfer_time);
        RAISE INFO 'total of __train_tree time: %', clock_timestamp() - exec_begin;        
    END IF;
    
    ret.cost_time = clock_timestamp() - exec_begin;
    RETURN ret;
END
$$ LANGUAGE PLPGSQL;

/**
 * @brief This is the long form API of training tree with all specified parameters.
 *
 * @param split_criterion_name      This parameter specifies which split criterion 
 *                                  should be used for tree construction and pruning. 
 *                                  The valid values are infogain, gainratio, or gini.
 * @param training_table_name       Name of the table/view with the source data
 * @param result_tree_table_name    The name of the table where the resulting DT will be stored.
 * @param validation_table_name     The validation table used for pruning tree. 
 * @param continuous_feature_names  A comma-separated list of the names of the features whose values are continuous. 
 * @param feature_col_names         A comma-separated list of names of the table columns, each of which defines a feature.
 * @param id_col_name               Name of the column containing id of each point.
 * @param class_col_name            Name of the column containing correct class of each point.
 * @param confidence_level          A statistical confidence interval of the resubstitution error.
 * @param how2handle_missing_value The way to handle missing value. The valid value is 'explicit' or 'ignore'.
 * @param max_num_iter              Max number of branches to follow (e.g. 2000)
 * @param max_tree_depth            Maximum decision tree depth 
 * @param min_percent_mode          Specifies the minimum number of cases required in a child node
 * @param min_percent_split         specifies the minimum number of cases required in a node in order for 
 *                                  a further split to be possible.
 * @param verbosity                 If True (or 1) will run in verbose mode
 *
 * @return
 * - <tt>training_set_size</tt> - Number of items in the training set
 * - <tt>tree_nodes</tt> - Number of nodes in the resulting tree
 * - <tt>cost_time</tt> - Time spent training the tree
 * - <tt>split_criterion</tt> - Split criterion used to build the tree.
 *
 * This will also create the output table with name specified via 'result_tree_table_name'.
 * The result table can be queried directly or displayed using c45_display.
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_train
    (
    split_criterion             TEXT,
    training_table_name         TEXT, 
    result_tree_table_name      TEXT,
    validation_table_name       TEXT, 
    continuous_feature_names    TEXT, 
    feature_col_names           TEXT, 
    id_col_name                 TEXT, 
    class_col_name              TEXT, 
    confidence_level            FLOAT,
    how2handle_missing_value    TEXT, 
    max_num_iter                INT, 
    max_tree_depth              INT, 
    min_percent_mode            FLOAT,
    min_percent_split           FLOAT, 
    verbosity                   INT
    ) 
RETURNS MADLIB_SCHEMA.c45_train_result AS $$
DECLARE
    cont_feature_col_names          TEXT[];
    feature_name_array              TEXT[];
    exec_begin                      TIMESTAMP;
    tree_schema_name                TEXT;
    tree_table_name                 TEXT;
    training_encoded_table_name     TEXT;
    training_metatable_name         TEXT;
    h2hmv_routine_id                INT := 1;  
    ret                             MADLIB_SCHEMA.c45_train_result;
BEGIN   
    exec_begin = clock_timestamp();
    
    IF (verbosity < 1) THEN
        -- get rid of the messages whose severity level is lower than 'WARNING'
        SET client_min_messages = WARNING;
    END IF;
    
    PERFORM MADLIB_SCHEMA.__assert
        (
            (split_criterion IS NOT NULL)   AND
            (
             split_criterion = 'infogain'   OR 
             split_criterion = 'gainratio'  OR 
             split_criterion = 'gini'
            ),
            'split_criterion must be infogain, gainratio or gini'
        );
            
    PERFORM MADLIB_SCHEMA.__assert
        (
                id_col_name IS NOT NULL AND
                class_col_name IS NOT NULL AND
                length(btrim(id_col_name, ' ')) > 0 AND
                length(btrim(class_col_name, ' ')) > 0,
                'invalid id column name or class column name'
            );
                        
    PERFORM MADLIB_SCHEMA.__assert
        (
            training_table_name IS NOT NULL AND
            MADLIB_SCHEMA.__table_exists
                (
                    MADLIB_SCHEMA.__get_schema_name(training_table_name),
                    MADLIB_SCHEMA.__strip_schema_name(training_table_name)
                ),
            'the specified training table' || coalesce('<' || training_table_name || '> does not exist', ' is NULL')
        );

    PERFORM MADLIB_SCHEMA.__assert
        (
            MADLIB_SCHEMA.__column_exists
                (
                    MADLIB_SCHEMA.__get_schema_name(training_table_name),
                    MADLIB_SCHEMA.__strip_schema_name(training_table_name),
                    lower(btrim(id_col_name, ' '))
                ),
            'the specified training table<' || training_table_name || '> does not have column ''' || id_col_name || ''''
        );

    PERFORM MADLIB_SCHEMA.__assert
        (
            MADLIB_SCHEMA.__column_exists
                (
                    MADLIB_SCHEMA.__get_schema_name(training_table_name),
                    MADLIB_SCHEMA.__strip_schema_name(training_table_name),
                    lower(btrim(class_col_name, ' '))
                ),
            'the specified training table<' || training_table_name || '> does not have column ''' || class_col_name || ''''
        );

    PERFORM MADLIB_SCHEMA.__assert
        (
            validation_table_name IS NULL OR
            MADLIB_SCHEMA.__table_exists
                (
                    MADLIB_SCHEMA.__get_schema_name(validation_table_name),
                    MADLIB_SCHEMA.__strip_schema_name(validation_table_name)
                ),
             'the specified validation table<' || validation_table_name || '> does not exist'
        );

    PERFORM MADLIB_SCHEMA.__assert
            (
                (result_tree_table_name IS NOT NULL) AND
                (
                 NOT MADLIB_SCHEMA.__table_exists
                    (
                        MADLIB_SCHEMA.__get_schema_name(result_tree_table_name),
                        MADLIB_SCHEMA.__strip_schema_name(result_tree_table_name)
                    )
                ),
                'the specified result tree table' || coalesce('<' || result_tree_table_name || '> exists', ' is NULL')
            );   

    -- the maximum length of an identifier 63
    -- encoding table name convension:  <tree table schema name>_<tree table name>_ed
    -- data info table name convension: <tree table schema name>_<tree table name>_di
    -- the KV table name convension:    <tree table schema name>_<tree table name>_<####>
    -- therefore, the maximum length of '<tree table schema name>_<tree table name>' is 58
    PERFORM MADLIB_SCHEMA.__assert
        (
            length(
                MADLIB_SCHEMA.__get_schema_name(result_tree_table_name) || 
                '_'                                                     || 
                MADLIB_SCHEMA.__strip_schema_name(result_tree_table_name)) <= 58,
            'the maximum length of ''<tree table schema name>_<tree table name>'' is 58'
        );
                    
    PERFORM MADLIB_SCHEMA.__assert
            (
                (confidence_level IS NOT NULL)      AND 
                float8ge(confidence_level, 0.001)   AND 
                float8le(confidence_level, 100), 
                'confidence level value must be in range from 0.001 to 100'
            );
            
    PERFORM MADLIB_SCHEMA.__assert
            (
                how2handle_missing_value = 'ignore' OR how2handle_missing_value = 'explicit',
                'how2handle_missing_value must be ignore or explicit!'
            );    

    PERFORM MADLIB_SCHEMA.__assert
            (
                max_num_iter        IS NOT NULL AND
                max_tree_depth      IS NOT NULL AND
                max_num_iter        > 0         AND
                max_tree_depth      > 0,
                'invalid parameters value for max_num_iter, max_tree_depth'
            );

    PERFORM MADLIB_SCHEMA.__assert
            (
                min_percent_mode IS NOT NULL    AND
                float8ge(min_percent_mode, 0)   AND
                float8le(min_percent_mode, 1),
                'min_percent_mode value must be in range from 0 to 1'
            );

    PERFORM MADLIB_SCHEMA.__assert
            (
                min_percent_split IS NOT NULL   AND                
                float8ge(min_percent_split, 0)  AND
                float8le(min_percent_split, 1),                
                'min_percent_split value must be in range from 0 to 1'
            );
            
    PERFORM MADLIB_SCHEMA.__assert
            (
                verbosity IS NOT NULL,                
                'verbosity must be non-null'
            );
                                                   
    IF (how2handle_missing_value = 'ignore') THEN
        h2hmv_routine_id = 1;
    ELSE
        h2hmv_routine_id = 2;
    END IF;
    
    PERFORM MADLIB_SCHEMA.__insert_into_traininginfo
                (
                result_tree_table_name,
                training_table_name,
                null,
                null,
                validation_table_name,
                how2handle_missing_value,
                split_criterion
                );

    cont_feature_col_names = MADLIB_SCHEMA.__csvstr_to_array(continuous_feature_names);
    
    IF ( verbosity > 0 ) THEN
        RAISE INFO 'continuous features:%', cont_feature_col_names;
    END IF;
    
    tree_table_name = MADLIB_SCHEMA.__strip_schema_name(result_tree_table_name);
    tree_schema_name = MADLIB_SCHEMA.__get_schema_name(result_tree_table_name);
    
    IF(verbosity > 0) THEN
        RAISE INFO 'table name after strip schema:%',tree_table_name;
    END IF;
    
    training_encoded_table_name = 'MADLIB_SCHEMA.' || tree_schema_name || '_' ||tree_table_name || '_ed';
    training_metatable_name =  tree_schema_name || '_' || tree_table_name || '_di';
 
    IF(verbosity > 0) THEN
        RAISE INFO 'Before encoding: %', clock_timestamp() - exec_begin;
    END IF; 
        
    PERFORM MADLIB_SCHEMA.__encode_tabular_table
        (
        training_table_name,
        lower(id_col_name),
        MADLIB_SCHEMA.__csvstr_to_array(feature_col_names),
        lower(class_col_name),
        cont_feature_col_names,
        training_encoded_table_name,
        training_metatable_name,
        h2hmv_routine_id,
        verbosity > 0
        );
    
    PERFORM  MADLIB_SCHEMA.__set_encode_and_metatable_name
                ( 
                result_tree_table_name, 
                training_metatable_name,
                training_encoded_table_name
                );
                
    IF(verbosity > 0) THEN
            RAISE INFO 'After encoding: %', clock_timestamp() - exec_begin;
            RAISE INFO 'successfully encode the input table :%',training_encoded_table_name;
    END IF;    

    ret = MADLIB_SCHEMA.__train_tree
            (
            split_criterion ,
            training_encoded_table_name ,
            training_metatable_name,
            result_tree_table_name ,
            validation_table_name , 
            'id', 
            'class', 
            confidence_level,
            max_num_iter , 
            max_tree_depth , 
            min_percent_mode ,
            min_percent_split,
            verbosity
            );

    IF ( verbosity > 0 ) THEN
            RAISE INFO 'Training Total Time: %', clock_timestamp() - exec_begin;
    END IF;
    
    ret.cost_time = clock_timestamp() - exec_begin;
    RETURN ret;
END
$$ LANGUAGE PLPGSQL;

/**
 * @brief C45 train algorithm in short form.
 *
 * @param split_criterion_name This parameter specifies which split criterion
 * should be used for tree construction and pruning.
 * The valid values are infogain, gainratio, or gini.
 * @param training_table_name Name of the table/view with the source data
 * @param result_tree_table_name The name of the table where the resulting DT will be stored.
 *
 * @return
 * - <tt>training_set_size</tt> - Number of items in the training set
 * - <tt>tree_nodes</tt> - Number of nodes in the resulting tree
 * - <tt>cost_time</tt> - Time spent training the tree
 * - <tt>split_criterion</tt> - Split criterion used to build the tree.
 *
 * This will also create the output table with name specified via 'result_tree_table_name'.
 * The result table can be queried directly or displayed using c45_display.
 *
 * @note  
 * This calls the long form of C45 with the following default parameters:
 * - validation_table_name := NULL
 * - continuous_feature_names := NULL
 * - id_column_name := 'id'
 * - class_column_name := 'class'
 * - confidence_level = 25
 * - how2handle_missing_value = 'explicit'
 * - max_num_iter := 2000
 * - max_tree_deapth := 10
 * - min_percent_mode := 0.001
 * - min_percent_split := 0.01
 * - verbosity := false
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_train
    (
    split_criterion         TEXT,
    training_table_name     TEXT, 
    result_tree_table_name  TEXT
    ) 
RETURNS MADLIB_SCHEMA.c45_train_result AS $$
DECLARE
    ret MADLIB_SCHEMA.c45_train_result;
BEGIN   
    ret = MADLIB_SCHEMA.c45_train(
        split_criterion,
        training_table_name, 
        result_tree_table_name,
        null,
        null,       
        null,
        'id',
        'class',
        25,
        'explicit',
        2000,
        10,
        0.001,
        0.01,
        0           
    );
    
    RETURN ret;
END
$$ LANGUAGE PLPGSQL;

/**
 * @brief C45 train algorithm in short form.
 *
 * @param split_criterion_name This parameter specifies which split criterion
 * should be used for tree construction and pruning.
 * The valid values are infogain, gainratio, or gini.
 * @param training_table_name Name of the table/view with the source data
 * @param result_tree_table_name The name of the table where the resulting DT will be stored.
 * @param validation_table_name The validation table used for pruning tree.
 * @param continuous_feature_names A comma-separated list of the names of the features whose values are continuous.
 * @param feature_col_names A comma-separated list of names of the table columns, each of which defines a feature.
 * @param id_col_name Name of the column containing id of each point.
 * @param class_col_name Name of the column containing correct class of each point.
 * @param confidence_level A statistical confidence interval of the resubstitution error.
 * @param how2handle_missing_value The way to handle missing value. The valid value is 'explicit' or 'ignore'.
 *
 * @return
 * - <tt>training_set_size</tt> - Number of items in the training set
 * - <tt>tree_nodes</tt> - Number of nodes in the resulting tree
 * - <tt>cost_time</tt> - Time spent training the tree
 * - <tt>split_criterion</tt> - Split criterion used to build the tree.
 *
 * This will also create the output table with name specified via 'result_tree_table_name'.
 * The result table can be queried directly or displayed using c45_display.
 *
 * @note     
 * This calls the long form of C45 with the following default parameters:
 * - max_num_iter := 2000
 * - max_tree_deapth := 10
 * - min_percent_mode := 0.001
 * - min_percent_split := 0.01
 * - verbosity := false
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_train
    (
    split_criterion             TEXT,
    training_table_name         TEXT, 
    result_tree_table_name      TEXT,
    validation_table_name       TEXT, 
    continuous_feature_names    TEXT, 
    feature_col_names           TEXT, 
    id_col_name                 TEXT, 
    class_col_name              TEXT, 
    confidence_level            FLOAT,
    how2handle_missing_value    TEXT
    ) 
RETURNS MADLIB_SCHEMA.c45_train_result AS $$
DECLARE
    ret MADLIB_SCHEMA.c45_train_result;
BEGIN   
    ret = MADLIB_SCHEMA.c45_train
            (
            split_criterion,
            training_table_name, 
            result_tree_table_name,
            validation_table_name , 
            continuous_feature_names , 
            feature_col_names , 
            id_col_name , 
            class_col_name , 
            confidence_level,
            how2handle_missing_value,
            2000,
            10,
            0.001,
            0.01,
            0           
            );
    RETURN ret;
END
$$ LANGUAGE PLPGSQL;

/**
 * @brief Display the trained decision tree model with rules.
 *
 * @param   tree_table_name     Name of the table containing the tree's information
 * @param   verbosity           If >= 1 will run in verbose mode
 *
 * @return  The rule representation text for a decision tree.
 *
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.c45_genrule
    (
    tree_table_name TEXT,
    verbosity       INT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_genrule
    (
    tree_table_name TEXT,
    verbosity       INT
    ) 
RETURNS SETOF TEXT AS $$
DECLARE
    metatable_name          TEXT;
    classtable_name         TEXT;
    class_column_name       TEXT;
    rec                     RECORD;
    fvalue_stmt             TEXT;
    feature_rule            TEXT;
    curstmt                 TEXT;
    union_stmt              TEXT := NULL;
    exec_begin              TIMESTAMP;
    exec_leaves_rule        INTERVAL;
    exec_internode_rule     INTERVAL;
    exec_union              INTERVAL;    
BEGIN

    IF (verbosity < 1) THEN
        -- get rid of the messages whose severity level is lower than 'WARNING'
        SET client_min_messages = WARNING;
    END IF;
    
    PERFORM MADLIB_SCHEMA.__assert
            (
                (tree_table_name IS NOT NULL) AND
                (
                 MADLIB_SCHEMA.__table_exists
                    (
                        MADLIB_SCHEMA.__get_schema_name(tree_table_name),
                        MADLIB_SCHEMA.__strip_schema_name(tree_table_name)
                    )
                ),
                'the specified tree table' || coalesce('<' || tree_table_name || '> does not exists', ' is NULL')
            );   
            
    PERFORM MADLIB_SCHEMA.__assert
            (
                verbosity IS NOT NULL,                
                'verbosity must be non-null'
            );              
    
    IF (verbosity > 0 ) THEN     
        exec_begin = clock_timestamp();
        exec_leaves_rule = exec_begin - exec_begin;
        exec_union = exec_leaves_rule;
        exec_internode_rule = exec_leaves_rule;
    END IF;
    
    -- get metatable and classtable name given the tree table name
    metatable_name = MADLIB_SCHEMA.__get_metatable_name(tree_table_name);
    classtable_name = MADLIB_SCHEMA.__get_classtable_name(metatable_name);
    class_column_name = MADLIB_SCHEMA.__get_class_column_name(metatable_name);
    
    EXECUTE 'SELECT id, maxclass, probability, cat_size, jump FROM ' || tree_table_name || ' WHERE id = 1' 
    INTO rec;
    
    -- in case the root node is leaf
    IF (rec.jump IS NULL) THEN
        RETURN NEXT 'All instances will be classified to class ' || 
                     MADLIB_SCHEMA.__get_class_value(rec.maxclass, metatable_name) ||
                     ' [' || (rec.probability * rec.cat_size)::BIGINT || '/' || rec.cat_size || ']'; 
        RETURN;       
    END IF;
    
    -- get the meta info for features in the tree table (as best split)
    SELECT MADLIB_SCHEMA.__format
        (
        'SELECT id, column_name, table_name, is_cont
         FROM 
            MADLIB_SCHEMA.% n1
         WHERE id IN
            (SELECT DISTINCT feature
             FROM %
             WHERE jump IS NOT NULL
            )',
        metatable_name,
        tree_table_name
        )
    INTO curstmt;
    
    -- put all the features' value together using 'union all' 
    FOR rec IN EXECUTE curstmt LOOP
        -- continuous feature will produce two rows
        IF (rec.is_cont) THEN
            SELECT MADLIB_SCHEMA.__format
                (
                'SELECT % as fid, 1 as key, ''% <= ''::TEXT as fname, null::text as fval
                 UNION ALL 
                 SELECT % as fid, 2 as key, ''% > ''::TEXT as fname, null::text as fval',
                ARRAY[
                    MADLIB_SCHEMA.__to_char(rec.id),
                    rec.column_name,
                    MADLIB_SCHEMA.__to_char(rec.id),
                    rec.column_name
                    ]
                )
            INTO fvalue_stmt;
            
        -- discrete feature will produce the number of rows which is the same with distinct values
        ELSE
            SELECT MADLIB_SCHEMA.__format
                (
                'SELECT % as fid, key, ''% = ''::TEXT as fname, MADLIB_SCHEMA.__to_char(%) as fval
                 FROM MADLIB_SCHEMA.%
                 WHERE key IS NOT NULL',
                ARRAY[
                    MADLIB_SCHEMA.__to_char(rec.id),
                    rec.column_name,
                    rec.column_name,
                    rec.table_name
                    ]
                )
            INTO fvalue_stmt;
        END IF;
        
        IF (union_stmt IS NULL) THEN
            union_stmt = fvalue_stmt;
        ELSE
            union_stmt = union_stmt || ' UNION ALL ' || fvalue_stmt;
        END IF;
    END LOOP; 
    
    IF (verbosity > 0 ) THEN     
        exec_union = clock_timestamp() - exec_begin;
        RAISE INFO 'compose feature values statement time:%', exec_union;
        RAISE INFO 'feature info stmt: %', curstmt;
        RAISE INFO 'feature value stmt: %', union_stmt;
    END IF;
    
    -- put the rules for leaves into a temp table
    DROP TABLE IF EXISTS c45_gen_rules_leaves;
    SELECT MADLIB_SCHEMA.__format
        (
        'CREATE TEMP TABLE c45_gen_rules_leaves as
         SELECT 
            id, 
                ''  then class ''                   || 
                MADLIB_SCHEMA.__to_char(class)      || 
                '' [''                              || 
                (probability * cat_size)::BIGINT    || 
                ''/''                               || 
                cat_size                            || 
                '']'' 
            as str,
            array_to_string(tree_location, '''') as location,
            1 as rlid
         FROM 
            (SELECT id, maxclass, tree_location, probability, cat_size
             FROM %
             WHERE jump IS NULL
            ) n1
            LEFT JOIN
            (SELECT % as class, key
             FROM MADLIB_SCHEMA.% 
             WHERE key IS NOT NULL
            ) n2
            ON n1.maxclass = n2.key 
         m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (location)')',
        ARRAY[
            tree_table_name,
            class_column_name,
            classtable_name
            ]
        )
    INTO curstmt;
    
    EXECUTE curstmt;
    
    IF (verbosity > 0 ) THEN     
        exec_leaves_rule = clock_timestamp() - exec_begin;
        RAISE INFO 'create table for leaves'' rules time:%', exec_leaves_rule - exec_union;
        RAISE INFO 'create tablefor leaves stmt: %', curstmt;
    END IF;
    
    DROP TABLE IF EXISTS c45_gen_rules_internode;
    -- put rules of the internal nodes into a table
    SELECT MADLIB_SCHEMA.__format
        (
        'CREATE TEMP TABLE c45_gen_rules_internode AS
        SELECT 
            jump[key + 1] AS id,
            CASE WHEN (id = 1) THEN
                    ''  if '' || fname || COALESCE(MADLIB_SCHEMA.__to_char(split_value), MADLIB_SCHEMA.__to_char(fval), ''NULL'') 
                 ELSE
                    ''     '' || fname || COALESCE(MADLIB_SCHEMA.__to_char(split_value), MADLIB_SCHEMA.__to_char(fval), ''NULL'') 
            END AS str,
            array_to_string(tree_location, '''') || key AS location,
            0  AS rlid                                  
        FROM
            (SELECT id, feature, tree_location, jump, split_value 
             FROM %
             WHERE jump IS NOT NULL
            ) n1
            LEFT JOIN
            (%) n2
            ON n1.feature = n2.fid
        WHERE n1.jump[n2.key + 1] IS NOT NULL 
        m4_ifdef(`GREENPLUM', `DISTRIBUTED BY (location)')',
        tree_table_name,
        union_stmt
        )
    INTO curstmt;    
    EXECUTE curstmt;
   
    IF (verbosity > 0 ) THEN     
        exec_internode_rule = clock_timestamp() - exec_begin;
        RAISE INFO 'create table for internal nodes'' rules time:%', exec_internode_rule - exec_leaves_rule;
        RAISE INFO 'create tablefor internal nodes stmt: %', curstmt;
    END IF;
   
    FOR rec IN EXECUTE '  
                  SELECT t1.id, t1.rlid, t2.location, t1.str 
                  FROM
                    c45_gen_rules_internode t1
                    LEFT JOIN
                    c45_gen_rules_leaves t2 
                    ON position(t1.location in t2.location) = 1 
                  UNION ALL 
                  SELECT id, rlid, location, str 
                  FROM  c45_gen_rules_leaves n 
                  ORDER BY location, rlid, id'
                  LOOP
        RETURN NEXT rec.str;
    END LOOP;
   
    IF (verbosity > 0 ) THEN     
        RAISE INFO 'Total rules generation time:%', clock_timestamp() - exec_begin;
    END IF;
    
    RETURN;
END $$ LANGUAGE PLPGSQL;

/**
 * @brief   Display the trained decision tree model with rules
 *
 * @param   tree_table_name     Name of the table containing the tree's information
 *
 * @return  The rule representation text for a decision tree.
 *
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.c45_genrule
    (
    tree_table_name TEXT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_genrule
    (
    tree_table_name TEXT
    ) 
RETURNS SETOF TEXT AS $$
DECLARE 
    str             TEXT;
BEGIN
    -- run in non-verbose mode
    FOR str IN EXECUTE
                'SELECT *
                 FROM MADLIB_SCHEMA.c45_genrule
                 (' || coalesce('''' || tree_table_name || '''', 'NULL') || ', 0)'
    LOOP
        RETURN NEXT str;
    END LOOP;
   
    RETURN;
END
$$ LANGUAGE PLPGSQL;

/*
 * This is a internal function for displaying one tree node in human readable 
 * format. It is the step function of aggregation named __display_tree_aggr.
 * Parameters:
 *      state:              This variable is used to store the accumulated tree 
 *                          display information.
 *      depth:              The depth of this node.
 *      is_cont:            Whether the feature used to split is continuous
 *      depth:              The depth of current node.
 *      feat_name:          The name of the feature used to split
 *      curr_val:           The value of the splitting feature for this node.
 *      sp_val:             For continuous feature, it specifies the split value. 
 *                          Otherwise, it is of no meaning.
 *      max_prob:           For those elements in this node, the probability that
 *                          an element belongs to the max_class.
 *      max_class:          The class name with the largest number of elements 
 *                          for those elements in this node.
 *      cat_size:           Total count of elements in this node.
 * Return:
 *      It returns the text containing the information of human readable information for tree.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__display_node_sfunc
    (
    state       TEXT,
    depth       INT,
    is_cont     BOOLEAN,
    feat_name   TEXT,
    curr_val    TEXT,
    sp_val      FLOAT8,
    max_prob    FLOAT8,
    max_class   TEXT,
    cat_size    INT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__display_node_sfunc
    (
    state       TEXT,
    depth       INT,
    is_cont     BOOLEAN,
    feat_name   TEXT,
    curr_val    TEXT,
    sp_val      FLOAT8,
    max_prob    FLOAT8,
    max_class   TEXT,
    cat_size    INT
    ) 
RETURNS TEXT AS $$ 
DECLARE
    ret                     TEXT := '';
    index                   INT;
BEGIN
    -- We add indentation based on the depth.
    FOR index IN 0..depth LOOP
        ret = ret || '    ';
    END LOOP;
    
    IF (depth > 0) THEN
        ret = ret ||coalesce(feat_name,'null')||': ';
        -- For continuous features, there are two splits.
        -- We will mark curr_val to 1 for '<='. Otherwise, 
        -- we will mark curr_val to 2.
        IF (is_cont) THEN
            IF (curr_val::INT = 1) THEN
                ret = ret || ' <= ';
            ELSE
                ret = ret || ' > ';
            END IF;
            ret = ret||coalesce(sp_val,0)||' ';
        ELSE
            ret = ret||' = '||coalesce(curr_val,'null')||' ';
        END IF;
    ELSE
        ret = ret||'Root Node ';
    END IF;

    ret = ret                               || 
          ' : class('                       ||  
          coalesce(max_class,null)          || 
          ')   num_elements('               || 
          coalesce(cat_size,0)              || 
          ')  predict_prob('                ||
          coalesce(max_prob,0)              ||
          ')';

    ret = ret || E'\n';

    -- If there exists information, append the information
    -- for this node.
    IF (state IS NOT NULL) THEN
        ret = state || ret;
    END IF;
    
    RETURN ret;
END 
$$ LANGUAGE PLPGSQL;


DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.__display_tree_aggr
    (
    INT,        -- depth
    BOOLEAN,    -- is_cont
    TEXT,       -- feature name
    TEXT,       -- curr_val
    FLOAT8,     -- split value
    FLOAT8,     -- max_probability
    TEXT,       -- max_class
    INT         -- cat_size
    ) CASCADE;
CREATE 
m4_ifdef(`__GREENPLUM__', m4_ifdef(`__HAS_ORDERED_AGGREGATES__', `ORDERED'))
AGGREGATE MADLIB_SCHEMA.__display_tree_aggr
    (
    INT,        -- depth
    BOOLEAN,    -- is_cont
    TEXT,       -- feature name
    TEXT,       -- curr_val
    FLOAT8,     -- split value
    FLOAT8,     -- max_probability
    TEXT,       -- max_class
    INT         -- cat_size
    ) 
(
  SFUNC=MADLIB_SCHEMA.__display_node_sfunc,
  STYPE=TEXT
);

/*
 * Display the trained decision tree model with human readable format.
 * It use the recursive algorithm, which is slower than the version with 
 * ordered aggregate.
 * Parameters:
 *      tree_table  Name of the table containing the tree's information
 *      max_depth   The max depth to be displayed. If it is set to null,
 *                  this function will show all levels.
 * Return:
 *      the text representing the tree with human readable format.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__c45_display_with_ordered_aggr
    (
    tree_table  TEXT,
    max_depth   INT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__c45_display_with_ordered_aggr
    (
    tree_table  TEXT,
    max_depth   INT
    ) 
RETURNS TEXT AS $$
DECLARE
    metatable_name  TEXT := null;
    curr_stmt       TEXT := null;
    feature_name    TEXT := null;
    table_name      TEXT := null;
    result          TEXT := null;
    result_rec      RECORD;
BEGIN
    PERFORM MADLIB_SCHEMA.__assert
            (
                MADLIB_SCHEMA.__get_schema_name(tree_table),
                MADLIB_SCHEMA.__strip_schema_name(tree_table),
                't'
            );   
            
    metatable_name = MADLIB_SCHEMA.__get_metatable_name( tree_table );
    -- We removed the schema when storing the metatable's name. 
    metatable_name = 'MADLIB_SCHEMA.'||metatable_name;
 
    -- This table is used for tree display.
    -- It is filled with the original information before
    -- encoding to facilitate the display procedure.
    DROP TABLE IF EXISTS auxiliary_tree_display;
    CREATE TEMP TABLE auxiliary_tree_display
    (
        id                      INT,
        tree_location           INT[],
        probability             FLOAT8, 
        maxclass                TEXT,
        cat_size                INT,
        parent_id               INT,
        curr_value              TEXT,
        parent_feature_id       INT,
        is_parent_feature_cont  BOOLEAN,
        parent_split_value      FLOAT8,
        parent_feature_name     TEXT
    ) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (id)');
    
    -- We made a self join for the tree table. For each node, we get the 
    -- feature information at its parent node so as to display this node. 
    SELECT MADLIB_SCHEMA.__format(
        'INSERT INTO auxiliary_tree_display SELECT m.*, 
        n.column_name as parent_feature_name
        FROM 
        (SELECT t1.id, t1.tree_location,t1.probability,t1.maxclass::TEXT,
            t1.cat_size,t1.parent_id, 
            t1.tree_location[array_upper(t1.tree_location,1)]::TEXT as curr_value, 
            t2.feature as parent_feature_id, t2.is_feature_cont as is_parent_feature_cont, 
            t2.split_value as parent_split_value 
            FROM % t1 LEFT JOIN % t2 on t1.parent_id = t2.id) m 
         LEFT JOIN % n 
            on m.parent_feature_id = n.id;',
        ARRAY[
            tree_table,
            tree_table,
            metatable_name
        ]
        )
    INTO curr_stmt;     
    EXECUTE curr_stmt;

    -- Get the metatable storing the encoding information of class.
    EXECUTE 'SELECT column_name,table_name FROM '||metatable_name||
            ' WHERE column_type=''c'' LIMIT 1;'
            INTO result_rec;
    table_name = 'MADLIB_SCHEMA.'||result_rec.table_name;
    
    IF (table_name IS NOT NULL) THEN
        -- Convert back for the class column.
        SELECT MADLIB_SCHEMA.__format(
            'UPDATE auxiliary_tree_display n 
             SET maxclass = MADLIB_SCHEMA.__to_char(m.%) 
             FROM % m 
             WHERE m.key = n.maxclass::INT
            ',
            ARRAY[
                result_rec.column_name,
                table_name
            ]
            )
        INTO curr_stmt;  
        EXECUTE curr_stmt;
    END IF;

    -- Get the metatables storing the encoding information for discrete features.
    SELECT MADLIB_SCHEMA.__format(
        'SELECT column_name,table_name FROM % 
         WHERE NOT is_cont AND column_type=''f'';',
        ARRAY[
            metatable_name
        ]
        )
    INTO curr_stmt;  
    
    -- Convert back for discrete features.
    FOR result_rec IN EXECUTE (curr_stmt) LOOP
        feature_name = result_rec.column_name;
        table_name = result_rec.table_name;
        table_name = 'MADLIB_SCHEMA.'||table_name;
        SELECT MADLIB_SCHEMA.__format(
            'UPDATE auxiliary_tree_display n 
             SET curr_value = MADLIB_SCHEMA.__to_char(m.%) 
             FROM % m 
             WHERE m.key = n.curr_value::INT 
             AND n.parent_feature_name = %
            ',
            ARRAY[
                feature_name,
                table_name,
                quote_literal(feature_name)
            ]
            )
        INTO curr_stmt;  
        EXECUTE curr_stmt;   
    END LOOP;

    -- Now we already get all the information. Invoke the
    -- aggregation to show the tree.
    -- If we order by tree_location, we can get the sequence 
    -- of depth first traversal.
    curr_stmt = 'SELECT MADLIB_SCHEMA.__display_tree_aggr(
                array_upper(tree_location,1)-1,
                is_parent_feature_cont,
                parent_feature_name,
                curr_value,
                parent_split_value,
                probability,
                maxclass,
                cat_size 
                order by tree_location)
         FROM auxiliary_tree_display';

    IF (max_depth IS NOT NULL) THEN
        curr_stmt = curr_stmt                                   ||
                    ' WHERE array_upper(tree_location,1) - 1 <='  ||
                    max_depth;
    END IF;

    EXECUTE curr_stmt INTO result; 
    RETURN result;
END $$ LANGUAGE PLPGSQL;


/*
 * This is a internal function for displaying the tree in human readable format.
 * It use the depth-first strategy to traverse a tree and print values.
 * Parameters:
 *      tree_table:         The name of the table with information for the 
 *                          trained tree.
 *      id:                 The id of current node. This node and all of its  
 *                          children are displayed.
 *      feature_id:         The id of a feature, which was used to split in the 
 *                          parent of current node.
 *      depth:              The depth of current node.
 *      is_cont:            It specifies whether the feature denoted by 'feature_id'
 *                          is continuous or not.
 *      split_value:        For continuous feature, it specifies the split value. 
 *                          Otherwise, it is of no meaning.
 *      metatable_name:     For tabular format, this table contains the meta data
 *                          to encode the input table.
 *      max_depth:          The max depth to be displayed. If it is set to null,
 *                          this function will show all levels. 
 * Return:
 *      It returns the text containing the information of human readable tree.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__display_tree_no_ordered_aggr
    (
    tree_table      TEXT, 
    id              INT, 
    feature_id      INT, 
    depth           INT, 
    is_cont         BOOLEAN, 
    split_value     FLOAT,
    metatable_name  TEXT,
    max_depth       INT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__display_tree_no_ordered_aggr
    (
    tree_table      TEXT, 
    id              INT, 
    feature_id      INT, 
    depth           INT, 
    is_cont         BOOLEAN, 
    split_value     FLOAT,
    metatable_name  TEXT,
    max_depth       INT
    ) 
RETURNS TEXT AS $$ 
DECLARE
    ret                     TEXT := '';
    tree_location           INT[];
    feature                 INT;
    jump                    INT[];
    maxclass                INT;
    cat_size                INT;
    is_feature_cont         BOOLEAN;
    temp_split_value        FLOAT;
    index                   INT;
    curr_value              INT;
    probability             FLOAT;
BEGIN
    IF (id IS NULL OR id <= 0) THEN
        RETURN ret;
    END IF;
    
    EXECUTE 'select tree_location, feature, jump,is_feature_cont, split_value,
        maxclass,cat_size,probability from '
        || tree_table || ' where id =' || id ||';' INTO tree_location, feature,jump, 
        is_feature_cont,temp_split_value,maxclass, cat_size, probability; 

    curr_value = tree_location[array_upper(tree_location,1)];

    FOR index IN 0..depth LOOP
        ret = ret || '    ';
    END LOOP;
    
    IF (id > 1) THEN
        ret = ret ||MADLIB_SCHEMA.__get_feature_name(feature_id,metatable_name)||': ';

        IF (is_cont) THEN
            IF (curr_value = 1) THEN
                ret = ret || ' <= ';
            ELSE
                ret = ret || ' > ';
            END IF;
            ret = ret || split_value;
        ELSE
            ret = ret   || 
                  ' = ' || 
                  MADLIB_SCHEMA.__get_feature_value
                    (
                    feature_id, 
                    curr_value, 
                    metatable_name
                    );
        END IF;
    ELSE
        ret = ret||'Root Node ';
    END IF;

    ret = ret                                                       || 
          ' : class('                                               ||  
          MADLIB_SCHEMA.__get_class_value(maxclass,metatable_name)  || 
          ')   num_elements('                                       || 
          cat_size                                                  || 
          ')  predict_prob('                                        ||
          probability                                               ||
          ')';

    ret = ret || E'\n';
    
    IF (max_depth IS NOT NULL AND 
        depth >= max_depth) THEN
        RETURN ret;
    END IF;

    index = array_lower(jump,1);
    WHILE index <= array_upper(jump,1) LOOP
        ret = ret || MADLIB_SCHEMA.__display_tree_no_ordered_aggr(
                            tree_table, 
                            jump[index], 
                            feature, 
                            depth+1, 
                            is_feature_cont, 
                            temp_split_value, 
                            metatable_name,
                            max_depth);
        index = index +1;
    END LOOP; 

    RETURN ret;
END $$ LANGUAGE PLPGSQL;


/*
 * Display the trained decision tree model with human readable format.
 * It use the recursive algorithm, which is slower than the version with 
 * ordered aggregate.
 * Parameters:
 *      tree_table  Name of the table containing the tree's information
 *      max_depth   The max depth to be displayed. If it is set to null,
 *                  this function will show all levels.
 * Return:
 *      the text representing the tree with human readable format.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__c45_display_no_ordered_aggr    
    (
    tree_table  TEXT,
    max_depth   INT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__c45_display_no_ordered_aggr
    (
    tree_table  TEXT,
    max_depth   INT
    ) 
RETURNS TEXT AS $$
DECLARE
    metatable_name  TEXT := null;
    curstmt         TEXT := '';
BEGIN
    PERFORM MADLIB_SCHEMA.__assert
            (
                MADLIB_SCHEMA.__get_schema_name(tree_table),
                MADLIB_SCHEMA.__strip_schema_name(tree_table),
                't'
            );   
            
    metatable_name = MADLIB_SCHEMA.__get_metatable_name( tree_table );
    
    RETURN MADLIB_SCHEMA.__display_tree_no_ordered_aggr(tree_table, 1, 0, 0, 'f', 
        0, metatable_name,max_depth);
END $$ LANGUAGE PLPGSQL;

/**
 * @brief Display the trained decision tree model with human readable format.
 *
 * @param tree_table Name of the table containing the tree's information
 * @param max_depth  The max depth to be displayed. If null, this function will show all levels.
 *                    
 * @return the text representing the tree with human readable format.
 *
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.c45_display
    (
    tree_table  TEXT,
    max_depth   INT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_display
    (
    tree_table  TEXT,
    max_depth   INT
    ) 
RETURNS TEXT AS $$
DECLARE
BEGIN
    -- get rid of the messages whose severity level is lower than 'WARNING'
    SET client_min_messages = WARNING;
    
    PERFORM MADLIB_SCHEMA.__assert
            (
                (tree_table IS NOT NULL) AND
                (
                 MADLIB_SCHEMA.__table_exists
                    (
                        MADLIB_SCHEMA.__get_schema_name(tree_table),
                        MADLIB_SCHEMA.__strip_schema_name(tree_table)
                    )
                ),
                'the specified tree table' || coalesce('<' || tree_table || '> does not exists', ' is NULL')
            );   

m4_changequote(`>>>', `<<<')
m4_ifdef(>>>__HAS_ORDERED_AGGREGATES__<<<, >>>
    RETURN MADLIB_SCHEMA.__c45_display_with_ordered_aggr(tree_table,max_depth);
<<<, >>>
    RETURN MADLIB_SCHEMA.__c45_display_no_ordered_aggr(tree_table,max_depth);
<<<)
m4_changequote(>>>`<<<, >>>'<<<)
END $$ LANGUAGE PLPGSQL;

/**
 * @brief Display the whole trained decision tree model with human readable format.
 *
 * @param tree_table Name of the table containing the tree's information
 *                    
 * @return the text representing the tree with human readable format.
 *
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.c45_display
    (
    tree_table  TEXT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_display
    (
    tree_table  TEXT
    ) 
RETURNS TEXT AS $$
DECLARE
BEGIN
    RETURN MADLIB_SCHEMA.c45_display(tree_table,NULL);
END $$ LANGUAGE PLPGSQL;

/*
 *  An internal c45 classification function. It is used to perform
 *  the real classification process.
 *
 *  Parameters:
 *      classification_table_name:  The table containing the classification set.
 *      tree_table_name:            The table containing the final tree.
 *      result_table_name:          The table containing the classification
 *                                  result.
 *      is_result_temp              It specifies whether the result_table should
 *                                  be temporary.
 *      verbosity:                  Whether printing those debug information.
 *  Return:
 *      The caller may need to clean that internal table.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__c45_classify_internal
    (
    classification_table_name   TEXT, 
    tree_table_name             TEXT, 
    result_table_name           TEXT, 
    is_result_temp              BOOLEAN,
    verbosity                   BOOLEAN
    );
    
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__c45_classify_internal
    (
    classification_table_name   TEXT, 
    tree_table_name             TEXT, 
    result_table_name           TEXT, 
    is_result_temp              BOOLEAN,
    verbosity                   BOOLEAN
    ) 
RETURNS TEXT AS $$
DECLARE
    table_pick              INT := 1;
    remains_to_classify     INT;
    size_finished           INT;
    time_stamp              TIMESTAMP;
    metatable_name          TEXT := '';
    id_col_name             TEXT := 'id';
    curr_level              INT := 1;
    max_level               INT := 0;
    create_text             TEXT := '';
    h2hmv_routine_id        INT := 0;
    curstmt                 TEXT := '';
    encoded_table_name      TEXT := 'c45_classify_internal_edt';
    table_names             TEXT[] = '{classified_instance_ping,classified_instance_pong}';
BEGIN
    time_stamp = clock_timestamp();

    PERFORM MADLIB_SCHEMA.__assert
            (
                (classification_table_name IS NOT NULL) AND
                (
                 MADLIB_SCHEMA.__table_exists
                    (
                        MADLIB_SCHEMA.__get_schema_name(classification_table_name),
                        MADLIB_SCHEMA.__strip_schema_name(classification_table_name)
                    )
                ),
                'the specified classification table' || coalesce('<' || classification_table_name || '> does not exists', ' is NULL')
            );   

    PERFORM MADLIB_SCHEMA.__assert
            (
                (tree_table_name IS NOT NULL) AND
                (
                 MADLIB_SCHEMA.__table_exists
                    (
                        MADLIB_SCHEMA.__get_schema_name(tree_table_name),
                        MADLIB_SCHEMA.__strip_schema_name(tree_table_name)
                    )
                ),
                'the specified tree table' || coalesce('<' || tree_table_name || '> does not exists', ' is NULL')
            ); 

    PERFORM MADLIB_SCHEMA.__assert
            (
                (result_table_name IS NOT NULL) AND
                (
                 NOT MADLIB_SCHEMA.__table_exists
                    (
                        MADLIB_SCHEMA.__get_schema_name(result_table_name),
                        MADLIB_SCHEMA.__strip_schema_name(result_table_name)
                    )
                ),
                'the specified result table' || coalesce('<' || result_table_name || '> exists', ' is NULL')
            ); 
                                    
    PERFORM MADLIB_SCHEMA.__assert
            (
                verbosity IS NOT NULL,                
                'verbosity must be non-null'
            );    
            
    EXECUTE 'DROP TABLE IF EXISTS ' || encoded_table_name || ' CASCADE';
                                 
    SELECT MADLIB_SCHEMA.__get_metatable_name(tree_table_name) INTO metatable_name;

    SELECT MADLIB_SCHEMA.__get_routine_id(tree_table_name) INTO h2hmv_routine_id;
    
    PERFORM MADLIB_SCHEMA.__encode_tabular_table
        (
            classification_table_name, 
            encoded_table_name, 
            metatable_name, 
            h2hmv_routine_id,
            verbosity
        );
        
    IF ( verbosity ) THEN
        RAISE INFO 'tabular format. id_col_name: %', id_col_name;
    END IF;        
    
    DROP TABLE IF EXISTS classified_instance_ping;
    CREATE TEMP TABLE classified_instance_ping
    (
        id          INT,
        jump        INT,
        class       INT,
        prob        FLOAT,
        parent_id   INT,
        leaf_id     INT
    ) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (jump)');
    
    DROP TABLE IF EXISTS classified_instance_pong;
    CREATE TEMP TABLE classified_instance_pong
    (
        id          INT,
        jump        INT,
        class       INT,
        prob        FLOAT,
        parent_id   INT,
        leaf_id     INT
    ) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (jump)');
    
    IF (is_result_temp) THEN
        create_text = 'CREATE TEMP TABLE ';
        EXECUTE 'DROP TABLE IF EXISTS '||result_table_name;
    ELSE
        create_text = 'CREATE TABLE ';
    END IF;
    
    EXECUTE create_text || result_table_name || E'
    (
        id          INT,
        jump        INT,
        class       INT,
        prob        FLOAT,
        parent_id   INT,
        leaf_id     INT
    )m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (id)');';


    EXECUTE 'INSERT INTO classified_instance_ping (id, jump, class, prob) SELECT '
        ||id_col_name||', 1, 0, 0 FROM ' || encoded_table_name || ';';  

    
    EXECUTE 'SELECT max(array_upper(tree_location,1)) FROM '||tree_table_name||';'  INTO max_level;

    IF( max_level is NULL ) THEN
        RAISE EXCEPTION 'tree should not be empty';
    END IF;

    FOR curr_level IN 1..max_level LOOP
        IF(verbosity) THEN  
            RAISE INFO 'new_depth: %', curr_level;
        END IF;

        EXECUTE 'INSERT INTO ' || result_table_name ||' SELECT * FROM '|| 
            table_names[(table_pick) % 2 + 1] ||' WHERE jump = 0;';
        EXECUTE 'TRUNCATE '|| table_names[(table_pick) % 2 + 1] ||';';
        EXECUTE 'SELECT count(id) FROM '||result_table_name||';' INTO size_finished;
        IF(verbosity) THEN  
            RAISE INFO 'size_finished %', size_finished;
        END IF;            
        table_pick = table_pick % 2 + 1; 
        
        EXECUTE 'SELECT count(*) FROM '|| table_names[(table_pick) % 2 + 1] ||';' 
            INTO remains_to_classify;
            
        IF (remains_to_classify = 0) THEN
            IF(verbosity) THEN  
                RAISE INFO 'size_finished: % remains_to_classify: %', 
                    size_finished, remains_to_classify;
            END IF;  
                  
            EXIT;
        END IF;

        SELECT MADLIB_SCHEMA.__format(
            'INSERT INTO %
            SELECT pt.id, 
            CASE WHEN (is_feature_cont) THEN 
                    COALESCE(gt.jump[
                                     CASE WHEN (gt.split_value < farray[gt.feature]) THEN
                                        3
                                     ELSE
                                        2
                                     END
                                    ], 0)
                ELSE 
                    COALESCE(gt.jump[farray[gt.feature] + 1],0) 
                END as newjump,
            gt.maxclass, gt.probability, gt.parent_id, gt.id 
            FROM (
                SELECT t1.id, t1.jump, % as farray  
                FROM % t1 
                LEFT JOIN % t2 
                ON t1.id = t2.id
            ) AS pt,
            (
                SELECT jump, maxclass,feature, probability, parent_id, id, is_feature_cont, split_value
                FROM % 
                WHERE array_upper(tree_location,1) = %
            ) AS gt
            WHERE pt.jump = gt.id;',
            ARRAY[
                table_names[table_pick],
                MADLIB_SCHEMA.__get_feature_name_list(metatable_name),
                table_names[(table_pick) % 2 + 1],
                encoded_table_name,
                tree_table_name,
                MADLIB_SCHEMA.__to_char(curr_level)
            ]
            )
        INTO curstmt;     
        EXECUTE curstmt;
         
    END LOOP;

    EXECUTE 'INSERT INTO '||result_table_name||' SELECT * FROM '|| 
        table_names[table_pick] ||' WHERE jump = 0;';
    EXECUTE 'INSERT INTO '||result_table_name||' SELECT * FROM '|| 
        table_names[table_pick % 2 + 1] ||' WHERE jump = 0;';
    
    IF(verbosity) THEN  
        RAISE INFO 'final classification time:%', clock_timestamp() - time_stamp;
    END IF;
    
    RETURN encoded_table_name;
END
$$ LANGUAGE PLPGSQL;
   
    
/**
 * @brief Classify dataset using trained decision tree model.
 *
 * @param tree_table_name               Name of trained tree
 * @param classification_table_name Name of the table/view with the source data
 * @param result_table_name             Name of result table
 * @param verbosity                     If set to 't' will use verbose mode
 *
 * @return The columns of classified table:
 * - <tt>id INT</tt> - Record id.
 * - <tt>class INT</tt> - Predicted class.
 * - <tt>prob FLOAT</tt> - Probability of the predicted class.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.c45_classify
    (
    tree_table_name             TEXT, 
    classification_table_name   TEXT, 
    result_table_name           TEXT, 
    verbosity                   BOOLEAN
    );
    
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_classify
    (
    tree_table_name             TEXT, 
    classification_table_name   TEXT, 
    result_table_name           TEXT, 
    verbosity                   BOOLEAN
    ) 
RETURNS MADLIB_SCHEMA.c45_classify_result AS $$
DECLARE
    encoded_table_name  TEXT := '';
    begin_time          TIMESTAMP;
    ret                 MADLIB_SCHEMA.c45_classify_result;
BEGIN
    IF (NOT verbosity) THEN
        -- get rid of the messages whose severity level is lower than 'WARNING'
        SET client_min_messages = WARNING;
    END IF;
    
    begin_time = clock_timestamp();
    
    SELECT MADLIB_SCHEMA.__c45_classify_internal
        (
        classification_table_name, 
        tree_table_name, 
        result_table_name, 
        'f',
        verbosity
        )  INTO encoded_table_name;
    
    EXECUTE 'ALTER TABLE '||result_table_name||' DROP COLUMN jump;';
    EXECUTE 'ALTER TABLE '||result_table_name||' DROP COLUMN parent_id;';
    EXECUTE 'ALTER TABLE '||result_table_name||' DROP COLUMN leaf_id;';
    EXECUTE 'DROP TABLE IF EXISTS ' || encoded_table_name || ';';
    EXECUTE 'SELECT COUNT(*) FROM ' ||classification_table_name||';' INTO ret.input_set_size;
    
    ret.cost_time = clock_timestamp() - begin_time;
    
    RETURN ret;
END
$$ LANGUAGE PLPGSQL;

/**
 * @brief Classify dataset using trained decision tree model.
 *  
 * @param classification_table_name     Name of the table/view with the source data
 * @param tree_table_name               Name of trained tree
 * @param result_table_name             Name of result table
 *
 * @return The columns of classified table:
 * - <tt>id INT</tt> - Record id.
 * - <tt>class INT</tt> - Predicted class.
 * - <tt>prob FLOAT</tt> - Probability of the predicted class.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.c45_classify
    (
    tree_table_name             TEXT, 
    classification_table_name   TEXT, 
    result_table_name           TEXT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_classify
    (
    tree_table_name             TEXT, 
    classification_table_name   TEXT, 
    result_table_name           TEXT
    ) 
RETURNS MADLIB_SCHEMA.c45_classify_result AS $$
DECLARE
    ret MADLIB_SCHEMA.c45_classify_result;
BEGIN
    -- get rid of the messages whose severity level is lower than 'WARNING'
    SET client_min_messages = WARNING;
    
	ret = MADLIB_SCHEMA.c45_classify
	       (
	       tree_table_name,
	       classification_table_name, 
	       result_table_name,
	       'f'
	       );
	       
    RETURN ret;
END $$ LANGUAGE PLPGSQL;

/**
 * @brief Check the accuracy of the decision tree model.
 * 
 * @param tree_table_name           Name of trained tree
 * @param scoring_table_name        Name of the table/view with the source data
 * @param verbosity                 If set to 't' will use verbose mode 
 *
 * @return The estimated accuracy information.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.c45_score
    (
    tree_table_name             TEXT, 
    scoring_table_name          TEXT, 
    verbosity                   BOOLEAN
    );
    
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_score
    (
    tree_table_name             TEXT, 
    scoring_table_name          TEXT, 
    verbosity                   BOOLEAN
    ) 
RETURNS FLOAT AS $$
DECLARE
    result_table_name   TEXT = 'c45_score_table_temp';
    id_col_name         TEXT := 'id';
    class_col_name      TEXT := 'class';
    curstmt             TEXT := '';
    num_of_row          FLOAT := 0.0;
    mis_of_row          FLOAT := 0.0;
    encoded_table_name  TEXT := '';
BEGIN

    IF (NOT verbosity) THEN
        -- get rid of the messages whose severity level is lower than 'WARNING'
        SET client_min_messages = WARNING;
    END IF;
    
    PERFORM MADLIB_SCHEMA.__assert
            (
                (tree_table_name IS NOT NULL) AND
                (
                 MADLIB_SCHEMA.__table_exists
                    (
                        MADLIB_SCHEMA.__get_schema_name(tree_table_name),
                        MADLIB_SCHEMA.__strip_schema_name(tree_table_name)
                    )
                ),
                'the specified tree table' || coalesce('<' || tree_table_name || '> does not exist', ' is NULL')
            ); 

    PERFORM MADLIB_SCHEMA.__assert
            (
                (scoring_table_name IS NOT NULL) AND
                (
                 MADLIB_SCHEMA.__table_exists
                    (
                        MADLIB_SCHEMA.__get_schema_name(scoring_table_name),
                        MADLIB_SCHEMA.__strip_schema_name(scoring_table_name)
                    )
                ),
                'the specified scoring table' || coalesce('<' || scoring_table_name || '> does not exist', ' is NULL')
            ); 

    PERFORM MADLIB_SCHEMA.__assert
        (
            MADLIB_SCHEMA.__column_exists
                (
                    MADLIB_SCHEMA.__get_schema_name(scoring_table_name),
                    MADLIB_SCHEMA.__strip_schema_name(scoring_table_name),
                    MADLIB_SCHEMA.__get_class_column_name(MADLIB_SCHEMA.__get_metatable_name(tree_table_name))
                ),
            'the specified scoring table<' || scoring_table_name || '> does not have class column'
        );
            
    result_table_name = MADLIB_SCHEMA.__get_schema_name(tree_table_name) || 
                        MADLIB_SCHEMA.__strip_schema_name(tree_table_name) || 
                        result_table_name;
    
    SELECT MADLIB_SCHEMA.__c45_classify_internal
        (
        scoring_table_name, 
        tree_table_name, 
        result_table_name, 
        't',
        verbosity
        ) 
    INTO encoded_table_name;
    

    SELECT MADLIB_SCHEMA.__format
        (
        'SELECT count(id) FROM %;',
        result_table_name
        ) 
    INTO curstmt;
    
    EXECUTE curstmt INTO num_of_row;
    
    SELECT MADLIB_SCHEMA.__format
        (
        'SELECT count(b.id) FROM % a, % b WHERE a.%=b.id and a.%<>b.class',
        encoded_table_name,
        result_table_name,
        id_col_name,
        class_col_name
        ) 
    INTO curstmt;
     
    EXECUTE curstmt INTO mis_of_row;
     
    EXECUTE 'DROP TABLE IF EXISTS ' || encoded_table_name || ';';
    
    RETURN (num_of_row - mis_of_row) / num_of_row;
    
END;
$$ LANGUAGE PLPGSQL;

/**
 * @brief Cleanup the trained tree table and any relevant tables.
 *
 * @param result_tree_table_name Name of the table containing the tree's information
 *
 * @return The status of that cleanup operation.
 *
 */
DROP FUNCTION IF EXISTS  MADLIB_SCHEMA.c45_clean
    (
    TEXT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_clean
    ( 
    result_tree_table_name TEXT
    ) 
RETURNS BOOLEAN AS $$
DECLARE
    metatable_name TEXT;
BEGIN
    -- get rid of the messages whose severity level is lower than 'WARNING'
    SET client_min_messages = WARNING;
        
    PERFORM MADLIB_SCHEMA.__assert
            (
                (result_tree_table_name IS NOT NULL) AND
                (
                 MADLIB_SCHEMA.__table_exists
                    (
                        MADLIB_SCHEMA.__get_schema_name(result_tree_table_name),
                        MADLIB_SCHEMA.__strip_schema_name(result_tree_table_name)
                    )
                ),
                'the specified tree table' || coalesce('<' || result_tree_table_name || '> does not exists', ' is NULL')
            ); 
                                    
                
    IF (MADLIB_SCHEMA.__table_exists('MADLIB_SCHEMA', 'training_info')) THEN
        metatable_name = MADLIB_SCHEMA.__get_metatable_name(result_tree_table_name);
        
        IF( metatable_name IS NOT NULL) THEN
            PERFORM MADLIB_SCHEMA.__drop_metatable(metatable_name);
            EXECUTE 'DROP TABLE IF EXISTS ' || 
                     MADLIB_SCHEMA.__get_encode_table_name(result_tree_table_name) || ';';
        END IF;
        
        EXECUTE 'DROP TABLE IF EXISTS ' || result_tree_table_name;
        PERFORM MADLIB_SCHEMA.__delete_traininginfo(result_tree_table_name);
    ELSE
        EXECUTE 'DROP TABLE IF EXISTS ' || result_tree_table_name;
    END IF;
    
    RETURN 't';    
END
$$ LANGUAGE PLPGSQL;
