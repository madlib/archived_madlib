/* ----------------------------------------------------------------------- *//**
 *
 * @file textfex.sql_in
 *
 * @brief SQL function for text feature extraction
 * @date February 2012
 *
 * @sa For an introduction to text feature extraction, see the module
 *     description \ref grp_textfex_viterbi
 *//* ----------------------------------------------------------------------- */

m4_include(`SQLCommon.m4')

/**
@addtogroup grp_textfex_viterbi

@about
The Feature Extraction module provides functionality for basic text-analysis
tasks such as part-of-speech (POS) tagging and named-entity resolution.
In addition to feature extraction, it also has a Viterbi implementation
to get the best label sequence and the conditional probability
\f$ \Pr( \text{best label sequence} \mid \text{Sentence}) \f$.

At present, six feature types are implemented.
    - Edge Feature: transition feature that encodes the transition feature
      weight from current label to next label.
    - Start Feature: fired when the current token is the first token in a sentence.
    - End Feature: fired when the current token is the last token in a sentence.
    - Word Feature: fired when the current token is observed in the trained
      dictionary.
    - Unknown Feature: fired when the current token is not observed in the trained
      dictionary for at least certain times.
    - Regex Feature: fired when the current token can be matched by the regular
      expression.

You can add your own feature type according to the training model.

Instead of scanning every token in a sentence and extracting features for
each token on the fly, we extract features for each distinct token and
materialize it in the table.  When we call the Viterbi function to get the best
label sequence, we only need a single lookup to get the feature weight.

@usage
  - Use CRF PACKAGE [1] to generate the trained models. Basically CRF PACKAGE
    generates two model files 'feature' and 'crf'.  Write some simple scripts
    to convert the data in the model files to the data format required by in
    this module.

  - Load model from local drive to database
    <pre>SELECT madlib.load_crf_model(
         '<em>/path/to/data</em>');</pre>

  - Run feature extraction in batch mode or interactive mode
    <pre>SELECT madlib.text_feature_extraction(
         '<em>segmenttbl</em>',
         '<em>dictionary</em>',
         '<em>labeltbl</em>',
         '<em>regextbl</em>',
         '<em>featuretbl</em>',
         '<em>viterbi_mtbl</em>',
         '<em>viterbi_rtbl</em>');</pre>

  - Run the Viterbi function to get the best label sequence and the conditional
    probability \f$ p(top1_label_sequence|sentence) \f$.
    <pre>SELECT madlib.vcrf_label(
         '<em>segmenttbl</em>',
         '<em>viterbi_mtbl</em>',
         '<em>viterbi_rtbl</em>',
         '<em>labeltbl</em>',
         '<em>resulttbl</em>');</pre>

@literature

[1] http://crf.sourceforge.net/

[2] http://www.cs.berkeley.edu/~daisyw/ViterbiCRF.html

[3] http://en.wikipedia.org/wiki/Viterbi_algorithm

@sa File model_loader.sql_in textfex.sql_in viterbi.sql_in documenting the SQL functions.
*/

/**
 * @brief This function extracts text features.
 *
 * This feature extraction function will produce two factor tables, "m table"
 * (\a viterbi_mtbl) and "r table" (\a viterbi_rtbl).  The \a viterbi_mtbl
 * table and \a viterbi_rtbl table are used to calculate the best label
 * sequence for each sentence.
 *
 * - <em>viterbi_mtbl</em> table
 * encodes the edge features which are solely dependent on upon current label and
 * previous y value. The m table has three columns which are prev_label, label,
 * and value respectively.
 * If the number of labels in \f$ n \f$, then the m factor table will \f$ n^2 \f$
 * rows.  Each row encodes the transition feature weight value from the previous label
 * to the current label.
 *
 * \a startFeature is considered as a special edge feature which is from the
 * beginning to the first token.  Likewise, \a endFeature can be considered
 * as a special edge feature which is from the last token to the very end.
 * So m table encodes the edgeFeature, startFeature, and endFeature.
 * If the total number of labels in the label space is 45 from 0 to 44,
 * then the m factor array is as follows:
 * <pre>
 *                  0  1  2  3  4  5...44
 * startFeature -1  a  a  a  a  a  a...a
 * edgeFeature   0  a  a  a  a  a  a...a
 * edgeFeature   1  a  a  a  a  a  a...a
 * ...
 * edgeFeature  44  a  a  a  a  a  a...a
 * endFeature   45  a  a  a  a  a  a...a</pre>
 *
 * - viterbi_r table
 * is related to specific tokens.  It encodes the single state features,
 * e.g., wordFeature, RegexFeature for all tokens.  The r table is represented
 * in the following way.
 * <pre>
 *        0  1  2  3  4...44
 * token1 a  a  a  a  a...a
 * token2 a  a  a  a  a...a</pre>
 *
 * @param segmenttbl Name of table containing all the testing sentences.
 * @param dictionary Name of table containing the dictionary.
 * @param labeltbl Name of table containing the the label space used in POS or other NLP tasks.
 * @param regextbl Name of table containing all the regular expressions to capture regex features.
 * @param viterbi_mtbl Name of table to store the m factors.
 * @param viterbi_rtbl Name of table to store the r factors.
 *
 */

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.text_feature_extraction(
        segmenttbl text,
        dictionary  text,
        labeltbl text,
        regextbl text,
        featuretbl text,
        viterbi_mtbl text,
        viterbi_rtbl text) RETURNS void AS
$$
        # Clear m&r tables
        plpy.execute("""
            DROP TABLE IF EXISTS {viterbi_mtbl}, {viterbi_rtbl};
            """.format(
                viterbi_mtbl = viterbi_mtbl,
                viterbi_rtbl = viterbi_rtbl
            ))
        # Create m&r factor table
        plpy.execute("""
            CREATE TABLE {viterbi_mtbl} (score integer[][]);
            """.format(viterbi_mtbl = viterbi_mtbl));
        plpy.execute("""
            CREATE TABLE {viterbi_rtbl}
            (seg_text text, label integer, score integer);
            """.format(viterbi_rtbl = viterbi_rtbl))
        # Create index for performance
        plpy.execute("""
            CREATE INDEX {viterbi_rtbl}_idx ON {viterbi_rtbl} (seg_text)
            """.format(viterbi_rtbl = viterbi_rtbl))

        plpy.execute("DROP TABLE IF EXISTS prev_labeltbl, segment_hashtbl, unknown_segment_hashtbl, rtbl, mtbl;")
        plpy.execute("DROP TABLE IF EXISTS prev_labeltbl, segment_hashtbl, unknown_segment_hashtbl, rtbl, mtbl CASCADE;")

        plpy.execute("CREATE TEMP TABLE prev_labeltbl(id int);")

        # Insert unique tokens into the segment_hashtbl
        plpy.execute("CREATE TEMP TABLE segment_hashtbl(seg_text text);")

        # create a temp partial dictionary table which stores the words whose occurance
        # is below certain threshold, refer to the CRF Package
        plpy.execute("CREATE TEMP TABLE unknown_segment_hashtbl(seg_text text);")

        # Generate a sparse matrix to store the r factors
        plpy.execute("CREATE TEMP TABLE rtbl (seg_text text NOT NULL, label integer, value double precision);")

        # Generate M factor table
        plpy.execute("CREATE TEMP TABLE mtbl(prev_label integer, label integer, value double precision);")

        # Calculate the number of labels in the label space
        rv = plpy.execute("SELECT COUNT(*) AS total_label FROM " + labeltbl + ";")
        nlabel = rv[0]['total_label']

        plpy.execute("""INSERT INTO segment_hashtbl(seg_text)
                        SELECT DISTINCT seg_text
                        FROM   """ + segmenttbl + """;""")

        plpy.execute("""INSERT INTO unknown_segment_hashtbl(seg_text)
                        ((SELECT DISTINCT seg_text
                          FROM   segment_hashtbl)
                         EXCEPT
                         (SELECT DISTINCT token
                          FROM   """ + dictionary + """
                          WHERE  total>1));""")

        plpy.execute("""INSERT INTO prev_labeltbl
                        SELECT id
                        FROM   """ + labeltbl + """;
                        INSERT INTO prev_labeltbl VALUES(-1);
                        INSERT INTO prev_labeltbl VALUES( """ + str(nlabel) + """);""")

        # Generate sparse M factor table
        plpy.execute("""INSERT INTO mtbl(prev_label, label, value)
                        SELECT prev_label.id, label.id, 0
                        FROM   """ + labeltbl + """ AS label,
                               prev_labeltbl as prev_label;""")

        # EdgeFeature and startFeature, startFeature can be considered as a special edgeFeature
        plpy.execute("""INSERT INTO mtbl(prev_label, label, value)
                        SELECT prev_label_id,label_id,weight
                        FROM   """ + featuretbl + """ AS features
                        WHERE  features.prev_label_id<>-1 OR features.name = 'S.';""")

        # EndFeature, endFeature can be considered as a special edgeFeature
        plpy.execute("""INSERT INTO mtbl(prev_label, label, value)
                        SELECT """ + str(nlabel) + """, label_id, weight
                        FROM   """ + featuretbl + """ AS features
                        WHERE  features.name = 'End.';""")

m4_ifdef(`__HAS_ORDERED_AGGREGATES__', `
        plpy.execute("""INSERT INTO {viterbi_mtbl}
                        SELECT array_agg(weight ORDER BY prev_label,label)
                        FROM   (SELECT prev_label, label, (SUM(value)*1000)::integer AS weight
                                FROM   mtbl
                                GROUP BY prev_label,label
                                ORDER BY prev_label,label) as TEMP_MTBL;""".format(
                                    viterbi_mtbl = viterbi_mtbl
                                ))
', `
        plpy.execute("""INSERT INTO {viterbi_mtbl}
                        SELECT ARRAY(
                            SELECT
                                (SUM(value) * 1000)::integer
                            FROM
                                mtbl
                            GROUP BY
                                prev_label, label
                            ORDER BY
                                prev_label, label
                        );""".format(
                            viterbi_mtbl = viterbi_mtbl
                        ))
')

        plpy.execute("""INSERT INTO rtbl(seg_text, label, value)
                        SELECT segment_hashtbl.seg_text, labels.id, 0
                        FROM   segment_hashtbl segment_hashtbl,
                         """ + labeltbl + """ AS labels;""")

        # RegExFeature
        plpy.execute("""INSERT INTO rtbl(seg_text, label, value)
                        SELECT segment_hashtbl.seg_text, features.label_id, features.weight
                        FROM   segment_hashtbl AS segment_hashtbl,
                         """ + featuretbl + """ AS features,
                         """ + regextbl + """ AS regex
                        WHERE  segment_hashtbl.seg_text ~ regex.pattern
                               AND features.name||'%' ='R_' || regex.name;""")

        # UnknownFeature
        plpy.execute("""INSERT INTO rtbl(seg_text, label, value)
                        SELECT segment_hashtbl.seg_text, features.label_id, features.weight
                        FROM   unknown_segment_hashtbl AS segment_hashtbl,
                         """ + featuretbl + """ AS features
                        WHERE  features.name = 'U';""")

        # Wordfeature
        plpy.execute("""INSERT INTO rtbl(seg_text, label, value)
                        SELECT seg_text, label_id, weight
                        FROM   segment_hashtbl,
                        """  + featuretbl + """
                        WHERE  name = 'W_' || seg_text;""")

        # Factor table
        plpy.execute("""INSERT INTO """ + viterbi_rtbl + """(seg_text, label, score)
                        SELECT seg_text,label,(SUM(value)*1000)::integer AS score
                        FROM   rtbl
                        GROUP BY seg_text,label;""")

$$ LANGUAGE plpythonu STRICT;
