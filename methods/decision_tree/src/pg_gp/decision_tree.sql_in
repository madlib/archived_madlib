/*
 *
 * @file decision_tree.sql_in
 *
 * @brief decision tree APIs and main controller written in PL/PGSQL
 *
 * @date Dec. 22 2011
 */

/*
@about

This module provides an implementation of the C4.5 decision tree algorithm. 
It assumes that:
- The input table format is tabular.
- Data features can be discrete or continuous. 
- A feature can have missing value

@input

The <b>training data</b> is expected to be of 
the following form:
<pre>{TABLE|VIEW} <em>trainingSource</em> (
    ...
    <em>id</em> INTEGER,
    <em>feature1</em> ANYTYPE,
    <em>feature2</em> ANYTYPE,
    <em>feature3</em> ANYTYPE,
    ....................
    <em>featureN</em> ANYTYPE,
    <em>class</em> INTEGER,
    ...
)</pre>

The <b>data to classify</b> is expected to be 
of the following form:
<pre>{TABLE|VIEW} <em>classifySource</em> (
    ...
    <em>id</em> INTEGER,
    <em>feature1</em> ANYTYPE,
    <em>feature2</em> ANYTYPE,
    <em>feature3</em> ANYTYPE,
    ....................
    <em>featureN</em> ANYTYPE,
    ...
)</pre>

@usage

- Run the training algorithm on the source data:
  <pre>SELECT * FROM \ref c45_train(
    '<em>split_criterion_name</em>',
    '<em>training_table_name</em>', 
    '<em>result_tree_table_name</em>', 
    '<em>validation_table_name</em>',
    '<em>continuous_feature_names</em>',
    '<em>feature_col_names</em>',
    '<em>id_col_name</em>', 
    '<em>class_col_name</em>',
    '<em>confidence_level</em>',
    '<em>how2handle_missing_value</em>'
    '<em>max_num_iter</em>',
    '<em>max_tree_depth</em>',
    '<em>min_percent_mode</em>',
    '<em>min_percent_split</em>');
  </pre>
  This will create the MADLIB_SCHEMA.tree table storing an abstract object 
  (representing the model) used for further classification. Column names:
  <pre>    
 id | tree_location | feature |    probability    |    ebp_coeff     | maxclass |    split_gain     | live | cat_size | parent_id |     jump      | is_feature_cont | split_value 
----+---------------+---------+-------------------+------------------+----------+-------------------+------+----------+-----------+---------------+-----------------+-------------
                                                     ...</pre>    
    
- Run the classification function using the learned model: 
  <pre>SELECT * FROM \ref c45_classify(
    '<em>tree_table_name</em>', 
    '<em>classification_table_name</em>', 
    '<em>result_table_name</em>');</pre>
  This will create the result_table with the 
  classification results. 
  <pre> </pre> 

- Run the display tree function using the learned model: 
  <pre>SELECT * FROM \ref c45_display(
    '<em>tree_table_name</em>');</pre>
  This will display the trained tree in human readable format. 
  <pre> </pre> 

- Run the clean tree function as below: 
  <pre>SELECT * FROM \ref c45_clean(
    '<em>tree_table_name</em>');</pre>
  This will clean up the learned model and all metadata.
  <pre> </pre> 

@examp

-# Prepare an input table/view, e.g.:
\verbatim
testdb=# select * from golf_data order by id;
 id | outlook  | temperature | humidity | windy  |    class     
----+----------+-------------+----------+--------+--------------
  1 | sunny    |          85 |       85 |  false |  Do not Play
  2 | sunny    |          80 |       90 |  true  |  Do not Play
  3 | overcast |          83 |       78 |  false |  Play
  4 | rain     |          70 |       96 |  false |  Play
  5 | rain     |          68 |       80 |  false |  Play
  6 | rain     |          65 |       70 |  true  |  Do not Play
  7 | overcast |          64 |       65 |  true  |  Play
  8 | sunny    |          72 |       95 |  false |  Do not Play
  9 | sunny    |          69 |       70 |  false |  Play
 10 | rain     |          75 |       80 |  false |  Play
 11 | sunny    |          75 |       70 |  true  |  Play
 12 | overcast |          72 |       90 |  true  |  Play
 13 | overcast |          81 |       75 |  false |  Play
 14 | rain     |          71 |       80 |  true  |  Do not Play
(14 rows)

\endverbatim
-# Train the decision tree model, e.g.:
\verbatim
sql> SELECT * FROM MADLIB.c45_train('infogain','golf_data','madlib.trained_tree_infogain',null,'f',
    'temperature,humidity',null,'id','class', 100,3000,10,0.001,0.001);   
CONTEXT:  PL/pgSQL function "c45_train" line 70 at assignment
 training_set_size | tree_nodes | tree_depth |    cost_time    | split_criterion 
-------------------+------------+------------+-----------------+-----------------
                14 |          8 |          3 | 00:00:00.889973 | infogain
(1 row)

 
(1 row)
\endverbatim
-# Check few rows from the tree model table:
\verbatim
testdb=# select * from madlib.trained_tree_infogain order by id;
 id | tree_location | feature |    probability    |    ebp_coeff     | maxclass |    split_gain     | live | cat_size | parent_id |     jump      | is_feature_cont | split_value 
----+---------------+---------+-------------------+------------------+----------+-------------------+------+----------+-----------+---------------+-----------------+-------------
  1 | {0}           |       3 | 0.642857142857143 | 7.87584209442139 |        2 | 0.171033941880327 |    0 |       14 |         0 | [2:4]={2,3,4} | f               |            
  2 | {0,1}         |       4 |                 1 | 1.75063467025757 |        2 |                 0 |    0 |        4 |         1 |               | f               |            
  3 | {0,2}         |       4 |               0.6 | 3.74199032783508 |        2 | 0.673011667009257 |    0 |        5 |         1 | [2:3]={5,6}   | f               |            
  4 | {0,3}         |       2 |               0.6 | 3.74199032783508 |        1 | 0.673011667009257 |    0 |        5 |         1 | [2:3]={7,8}   | t               |          70
  5 | {0,2,1}       |       4 |                 1 | 1.60752332210541 |        2 |                 0 |    0 |        3 |         3 |               | f               |            
  6 | {0,2,2}       |       4 |                 1 | 1.36754441261292 |        1 |                 0 |    0 |        2 |         3 |               | f               |            
  7 | {0,3,1}       |       4 |                 1 | 1.36754441261292 |        2 |                 0 |    0 |        2 |         4 |               | f               |            
  8 | {0,3,2}       |       4 |                 1 | 1.60752332210541 |        1 |                 0 |    0 |        3 |         4 |               | f               |            
(8 rows)

\endverbatim
-# To display the tree with human readable format:
\verbatim
testdb=# select madlib.c45_display('madlib.trained_tree_infogain');
                                      c45_display                                      
---------------------------------------------------------------------------------------
         outlook:  = overcast : class( Play)   num_elements(4)  predict_prob(1)        
         outlook:  = rain : class( Play)   num_elements(5)  predict_prob(0.6)          
             windy:  =  false : class( Play)   num_elements(3)  predict_prob(1)        
             windy:  =  true : class(do not play)   num_elements(2)  predict_prob(1)   
         outlook:  = sunny : class(do not play)   num_elements(5)  predict_prob(0.6)   
             humidity:  <= 77.5 : class( Play)   num_elements(2)  predict_prob(1)      
             humidity:  > 77.5 : class(do not play)   num_elements(3)  predict_prob(1) 
 
(1 row)

\endverbatim
-# To classify data with the learned model:
\verbatim
testdb=# select * from  madlib.c45_classify
testdb-#     (
testdb(#     'madlib.trained_tree_infogain', 
testdb(#     'golf_data', 
testdb(#     'madlib.classification_result'); 
PL/pgSQL function "c45_classify" line 2 at perform
 c45_classify 
--------------
 
(1 row)
\endverbatim
-# Check classification results: 
\verbatim
testdb=# select t.id,t.outlook,t.temperature,t.humidity,t.windy,c.class from 
    madlib.classification_result c,golf_data t where t.id=c.id order by id;
 id | outlook  | temperature | humidity | windy  | class 
----+----------+-------------+----------+--------+-------
  1 | sunny    |          85 |       85 |  false |     2
  2 | sunny    |          80 |       90 |  true  |     2
  3 | overcast |          83 |       78 |  false |     1
  4 | rain     |          70 |       96 |  false |     1
  5 | rain     |          68 |       80 |  false |     1
  6 | rain     |          65 |       70 |  true  |     2
  7 | overcast |          64 |       65 |  true  |     1
  8 | sunny    |          72 |       95 |  false |     2
  9 | sunny    |          69 |       70 |  false |     1
 10 | rain     |          75 |       80 |  false |     1
 11 | sunny    |          75 |       70 |  true  |     1
 12 | overcast |          72 |       90 |  true  |     1
 13 | overcast |          81 |       75 |  false |     1
 14 | rain     |          71 |       80 |  true  |     2
(14 rows)
(notes: The class value of 2 refers to 'do not play'. The class value of 1
refers to 'Play'. We plan to add a view to translate the numeric value to original
value soon.
\endverbatim

-# clean up the tree and metadata: 
\verbatim
testdb=# select madlib.c45_clean('madlib.trained_tree_infogain');
 c45_clean 
-----------
 
(1 row)
\endverbatim

@literature

[1] http://en.wikipedia.org/wiki/C4.5_algorithm

@sa File decision_tree.sql_in documenting the SQL functions.
*/

/*
 * This structure is used to store the result for the function of c45_train.
 *      training_set_size: It means how many records there exists in 
 *                         the training set.
 *      tree_nodes:        It is the number of total tree nodes.
 *      tree_depth:        It is the depth of the trained tree.
 *      cost_time:         It is the time consumed during training the tree.
 *      split_criterion:   It is the split criterion used to train the tree.
 */
DROP TYPE IF EXISTS MADLIB_SCHEMA.c45_train_result;
CREATE TYPE MADLIB_SCHEMA.c45_train_result AS 
    (   
    training_set_size        BIGINT,   
    tree_nodes               BIGINT,
    tree_depth               INT,
    cost_time                INTERVAL,
    split_criterion          TEXT
    );

/*
 * This structure is used to store the result for the function of c45_classify.
 *      input_set_size:    It means how many records there exists in 
 *                         the classification set.
 *      cost_time:         It is the time consumed during classifying the tree.
 */
DROP TYPE IF EXISTS MADLIB_SCHEMA.c45_classify_result;
CREATE TYPE MADLIB_SCHEMA.c45_classify_result AS 
    (   
    input_set_size        BIGINT,   
    cost_time             INTERVAL
    );

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__scv_aggr_sfunc
    (
    result              FLOAT8[],
    split_criterion     INT,
    feature_value       FLOAT8,
    class               FLOAT8,
    is_cont             boolean,
    less                FLOAT8,
    great               FLOAT8,
    init_impurity_val   FLOAT8,
    true_total_count    FLOAT8
    )CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__scv_aggr_sfunc
    (
    result              FLOAT8[],
    split_criterion     INT,
    feature_value       FLOAT8,
    class               FLOAT8,
    is_cont             boolean,
    less                FLOAT8,
    great               FLOAT8,
    init_impurity_val   FLOAT8,
    true_total_count    FLOAT8
    ) 
RETURNS FLOAT8[]  
AS 'MODULE_PATHNAME', 'scv_aggr_sfunc'
LANGUAGE C IMMUTABLE;

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__scv_aggr_prefunc
    (
    FLOAT8[],
    FLOAT8[]
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__scv_aggr_prefunc
    (
    arg1 FLOAT8[], 
    arg2 FLOAT8[]
    ) 
RETURNS FLOAT8[] 
AS 'MODULE_PATHNAME', 'scv_aggr_prefunc'
LANGUAGE C IMMUTABLE;

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__scv_aggr_ffunc
    (
    FLOAT8[]
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__scv_aggr_ffunc
    (
    internal_result     FLOAT8[]
    ) 
RETURNS FLOAT8[]
AS 'MODULE_PATHNAME', 'scv_aggr_ffunc'
LANGUAGE C IMMUTABLE;

DROP TYPE IF EXISTS MADLIB_SCHEMA.__scv_aggr_result CASCADE;
CREATE TYPE MADLIB_SCHEMA.__scv_aggr_result AS
    (
    info_impurity       FLOAT8,
    class_prob          FLOAT8,
    class_id            INT,
    total_size          FLOAT8,
    is_cont             BOOLEAN
    );

DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.__scv_aggr
    (
    INT,
    FLOAT8,
    FLOAT8,
    boolean,
    FLOAT8,
    FLOAT8,
    FLOAT8,
    FLOAT8
    ) CASCADE;
CREATE AGGREGATE MADLIB_SCHEMA.__scv_aggr
    (
    INT,
    FLOAT8,
    FLOAT8,
    boolean,
    FLOAT8,
    FLOAT8,
    FLOAT8,
    FLOAT8
    ) 
(
  SFUNC=MADLIB_SCHEMA.__scv_aggr_sfunc,
  m4_ifdef(`GREENPLUM',`prefunc=MADLIB_SCHEMA.__scv_aggr_prefunc,')
  FINALFUNC=MADLIB_SCHEMA.__scv_aggr_ffunc,
  STYPE=FLOAT8[],
  --BASETYPE=FLOAT8,
  initcond = '{0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}'
);

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__scv_aggr_wrapper
    (
    internal_result FLOAT8[]
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__scv_aggr_wrapper(
    internal_result FLOAT8[]
    )
RETURNS MADLIB_SCHEMA.__scv_aggr_result AS $$
DECLARE
    result              MADLIB_SCHEMA.__scv_aggr_result;
    split_criterion     INT;
    calc_pre_split      INT;
    is_cont             INT;
BEGIN
    split_criterion = internal_result[4];
    calc_pre_split = internal_result[9];
    is_cont = internal_result[8];

    IF ( split_criterion = 1 ) THEN
        IF ( calc_pre_split > 0) THEN
            result.info_impurity = internal_result[1];
        ELSE
            result.info_impurity = internal_result[5];
        END IF;
    ELSIF( split_criterion = 2 ) THEN
        IF ( calc_pre_split > 0) THEN
            result.info_impurity = internal_result[1];
        ELSE
            result.info_impurity = internal_result[6];
        END IF;
    ELSE
        IF ( calc_pre_split > 0) THEN
            result.info_impurity = internal_result[3];
        ELSE
            result.info_impurity = internal_result[7];
        END IF;
    END IF;
    
    result.class_id = internal_result[10];
    result.total_size = internal_result[12];
    result.class_prob =0;
    
    IF ( result.total_size>0 ) THEN
        IF ( internal_result[11] <= result.total_size ) THEN
            result.class_prob = internal_result[11]/result.total_size;
        ELSE
            RAISE WARNING 'result.total_size is %, max class size%', 
                result.total_size, internal_result[11];
        END IF;
    ELSE
        RAISE WARNING 'result.total_size is %', result.total_size;
    END IF;

    IF (is_cont>0) THEN
        result.is_cont = 't';
    ELSE
        result.is_cont = 'f';
    END IF;
    RETURN result;
END 
$$ LANGUAGE PLPGSQL;


/*
 * Attribute info type
 * Parameters:
 *      fid:    feature index
 *      fval:   feature value
 *      is_cont: is continuous feature or not
 */
DROP TYPE IF EXISTS MADLIB_SCHEMA.__attr_info;
CREATE TYPE MADLIB_SCHEMA.__attr_info AS
    (
    fid     INT,
    fval    FLOAT8,
    is_cont  BOOLEAN
    );

/*
 * Customized coalesce function
 * Parameters:
 *      lhs:    the first attribute info type
 *      rhs:    the second attribute info type
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA__tfc
    (
    lhs     MADLIB_SCHEMA.__attr_info,
    rhs     MADLIB_SCHEMA.__attr_info
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__tfc
    (
    lhs     MADLIB_SCHEMA.__attr_info,
    rhs     MADLIB_SCHEMA.__attr_info
    )  
RETURNS MADLIB_SCHEMA.__attr_info AS $$
DECLARE
BEGIN
    IF (lhs.fval IS NULL) THEN
        RETURN rhs;
    END IF;
    
    RETURN lhs;
END
$$ LANGUAGE PLPGSQL;

/*
 * generate the training instances for current leaf nodes
 * Parameters:
 *      input_table_name:    The name of the original table containing all
 *                           the records in training set.
 *                           uniquely identify one record.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__generate_training_instance
    (
    input_table_name     TEXT, 
    metatable_name       TEXT,
    instance_table_name  TEXT,
    selection_table_name TEXT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__generate_training_instance
    (
    input_table_name      TEXT, 
    metatable_name        TEXT,
    instance_table_name   TEXT,
    selection_table_name  TEXT
    )
RETURNS void AS $$
DECLARE
    curstmt TEXT := '';
    result_rec RECORD;
    feature_stmt TEXT := '';
    group_stmt TEXT := 'GROUP BY GROUPING SETS((), selection, (class, selection),';
    bracket_stmt TEXT := '';
    class_table_name TEXT := '';
    group_table_name TEXT := 'training_instance_aux';
BEGIN
    EXECUTE 'TRUNCATE ' || instance_table_name;
    EXECUTE 'TRUNCATE ' || group_table_name;
    
    SELECT MADLIB_SCHEMA.__format
        (
        'SELECT table_name 
        FROM MADLIB_SCHEMA.% 
        WHERE column_type = ''c''',
        metatable_name
        )
    INTO curstmt;
    
    EXECUTE curstmt INTO class_table_name;
              
    SELECT MADLIB_SCHEMA.__format
        (
            'SELECT id, column_name, is_cont 
            FROM MADLIB_SCHEMA.% 
            WHERE column_type = ''f'' ORDER BY id',
            metatable_name
        )
        INTO curstmt;
    
    FOR result_rec IN EXECUTE (curstmt) LOOP
        feature_stmt = feature_stmt || 
                        'MADLIB_SCHEMA.__tfc((' || 
                        result_rec.id || 
                        ',' ||
                        result_rec.column_name || 
                        '::FLOAT8,''' || 
                        MADLIB_SCHEMA.__to_char(result_rec.is_cont) ||
                        ''')::MADLIB_SCHEMA.__attr_info,';
        
        group_stmt =  group_stmt || 
                    '(' ||
                    result_rec.column_name || 
                    ', selection)' || 
                    ',(' || 
                    result_rec.column_name || 
                    ',class, selection),';
                    
        bracket_stmt = bracket_stmt || ')';
    END LOOP;

    SELECT MADLIB_SCHEMA.__format
        (
            'INSERT INTO % 
                SELECT ((%, null%)::MADLIB_SCHEMA.__attr_info).*, class, COUNT(*), selection
                FROM 
                    (SELECT t1.*, selection FROM % t1, % t2 WHERE t1.id = t2.id) s
                %)
                HAVING selection IS NOT NULL',
            ARRAY[
            group_table_name,
            rtrim(feature_stmt, ','),
            bracket_stmt,
            input_table_name,
            selection_table_name,
            rtrim(group_stmt, ',')
            ]
        ) INTO curstmt;
        
    EXECUTE curstmt;
    
    SELECT MADLIB_SCHEMA.__format
        (
            'INSERT INTO %(fid, fval, class, is_cont, split_value, less, great, selection)  
            SELECT fid, fval, class, is_cont,
                CASE WHEN (is_cont) THEN 
                        fval::float8 
                    ELSE 
                        NULL 
                    END AS split_value, 
                CASE WHEN (is_cont) THEN 
                        sum(count) OVER 
                            (
                            PARTITION BY selection,class,fid ORDER BY fval 
                            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
                            ) 
                        ELSE 
                            count 
                        END AS less, 
                CASE WHEN (is_cont) THEN 
                        sum(count) OVER
                            ( 
                            PARTITION BY selection,class,fid ORDER BY fval 
                            ROWS BETWEEN 1 FOLLOWING AND  UNBOUNDED FOLLOWING) 
                        ELSE 
                            NULL 
                        END AS great,
                selection
            FROM (
                SELECT DISTINCT n1.fid, n1.fval, n1.is_cont, n1.class, n2.count, n1.selection 
                FROM 
                    (
                    SELECT fid, fval, is_cont, key as class, count, selection 
                    FROM 
                        (
                        SELECT DISTINCT fval, count, fid, is_cont, selection 
                        FROM %
                        ) AS t1
                    CROSS JOIN
                    (
                        SELECT key FROM MADLIB_SCHEMA.%
                    ) as t2
                ) AS n1 
                LEFT JOIN % n2   
                ON coalesce(n2.class,0) = coalesce(n1.class,0) AND
                    coalesce(n2.selection,0) = coalesce(n1.selection,0) AND
                    coalesce(n2.fid,0)= coalesce (n1.fid,0) AND
                    coalesce(n2.fval,0) = coalesce(n1.fval,0)  
            ) t;',
            instance_table_name,
            group_table_name,
            class_table_name,
            group_table_name
        ) INTO curstmt;
        
    EXECUTE curstmt;
END
$$ LANGUAGE PLPGSQL;
	

DROP TYPE IF EXISTS MADLIB_SCHEMA.__rep_type CASCADE;
CREATE TYPE MADLIB_SCHEMA.__rep_type AS
    (
    numOfOrgClasses BIGINT[]
    );

DROP TYPE IF EXISTS MADLIB_SCHEMA.__rep_result CASCADE;
CREATE TYPE MADLIB_SCHEMA.__rep_result AS
    (
    maxClass  INT,
    isReplace INT
    );

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__rep_aggr_class_count_sfunc
    (
    BIGINT[],
    INT, 
    INT, 
    INT
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__rep_aggr_class_count_sfunc
    (
    class_count_array       BIGINT[],        
    classified_class        INT,
    original_class          INT,
    max_num_of_classes      INT
    ) 
RETURNS BIGINT[]
AS 'MODULE_PATHNAME', 'rep_aggr_class_count_sfunc'
LANGUAGE C IMMUTABLE;

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__rep_aggr_class_count_prefunc
    (
    BIGINT[],
    BIGINT[]
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__rep_aggr_class_count_prefunc
    (
    BIGINT[],
    BIGINT[]
    ) 
RETURNS BIGINT[]
AS 'MODULE_PATHNAME', 'rep_aggr_class_count_prefunc'
LANGUAGE C IMMUTABLE;

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__rep_aggr_class_count_ffunc
    (
    BIGINT[]
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__rep_aggr_class_count_ffunc
    (
    class_count_array       BIGINT[]        
    ) 
RETURNS BIGINT[]
AS 'MODULE_PATHNAME', 'rep_aggr_class_count_ffunc'
LANGUAGE C IMMUTABLE;

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__rep_aggr_class_count_wrapper
    (
    BIGINT[]
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__rep_aggr_class_count_wrapper
    (
    internal_result BIGINT[]
    ) 
RETURNS MADLIB_SCHEMA.__rep_result AS $$
DECLARE
    result MADLIB_SCHEMA.__rep_result;
BEGIN
    IF(internal_result IS NOT NULL) THEN
       result.maxClass = internal_result[1];
       result.isReplace = internal_result[2];        
    ELSE
       result.maxClass = -1;
       result.isReplace = -1;
    END IF;
    RETURN result;
END
$$ LANGUAGE PLPGSQL;

DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.__rep_aggr_class_count
    (
    INT,
    INT,
    INT
    );
CREATE AGGREGATE MADLIB_SCHEMA.__rep_aggr_class_count
    (
    INT,
    INT,
    INT
    ) 
(
  SFUNC=MADLIB_SCHEMA.__rep_aggr_class_count_sfunc,
  m4_ifdef(`GREENPLUM',`prefunc=MADLIB_SCHEMA.__rep_aggr_class_count_prefunc,')
  FINALFUNC=MADLIB_SCHEMA.__rep_aggr_class_count_ffunc,
  STYPE=BIGINT[]
);

/*
 * This type is used to store information for the calculated best split 
 *
 * Parameters:
 *      feature:              The ID of the selected feature.
 *      probability:          The predicted probability of our chosen class.
 *      maxclass:             The ID of the class chosen by the algorithm
 *      infoGain:             The information gain.
 *      live:                 1- For the chosen split, we should split further.
 *                            0- For the chosen split, we shouldn't split further.
 *      ebp_coeff:            total error for error-based pruning.
 *      is_cont_feature:      whether the selected feature is continuous.
 *      split_value:          If the selected feature is continuous, it specifies
 *                            the split value. Otherwise, it is of no use.
 *      distinct_features:    The number of distinct values for the selected feature.
 */
DROP TYPE IF EXISTS MADLIB_SCHEMA.__best_split_result CASCADE;
CREATE TYPE MADLIB_SCHEMA.__best_split_result AS
    (
    node_id             INT,
	feature             INT,
	probability         FLOAT,
	maxclass            INTEGER,
	infogain            FLOAT,
	live                INT,
	ebp_coeff           FLOAT,
    is_cont_feature     BOOLEAN,
    split_value         FLOAT,
    distinct_features   INT,
    total_size           INT
    );

/*
 * This function find the best split and return the information.
 *
 * Parameters:
 *  feature_dimensions:     The total number of different features
 *  featureValCountStr:     A string in csv format. Each element is 
 *                          a numeric value equal to the count of 
 *                          distinct features for each feature.
 *  distinct_classes:       Total number of different classes.
 *  selection:              It specifies which part of records should 
 *                          be used to calculate the best split.
 *  table_name:             The name of the table containing the training
 *                          set.
 *  confidence_level:       This parameter is used by the 'Error-Based Pruning'.
 *                          Please refer to the paper for detailed definition.
 *                          The paper's name is 'Error-Based Pruning of Decision  
 *                          Trees Grown on Very Large Data Sets Can Work!'.
 *  feature_table_name:     Is is the name of one internal table, which contains
 *                          meta data for each feature.
 *  sp_criterion:           It defines the split criterion to be used.
 *                          (1- information gain. 2- gain ratio. 3- gini)
 *  continue_gow:           It specifies whether we should still grow the tree
 *                          on the selected branch.
 * Return:
 *  The return is of the type of MADLIB_SCHEMA.__best_split_result, which contains the information
 *  for best split. Please refer to that structure for detailed definition. 
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__find_best_split
    (
    feature_dimensions      INT, 
    distinct_classes        INT,  
    selection_begin         INT, 
    selection_cnt           INT,
    table_name              TEXT, 
    confidence_level        FLOAT,
    feature_table_name      TEXT, 
    sp_criterion            INT, 
    continue_gow            INT
    );
    
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__find_best_split
    (
    feature_dimensions      INT, 
    distinct_classes        INT,  
    selection_begin         INT, 
    selection_cnt           INT, 
    table_name              TEXT, 
    confidence_level        FLOAT,
    feature_table_name      TEXT, 
    sp_criterion            INT, 
    continue_gow            INT
    ) 
RETURNS SETOF MADLIB_SCHEMA.__best_split_result AS $$
DECLARE
	total_size INT;
	result MADLIB_SCHEMA.__best_split_result;
	has_cont_text TEXT := 't';
	curstmt TEXT := '';
	result_rec RECORD;
	exec_begin TIMESTAMP;
    best_answer FLOAT8[];
BEGIN	 
	exec_begin = clock_timestamp();
	
	TRUNCATE info_impurity;
	
	SELECT MADLIB_SCHEMA.__format
        (
        'INSERT INTO info_impurity 
        SELECT selection, 
            (MADLIB_SCHEMA.__scv_aggr_wrapper(
                    MADLIB_SCHEMA.__scv_aggr
                        (%,fval,class,is_cont,less,great, 0, 0)
                    )::MADLIB_SCHEMA.__scv_aggr_result).* 
        FROM (
            SELECT * FROM % 
            WHERE fid IS NULL AND selection IS NOT NULL
            ORDER BY selection,fid,fval,split_value,class desc
            ) y 
        GROUP BY selection,fid,split_value',
        MADLIB_SCHEMA.__to_char(sp_criterion),
        table_name
        )
    INTO curstmt;
    
    EXECUTE curstmt;
        
	SELECT MADLIB_SCHEMA.__format
        (
        'SELECT s1.selection, 
            MAX(ARRAY[s1.info_impurity, s1.fid, s1.split_value, s2.class_prob, s2.class_id, s1.total_size]::FLOAT8[]) as info
        FROM (
            SELECT selection,fid,split_value,
                (MADLIB_SCHEMA.__scv_aggr_wrapper(
                    MADLIB_SCHEMA.__scv_aggr
                        (%,fval,class,is_cont,less,great, impurity, total_size)
                    )::MADLIB_SCHEMA.__scv_aggr_result).*
            FROM (
                SELECT t1.*, t2.impurity, t2.total_size FROM % t1, info_impurity t2
                WHERE t1.selection = t2.selection AND 
                        (t1.fid IS NOT NULL) AND (t1.selection IS NOT NULL AND t2.selection IS NOT NULL)
                ORDER BY t1.selection, t1.fid, t1.fval, t1.split_value, t1.class desc
                ) y 
            GROUP BY selection,fid,split_value 
            ) s1, info_impurity s2
        WHERE s1.selection = s2.selection
        GROUP BY s1.selection
        ORDER BY s1.selection;',
        MADLIB_SCHEMA.__to_char(sp_criterion),
        table_name
        )
    INTO curstmt;      
    
    -- s1.info_impurity[1], s1.fid[2], s1.split_value[3], 
    -- s2.class_prob[4], s2.class_id[5], s1.total_size[6]
    FOR result_rec IN EXECUTE (curstmt) LOOP
        result.node_id = result_rec.selection;
        best_answer = result_rec.info;
        result.feature = best_answer[2];
        result.maxclass = best_answer[5];
        result.probability = best_answer[4];
        result.infogain = best_answer[1];
        result.total_size = best_answer[6];
        result.distinct_features = MADLIB_SCHEMA.__distinct_feature_value(feature_table_name, result.feature);
        
        IF (result.probability > 0.999999999 
            OR result.infogain = 0 ) THEN
            result.live = 0;
        ELSE
            result.live = 1;
        END IF;
        
        result.ebp_coeff = MADLIB_SCHEMA.__ebp_calc_coeff(result.total_size, result.probability, confidence_level); 
        
        result.split_value = best_answer[3];
        result.is_cont_feature = (result.split_value IS NOT NULL);
        
        RETURN next result;
    END LOOP;
    	
	RETURN;
END
$$ LANGUAGE PLPGSQL;

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__jump_aggr_sfunc
    (
    INT[], 
    INT, 
    INT
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__jump_aggr_sfunc
    (
    INT[], 
    INT, 
    INT
    ) 
RETURNS INT[] AS $$
DECLARE
	temp INT[];
BEGIN
	temp = $1;
	temp[$2+1] = $3;
	
	RETURN temp;
END
$$ LANGUAGE PLPGSQL;

DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.__jump_aggr
    (
    INT, 
    INT
    );
CREATE AGGREGATE MADLIB_SCHEMA.__jump_aggr
    (
    INT, 
    INT
    ) 
(
  SFUNC=MADLIB_SCHEMA.__jump_aggr_sfunc,
  STYPE=INT[]
);


/*
 *   For training one decision tree, we need some internal tables
 *   to store intermediate results. This function creates those
 *   tables. Moreover, this function also creates the tree table
 *   specified by user.
 *
 *   Parameters:
 *      result_tree_table_name: The name of the tree specified by user.      
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__create_tree_tables
    (
    TEXT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__create_tree_tables
    (
    result_tree_table_name TEXT
    ) 
RETURNS void AS $$ 
BEGIN
    -- The training algorithm starts by eliminating all redundant points, 
    -- by producing a smaller subset of unique, weighted points,
    -- which was stored by the two tables below.
    --  Columns:
    --      id:         It is used to uniquely identify one record.
    --      feature:    It is used to store the value of one unique
    --                  record.
    --      class:      The class of that record.
    --      weight:     The count of such a record.
    --      selection:  This field is not used while removing redundant records.
    --                  It is used to train a decision tree.

	DROP TABLE IF EXISTS training_instance CASCADE;
	CREATE TEMP TABLE training_instance
	(
    	fid         INTEGER,
    	fval        FLOAT8,
    	class       INTEGER,
    	is_cont     BOOLEAN,
    	split_value FLOAT8,
    	less        BIGINT,
    	great       BIGINT,
    	selection   BIGINT
	) m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (fid, fval)');
	
    DROP TABLE IF EXISTS training_instance_aux CASCADE;
    CREATE TEMP TABLE training_instance_aux
    (
        fid         INTEGER,
        fval        FLOAT8,
        is_cont     BOOLEAN,
        class       INTEGER,
        count       INTEGER,
        selection   BIGINT
    ) m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (fid, fval)');

    CREATE TEMP TABLE instance_selection
    (
        id SERIAL,
        selection BIGINT
    )m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (id)');

    CREATE INDEX instance_selection_selection_index ON instance_selection (selection);
    
    CREATE TEMP TABLE info_impurity
    (
        selection            BIGINT,
        impurity             FLOAT8,
        class_prob           FLOAT8,
        class_id             INT,
        total_size           FLOAT8,
        is_cont              BOOLEAN
    )m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (selection)');
    
    -- The table below stores the decision tree information just constructed.
    -- It is an internal table, which contains some redundant nodes. 
    -- In the last step, we will remove those redundant nodes and move the
    -- useful records to the table specified by user.
    -- Columns:
    --      id:             Tree node id
    --      tree_location:  Set of values that lead to this branch. 
    --                      0 is the initial point (no value). But this path 
    --                      does not specify which feature was used
    --                      for the branching.
    --      feature:        Which element of the feature vector was used for 
    --                      branching at this node. Notice that this feature is not 
    --                      used in the current tree_location. It will be added 
    --                      in the next step.
    --      probability:    If forced to make a call for a dominant class 
    --                      at a given point this would be the confidence of the 
    --                      call (this is only an estimated value).
    --      maxclass:       If forced to make a call for a dominant class 
    --                      at a given point this is the selected class.
    --      split_gain:     Information gain computed using entropy (at this 
    --                      node), also used to determine termination of the branch.
    --      live:           Indication that the branch is still growing. 1 means "live". 
    --      cat_size:       Number of data point at this node.
    --      parent_id:      Id of the parent branch.
    --      jump:           Location of children for each feature value. 
    --                      Result such as [2:3]={2,3}, should be read: 
    --                      jump['feature value'+1], so in this case there were no 
    --                      0-value points for this feature. For value 1 jump to 2; 
    --                      for value 2 jump to 3;
    --      is_feature_cont: It specifies whether the selected feature is a continuous feature.
    --      split_value:    For continuous feature, it specifies the split value. Otherwise, 
    --                      it is of no meaning and fixed to 0.    
    --
	DROP TABLE IF EXISTS tree_internal CASCADE;
	CREATE TEMP TABLE tree_internal
	(
    	id              SERIAL,
    	tree_location   INT[],
    	feature         INT,
    	probability     FLOAT,
    	ebp_coeff       FLOAT,
    	maxclass        INTEGER,
    	split_gain      FLOAT,
    	live            INT,
    	cat_size        INT,
    	parent_id       INT,
    	jump            INT[],
        is_feature_cont BOOLEAN,
        split_value     FLOAT
	) m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (id)');

    -- The table below stores the final decision tree information.
    -- It is an the table specified by users. 
    -- Please refer the table above for detailed column definition.
	EXECUTE 'DROP TABLE IF EXISTS '||result_tree_table_name||' CASCADE;';
	EXECUTE 'CREATE TABLE '||result_tree_table_name||E'
	(
    	id              SERIAL,
    	tree_location   INT[],
    	feature         INT,
    	probability     FLOAT,
    	ebp_coeff       FLOAT,
    	maxclass        INTEGER,
    	split_gain      FLOAT,
    	live            INT,
    	cat_size        INT,
    	parent_id       INT,
    	jump            INT[],
        is_feature_cont BOOLEAN,
        split_value     FLOAT    
	) m4_ifdef(\`GREENPLUM\',\`DISTRIBUTED BY (id)\');';

    -- These two auxiliary internal tables help to
    -- remove redundant tree nodes and move the useful
    -- node to the final tree table.
	DROP TABLE IF EXISTS auxiliary_tree_info CASCADE;
	CREATE TEMP TABLE auxiliary_tree_info
	(
		id          INT,
		new_id      INT,
		parent_id   INT
	) m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (id)');

	DROP TABLE IF EXISTS auxiliary_tree_info2 CASCADE;
	CREATE TEMP TABLE auxiliary_tree_info2
	(
		id          INT,
		new_id      INT,
		parent_id   INT
	) m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (id)');

END 
$$ LANGUAGE PLPGSQL;

/*
 * Prune the trained tree with "Reduced Error Pruning" algorithm
 *  
 * Parameters:
 *      tree_table_name:    The name of the table containing the tree.
 *      validation_table:   The name of the table containing validation set.
 *      max_num_classes:    The count of different classes.
 */
DROP FUNCTION IF EXISTS  MADLIB_SCHEMA.__rep_prune_tree
    (
    tree_table_name     TEXT, 
    validation_table    TEXT, 
    max_num_classes     INT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__rep_prune_tree
    (
    tree_table_name     TEXT, 
    validation_table    TEXT, 
    max_num_classes     INT
    ) 
RETURNS void AS $$
DECLARE
    num_parent_ids INTEGER;
    classification_result TEXT := '_classified_temp';
    encode_table_name TEXT;
    metatable_name TEXT := '';
    curstmt TEXT;
    id_col_name  TEXT := 'id';
    class_col_name TEXT := 'class';
BEGIN
    classification_result = MADLIB_SCHEMA.__get_schema_name(tree_table_name) ||
                            MADLIB_SCHEMA.__strip_schema_name(tree_table_name) || 
                            classification_result;
                            
    SELECT MADLIB_SCHEMA.__c45_classify_internal
    (
        validation_table, 
        tree_table_name, 
        classification_result, 
        't',
        'f'
    ) INTO encode_table_name;
                            
    LOOP
        DROP TABLE IF EXISTS selected_parent_ids_rep;
        CREATE TEMP TABLE selected_parent_ids_rep(parent_id BIGINT) 
            m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (parent_id)');
       
        SELECT MADLIB_SCHEMA.__format
            (
                'INSERT INTO selected_parent_ids_rep 
                SELECT parent_id 
                FROM 
                (
                    SELECT parent_id, 
                           MADLIB_SCHEMA.__rep_aggr_class_count_wrapper(MADLIB_SCHEMA.__rep_aggr_class_count
                                (c.class, s.%, % )) as g 
                    FROM % c, % s 
                    WHERE c.id=s.% 
                    GROUP BY parent_id
                ) t 
                WHERE (t.g).isreplace >= 0 AND 
                      t.parent_id IN 
                      (
                          Select parent_id FROM % 
                          WHERE parent_id NOT IN
                              (
                                  Select parent_id  
                                  FROM % 
                                  WHERE jump IS NOT NULL
                              ) and id <> 1
                      );',
                  ARRAY[
                      class_col_name,
                      MADLIB_SCHEMA.__to_char(max_num_classes),
                      classification_result,
                      encode_table_name,
                      id_col_name,
                      tree_table_name,
                      tree_table_name
                  ]
              )
              INTO curstmt;
            
        EXECUTE curstmt;
                        
        EXECUTE 'SELECT parent_id FROM selected_parent_ids_rep limit 1;' INTO num_parent_ids;
        IF (num_parent_ids IS NULL)  THEN
            EXIT;
        END IF;

        SELECT MADLIB_SCHEMA.__format
            (
                'UPDATE % m set class = t.maxclass, parent_id = t.parent_id,leaf_id = t.id  FROM % t
                    WHERE t.id IN (SELECT parent_id FROM selected_parent_ids_rep) and
                    m.parent_id=t.id',
                classification_result,
                tree_table_name
            )
            INTO curstmt;
        EXECUTE curstmt;

        SELECT MADLIB_SCHEMA.__format
            (
                'DELETE FROM % WHERE parent_id IN (SELECT parent_id FROM selected_parent_ids_rep)',
                tree_table_name
            )
            INTO curstmt;
        
        EXECUTE curstmt;

        SELECT MADLIB_SCHEMA.__format
            (
                'UPDATE % set jump = NULL WHERE id IN (SELECT parent_id FROM selected_parent_ids_rep)',
                tree_table_name
            )
            INTO curstmt;
        
        EXECUTE curstmt;
        
    END LOOP;
    
    EXECUTE 'DROP TABLE IF EXISTS ' || encode_table_name || ' CASCADE;';
END
$$ LANGUAGE PLPGSQL;


DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__ebp_calc_coeff
    (
    total            FLOAT8,
    prob             FLOAT8,
    confidence_level FLOAT8
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__ebp_calc_coeff
    (
    total               FLOAT8,
    prob                FLOAT8,
    confidence_level    FLOAT8
    ) RETURNS FLOAT8 
AS 'MODULE_PATHNAME', 'ebp_calc_coeff'
LANGUAGE C IMMUTABLE;

/*
 * Prune the trained tree with " Error based Pruning" algorithm
 *  
 * Parameters:
 *      tree_table_name:    The name of the table containing the tree.
 */
DROP FUNCTION IF EXISTS  MADLIB_SCHEMA.__ebp_prune_tree
    (
    tree_table_name TEXT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__ebp_prune_tree
    (
    tree_table_name TEXT
    ) 
RETURNS void AS $$
DECLARE
    num_parent_ids INTEGER;
    curstmt TEXT;
BEGIN
    LOOP
        DROP TABLE IF EXISTS selected_parent_ids_ebp;
        CREATE TEMP TABLE selected_parent_ids_ebp(parent_id BIGINT) DISTRIBUTED BY(parent_id);
        
        SELECT MADLIB_SCHEMA.__format
            (
                'INSERT INTO selected_parent_ids_ebp 
                SELECT s.parent_id as parent_id 
                FROM  
                (
                    Select parent_id, sum(ebp_coeff) as ebp_coeff 
                    FROM 
                    (
                        Select parent_id, ebp_coeff 
                        FROM % 
                        WHERE parent_id NOT IN 
                            (
                            Select parent_id  FROM % WHERE jump IS NOT NULL
                            )  and id <> 1
                    ) m 
                    GROUP BY m.parent_id
                 ) s 
                 LEFT JOIN  % p 
                    ON p.id = s.parent_id 
                 WHERE  p.ebp_coeff < s.ebp_coeff;',
                 tree_table_name,
                 tree_table_name,
                 tree_table_name
            )
            INTO curstmt;
         
        EXECUTE curstmt;
                 
        EXECUTE 'SELECT parent_id FROM selected_parent_ids_ebp LIMIT 1;' INTO num_parent_ids;

        IF (num_parent_ids IS NULL)  THEN
            EXIT;
        END IF;
        
        SELECT MADLIB_SCHEMA.__format
            (
                'DELETE FROM % 
                WHERE parent_id IN 
                    (SELECT parent_id FROM selected_parent_ids_ebp)',
                tree_table_name
            )
            INTO curstmt;
            
        EXECUTE curstmt;
        
        SELECT MADLIB_SCHEMA.__format
            (
                'UPDATE %  
                SET jump = NULL 
                WHERE id IN 
                    (SELECT parent_id FROM selected_parent_ids_ebp)',
                tree_table_name
            )
            INTO curstmt;
            
        EXECUTE curstmt;
        
    END LOOP;
END
$$ LANGUAGE PLPGSQL;

/*
 * Generate the final trained tree
 *  
 * Parameters:
 *      result_tree_table_name:    The name of the table containing the tree.
 */
DROP FUNCTION IF EXISTS  MADLIB_SCHEMA.__generate_final_tree(TEXT);
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__generate_final_tree
    (
    result_tree_table_name TEXT
    ) 
RETURNS void AS $$
DECLARE
    tree_size INTEGER;
BEGIN
    TRUNCATE auxiliary_tree_info;
    TRUNCATE auxiliary_tree_info2;
    
    EXECUTE 'DELETE FROM tree_internal WHERE COALESCE(cat_size,0) = 0';
    
    EXECUTE 'SELECT count(*) FROM tree_internal' INTO tree_size;
    EXECUTE 'INSERT INTO auxiliary_tree_info (id, parent_id, new_id) SELECT id, 
            MAX(parent_id), ('||tree_size||'+1) - count(1) 
            OVER(ORDER BY id DESC ROWS UNBOUNDED PRECEDING) FROM tree_internal GROUP BY id';
    EXECUTE 'INSERT INTO auxiliary_tree_info2 (id, parent_id, new_id) 
            SELECT  g2.id,g.new_id,g2.new_id FROM auxiliary_tree_info g, 
            auxiliary_tree_info g2  WHERE g.id = g2.parent_id';
    
    TRUNCATE auxiliary_tree_info;
    EXECUTE 'TRUNCATE '||result_tree_table_name||';';
    
    EXECUTE 'INSERT INTO '|| result_tree_table_name||' SELECT n.new_id, 
            g.tree_location, g.feature, g.probability, 
            g.ebp_coeff,g.maxclass, g.split_gain, g.live, g.cat_size, 
            n.parent_id, g.jump, g.is_feature_cont, g.split_value 
            FROM tree_internal g, auxiliary_tree_info2 n WHERE n.id = g.id';
            
    EXECUTE 'INSERT INTO '||result_tree_table_name
            ||' SELECT * FROM tree_internal WHERE id = 1';
            
    TRUNCATE tree_internal;
    EXECUTE 'INSERT INTO tree_internal (id, jump) SELECT parent_id, 
            MADLIB_SCHEMA.__jump_aggr(tree_location[array_upper(tree_location,1)], id) FROM '
            ||result_tree_table_name||' GROUP BY parent_id';
            
    TRUNCATE auxiliary_tree_info2;
    EXECUTE 'UPDATE '||result_tree_table_name||' k 
            SET jump = g.jump FROM tree_internal g WHERE g.id = k.id';
    TRUNCATE tree_internal;
END
$$ LANGUAGE PLPGSQL;

/*
 * This function trains a tree 
 *  
 * Parameters:
 *      split_criterion:            This parameter specifies which split criterion 
 *                                  should be used for tree construction and pruning. The  
 *                                  valid values are ‘gain’, ‘gainratio’ and ‘gini’.
 *      training_table_name:        Name of the table/view with the source data
 *      result_tree_table_name:     The name of the table where the resulting DT will be stored.
 *      validation_table_name:      The validation table used for pruning tree. 
 *      id_col_name:                Name of the column containing id of each point.
 *      class_col_name:             Name of the column containing correct class of each point.
 *      confidence_level:                  A statistical confidence interval of the resubstitution error.
 *      max_num_iter:               Max number of branches to follow (e.g. 2000)
 *      max_tree_depth:             Maximum decision tree depth 
 *      min_percent_mode:           Specifies the minimum number of cases required in a child node
 *      min_percent_split:          specifies the minimum number of cases required in a node  
 *                                  in order for a further split to be possible.
 *      verbosity:                  If True (or 1) will run in verbose mode
 *
 * Return:
 *      One summary result for training tree. Please refer to the structure of
 *      'MADLIB_SCHEMA.c45_train_result' for detailed definition.
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__train_tree
    (
    split_criterion         TEXT,
    training_table_name     TEXT, 
    training_table_meta     TEXT,
    result_tree_table_name  TEXT,
    validation_table_name   TEXT,
    id_col_name             TEXT, 
    class_col_name          TEXT, 
    confidence_level        FLOAT, 
    max_num_iter            INT, 
    max_tree_depth          INT,
    min_percent_mode        FLOAT,
    min_percent_split       FLOAT,
    verbosity               INT
    ) 
RETURNS MADLIB_SCHEMA.c45_train_result AS $$
DECLARE
    feature_dimension INT;
    num_live_nodes INT;
    selection INT;
    location INT[];
    temp_location INT[];
    num_classes INT;
    max_iter INT = max_num_iter;
    max_depth INT := max_tree_depth;
    answer MADLIB_SCHEMA.__best_split_result;
    location_size INT;
    max_id INT;
    exec_begin TIMESTAMP;
    find_best_time INTERVAL;
    begin_find_best_time TIMESTAMP;
    data_transfer_time INTERVAL;
    begin_data_transfer TIMESTAMP;
    total_size FLOAT;
    sp_crit   INT := 1;
    curstmt TEXT := '';
    grow_tree INT := 0;
    ret MADLIB_SCHEMA.c45_train_result;
    selection_visit BOOLEAN[];
    selection_index INT := 1;
    select_max_id INT := 0;
    best_time_cmp BOOLEAN := 'f';
BEGIN   
    exec_begin = clock_timestamp();
    find_best_time = exec_begin - exec_begin;

    ret.split_criterion = split_criterion;   
                  
    IF(split_criterion = 'infogain') THEN
        sp_crit = 1;
    ELSIF (split_criterion = 'gainratio') THEN
        sp_crit = 2;
    ELSIF (split_criterion = 'gini') THEN
        sp_crit = 3;
    ELSE
        RAISE EXCEPTION '%', 'Invalid split criterion!';
    END IF;

    PERFORM MADLIB_SCHEMA.__create_tree_tables(result_tree_table_name);
    
    EXECUTE 'SELECT count(*) FROM '|| training_table_name ||';' INTO total_size;
    
    IF(verbosity > 0) THEN
        RAISE INFO 'INPUT TABLE SIZE: %', total_size;
    END IF;
    
    ret.training_set_size = total_size;
    
    EXECUTE 'INSERT INTO instance_selection SELECT id, 1 FROM ' || training_table_name;
    
    feature_dimension = MADLIB_SCHEMA.__num_of_feature(training_table_meta);

    num_classes = MADLIB_SCHEMA.__num_of_class(training_table_meta); 
    
    IF(verbosity > 0) THEN
        RAISE INFO 'NUMBER OF CLASSES IN THE TRAINING SET %', num_classes;
    END IF;
    
    IF(num_classes < 2 OR num_classes > (1024 * 1024 * 8))THEN
        RAISE EXCEPTION 'The number of classes must be in range 2 to 8million!';
    END IF;
    
    INSERT INTO tree_internal (tree_location, feature, probability, maxclass, 
        split_gain, live, cat_size, parent_id) 
        VALUES(ARRAY[0], 0, 1, 1, 1, 1, 0, 0);
               
    location_size = 0;
    
    begin_data_transfer = clock_timestamp();
    
    LOOP
        EXECUTE 'SELECT COUNT(id) FROM tree_internal WHERE live = 1' INTO num_live_nodes;
        
        IF ((max_depth < 0) OR (num_live_nodes < 1)) THEN
            IF(verbosity > 0) THEN
                RAISE INFO 'EXIT: LIMIT tree depth: % OR LIMIT iteration: % OR NO NODES LEFT', max_depth, max_iter;
            END IF;
            
            EXIT;
        END IF;
        
        max_depth = max_depth - 1;
        IF(verbosity > 0) THEN
            RAISE INFO 'current level: %', max_tree_depth - max_depth;
        END IF;

        SELECT id FROM tree_internal WHERE live = 1 ORDER BY id LIMIT 1 INTO selection;
        SELECT id FROM tree_internal WHERE live = 1 ORDER BY id DESC LIMIT 1 INTO max_id;
        
        FOR selection_index IN 1..(max_id - selection + 1) LOOP
            selection_visit[selection_index] = 'f';
        END LOOP;
        
        select_max_id = max_id;
        
        PERFORM MADLIB_SCHEMA.__generate_training_instance
            (
            training_table_name,
            training_table_meta,
            'training_instance',
            'instance_selection'
            );

        begin_find_best_time = clock_timestamp();
        
        best_time_cmp = 't';
        
        FOR answer IN 
                    (
                    SELECT * FROM MADLIB_SCHEMA.__find_best_split
                        (
                        feature_dimension, 
                        num_classes, 
                        selection, 
                        max_id - selection + 1, 
                        'training_instance',
                        confidence_level,
                        training_table_meta,
                        sp_crit,
                        grow_tree
                        )
                    ) 
                    LOOP
            IF (0 = answer.feature) THEN
                CONTINUE;
            END IF;
            
            -- mark this node id was visited
            selection_visit[answer.node_id - selection + 1] = 't';
            
            IF (answer.is_cont_feature) THEN
                IF(verbosity > 0) THEN
                    RAISE INFO 'selected feature is continuous';
                    RAISE INFO 'answer:%', answer;  
                END IF;
                
                EXECUTE 'UPDATE tree_internal 
                        SET feature = '||answer.feature||',
                            probability = '|| answer.probability||',
                            maxclass = '||answer.maxclass||',
                            split_gain = '||answer.infogain||',
                            ebp_coeff = '||answer.ebp_coeff||',
                            cat_size = '||answer.total_size|| E',
                            live = 0,
                            is_feature_cont = \'t\',
                            split_value = '|| answer.split_value ||
                         ' WHERE id =' || answer.node_id || ';';
            ELSE
                IF (verbosity > 0) THEN
                    RAISE INFO 'selected feature is discrete';
                    RAISE INFO 'answer:%', answer;       
                END IF;   
                  
                EXECUTE 'UPDATE tree_internal 
                        SET feature = '||answer.feature||',
                            probability = '|| answer.probability||',
                            maxclass = '||answer.maxclass||',
                            split_gain = '||answer.infogain||',
                            ebp_coeff = '||answer.ebp_coeff||',
                            cat_size = '||answer.total_size|| E',
                            live = 0,
                            is_feature_cont = \'f\',
                            split_value = null
                        WHERE id =' || answer.node_id || ';';
            END IF;   
            
            -- no need to grow tree with the attribute 
            -- if comes up the maximum number;
            -- if its percent is lower than minimum split value
            IF (answer.node_id >= max_num_iter) THEN
                max_iter = 0;
                CONTINUE;
            END IF;

            IF (answer.total_size < min_percent_split * total_size) THEN
                CONTINUE;
            END IF;
                        
            -- grow the tree
            EXECUTE 'SELECT gt.tree_location FROM tree_internal gt 
                    WHERE gt.id =' || answer.node_id ||';' 
                    INTO location;
            
            --here insert live determination function 
            IF (answer.live > 0 and answer.is_cont_feature = 'f') THEN 
                IF(verbosity > 0) THEN
                    RAISE INFO 'determine live for discrete';
                END IF;  
                            
                FOR i IN 1..answer.distinct_features LOOP
                    temp_location = location;
                    temp_location[array_upper(temp_location,1)+1] = i;
                    EXECUTE 'INSERT INTO tree_internal (tree_location, feature, probability, 
                        maxclass, split_gain, live, parent_id) 
                        VALUES(ARRAY['||array_to_string(temp_location, ',')||'], 0, 1, 1, 1, 1, '|| answer.node_id ||');';
                END LOOP;
                
                SELECT MADLIB_SCHEMA.__format
                    (
                    'UPDATE instance_selection s
                    SET selection = % + %
                    FROM % t
                    WHERE selection = % and t.id = s.id;',
                    ARRAY[
                        MADLIB_SCHEMA.__get_feature_name(answer.feature,training_table_meta),
                        MADLIB_SCHEMA.__to_char(select_max_id),
                        training_table_name,
                        MADLIB_SCHEMA.__to_char(answer.node_id)
                    ]
                    ) 
                    INTO curstmt;
                    
                EXECUTE curstmt;
                
                select_max_id = select_max_id + answer.distinct_features;
                
            ELSIF (answer.live > 0 and answer.is_cont_feature = 't') THEN
                IF(verbosity > 0) THEN
                    RAISE INFO 'determine live for continuous';
                END IF;  
                FOR i IN 1..2 LOOP
                    temp_location = location;
                    temp_location[array_upper(temp_location,1)+1] = i;
                    EXECUTE 'INSERT INTO tree_internal (tree_location, feature, probability, 
                        maxclass, split_gain, live, parent_id) 
                        VALUES(ARRAY['||array_to_string(temp_location, ',')||'], 0, 1, 1, 1, 1, '|| answer.node_id ||');';
                    
                END LOOP;

                SELECT MADLIB_SCHEMA.__format
                    (
                    'UPDATE instance_selection s
                    SET selection = (
                                    CASE WHEN (% < %) THEN
                                        2
                                    ELSE
                                        1
                                    END
                                    )
                                    + %
                    FROM % t
                    WHERE selection = % and t.id = s.id;',
                    ARRAY[
                        MADLIB_SCHEMA.__to_char(answer.split_value),
                        MADLIB_SCHEMA.__get_feature_name(answer.feature, training_table_meta),
                        MADLIB_SCHEMA.__to_char(select_max_id),
                        training_table_name,
                        MADLIB_SCHEMA.__to_char(answer.node_id)
                    ]
                    ) 
                    INTO curstmt;
                    
                EXECUTE curstmt;
                               
                select_max_id = select_max_id + 2;
            ELSE
                -- answer.live = 0
                -- process the min_percent_mode
                IF (total_size * min_percent_mode > answer.total_size) THEN
                    --RAISE INFO '%', 'min_percent_mode';
                    EXECUTE 'DELETE FROM tree_internal WHERE id = ' || answer.node_id || ';';
                END IF;
            END IF;                      
        END LOOP;
        
        -- Remove the nodes which contains no data
        FOR selection_index IN selection..max_id  LOOP
            IF (NOT selection_visit[selection_index - selection + 1]) THEN
                --RAISE NOTICE '%, %', 'unneccesary node!', selection_index;
                EXECUTE 'DELETE FROM tree_internal WHERE id = '|| selection_index ||';';
            END IF;
        END LOOP;
               
        IF(verbosity > 0) THEN
            RAISE INFO 'computation time in this level:%',(clock_timestamp() - begin_find_best_time);
        END IF;
        IF (best_time_cmp) THEN
            find_best_time = find_best_time + (clock_timestamp() - begin_find_best_time);
            best_time_cmp = 'f';
        END IF;
    END LOOP;
    
    data_transfer_time =  (clock_timestamp() - begin_data_transfer) - find_best_time;
    
    PERFORM MADLIB_SCHEMA.__generate_final_tree( result_tree_table_name );
    
    IF (confidence_level < 1.0) THEN
       EXECUTE 'SELECT MADLIB_SCHEMA.__ebp_prune_tree(' || quote_literal(result_tree_table_name) || ');';
    END IF;
    
    IF (validation_table_name IS NOT NULL) THEN
       PERFORM MADLIB_SCHEMA.__rep_prune_tree(result_tree_table_name, validation_table_name , num_classes);
    END IF;
    
    EXECUTE 'select count(*) from '||result_tree_table_name||';' into ret.tree_nodes;
    EXECUTE 'select max(array_upper(tree_location,1)) from '||result_tree_table_name||';' into ret.tree_depth;
    IF(verbosity > 0) THEN
        /*
         * We measure the time with the dataset of kddcup1999, which can be found at
         * http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html.
         * We test the training with that dataset on two machines with two X5570@2.93GHz CPUs
         * and 48GB memory running x86_64 GNU/Linux and GPDB 4.2. 
         * 
         * We use three different configurations, 4 segments, 8 segments and 16 segments.
         * The 16 segments configuration spans over two machines connected with 10GB Ethernet HUB
         * The test results are as follows:
         * 
         * 4 segments results:
         * Data conversion:            5 minutes 43 seconds
         * Find best time:             30 minutes 10 seconds
         * olap query/windows func:    43 minutes 5 seconds
         * Total train:                79 minutes 06 seconds
         * 
         * 8 segments results:
         * Data conversion:            4 minutes 10 seconds
         * Find best time:             19 minutes 14 seconds
         * olap query/windows func:    29 minutes 58 seconds
         * Total train:                53 minutes 40 seconds
    
         * 16 segments results:
         * Data conversion:            2 minutes 24 seconds
         * Find best time:             10 minutes 29 seconds
         * olap query/windows func:    16 minutes 17 seconds
         * Total train:                29 minutes 13 seconds
         */

        RAISE INFO 'total of find best time: %', find_best_time;
        RAISE INFO 'total of data transfer time: %', data_transfer_time;
        RAISE INFO 'total of pruning time: %', (clock_timestamp() - begin_data_transfer -find_best_time - data_transfer_time);
        RAISE INFO 'total of __train_tree time: %', clock_timestamp() - exec_begin;        
    END IF;
    
    ret.cost_time = clock_timestamp() - exec_begin;
    
    RETURN ret;
END
$$ LANGUAGE PLPGSQL;

/*
 * @brief Train a decision tree model. User must specify all those data with that 
 *        long form function.
 *
 * @param split_criterion_name This parameter specifies which split criterion 
 *          should be used for tree construction and pruning. 
 *          The valid values are infogain, gainratio, or gini.
 * @param training_table_name Name of the table/view with the source data
 * @param result_tree_table_name The name of the table where the resulting DT will be stored.
 * @param validation_table_name The validation table used for pruning tree. 
 * @param continuous_feature_names A comma-separated list of the names of the features whose values are continuous. 
 * @param feature_col_names A comma-separated list of names of the table columns, each of which defines a feature.
 * @param id_col_name Name of the column containing id of each point.
 * @param class_col_name Name of the column containing correct class of each point.
 * @param confidence_level A  statistical  confidence  interval of  the   resubstitution  error.
 * @param max_num_iter Max number of branches to follow (e.g. 2000)
 * @param max_tree_depth Maximum decision tree depth 
 * @param min_percent_mode Specifies the minimum number of cases required in a child node
 * @param min_percent_split specifies the minimum number of cases required in a node in order for 
 *           a further split to be possible.
 * @param verbosity If True (or 1) will run in verbose mode
 *
 * @return Table MADLIB_SCHEMA.tree:
 *  - <tt>id SERIAL</tt> - Tree node id
 *  - <tt>tree_location INT[]</tt> - Set of values that lead to this branch. 
 *     0 is the initial point (no value). But this path does not specify which 
 *     feature was used for the branching.
 *  - <tt>feature INT</tt>: Which element of the feature vector was used for 
 *     branching at this node. Notice that this feature is not used in the current 
 *     <tt>tree_location</tt>, it will be added in the next step.
 *  - <tt>probability FLOAT</tt> - If forced to make a call for a dominant class 
 *     at a given point this would be the confidence of the call (this is only an 
 *     estimated value).
 *  - <tt>maxclass INTEGER</tt> - If forced to make a call for a dominant class 
 *     at a given point this is the selected class.
 *  - <tt>split_gain FLOAT</tt> - Information gain computed using entropy (at this 
 *     node), also used to determine termination of the branch.
 *  - <tt>live INT</tt> - Indication that the branch is still growing. 1 means "live". 
 *     Exit value may be 1 if number of branches reached before termination condition is met.
 *  - <tt>cat_size INT</tt> - Number of data point at this node.
 *  - <tt>parent_id INT</tt> - Id of the parent branch.
 *  - <tt>jump INT[]</tt> - Location of children for each feature value. Notice 
 *     that assuming that the data is sparse we can have value 0 - representing 
 *     no value. Result such as [2:3]={2,3}, should be read: 
 *     jump['feature value'+1], so in this case there were no 0-value points for 
 *     this feature. For value 1 jump to 2; for value 2 jump to 3;
 *  - <tt>is_feature_cont BOOLEAN</tt> - It specifies whether the selected feature is a continuous feature.
 *  - <tt>split_value FLOAT</tt> - For continuous feature, it specifies the split value. Otherwise, 
 *     it is fixed to 0.
 *
 * @note This function starts an iterative algorithm. It is not an aggregate
 *       function. Source and column names have to be passed as strings (due to
 *       limitations of the SQL syntax).
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_train
    (
    split_criterion             TEXT,
    training_table_name         TEXT, 
    result_tree_table_name      TEXT,
    validation_table_name       TEXT, 
    continuous_feature_names    TEXT, 
    feature_col_names           TEXT, 
    id_col_name                 TEXT, 
    class_col_name              TEXT, 
    confidence_level            FLOAT,
    how2handle_missing_value   TEXT, 
    max_num_iter                INT, 
    max_tree_depth              INT, 
    min_percent_mode            FLOAT,
    min_percent_split           FLOAT, 
    verbosity                   INT
    ) 
RETURNS MADLIB_SCHEMA.c45_train_result AS $$
DECLARE
    cont_feature_col_names TEXT[];
    feature_name_array TEXT[];
    exec_begin TIMESTAMP;
    tree_schema_name TEXT;
    tree_table_name TEXT;
    training_encoded_table_name TEXT;
    training_metatable_name TEXT;
    h2hmv_routine_id INT := 1;  
    ret MADLIB_SCHEMA.c45_train_result;
BEGIN   
    exec_begin = clock_timestamp();

    PERFORM MADLIB_SCHEMA.__assert
        (
            split_criterion = 'infogain' OR split_criterion = 'gainratio' OR split_criterion = 'gini',
            'split_criterion must be infogain, gainratio or gini!'
        );
            
    PERFORM MADLIB_SCHEMA.__assert
        (
            MADLIB_SCHEMA.__get_schema_name(training_table_name),
            MADLIB_SCHEMA.__strip_schema_name(training_table_name),
            't'
        );

    PERFORM MADLIB_SCHEMA.__assert
        (
            validation_table_name IS NULL OR
            MADLIB_SCHEMA.__table_exists
                (
                    MADLIB_SCHEMA.__get_schema_name(validation_table_name),
                    MADLIB_SCHEMA.__strip_schema_name(validation_table_name)
                )
             ,
             'The specified validation table doesnot exist!'
        );

    PERFORM MADLIB_SCHEMA.__assert
            (
                MADLIB_SCHEMA.__get_schema_name(result_tree_table_name),
                MADLIB_SCHEMA.__strip_schema_name(result_tree_table_name),
                'f'
            );   
            
    PERFORM MADLIB_SCHEMA.__assert
            (
                id_col_name IS NOT NULL                 AND 
                class_col_name IS NOT NULL              AND
                length(btrim(id_col_name, ' ')) > 0     AND 
                length(btrim(class_col_name, ' ')) > 0, 
                'id and class column names must be non-empty!'
            );
                                                         
    PERFORM MADLIB_SCHEMA.__assert
            (
                confidence_level >= 0.001 OR confidence_level <= 100.0, 
                'Confidence level value should be in range from 0.001 to 100!'
            );
            
    PERFORM MADLIB_SCHEMA.__assert
            (
                how2handle_missing_value = 'ignore' OR how2handle_missing_value = 'explicit',
                'how2handle_missing_value must be ignore or explicit!'
            );    

    PERFORM MADLIB_SCHEMA.__assert
            (
                max_num_iter        >= 0 AND
                max_tree_depth      >= 0 AND
                min_percent_mode    >= 0 AND
                min_percent_split   >= 0,
                'Invalid parameters'
            );
                           
    IF (how2handle_missing_value = 'ignore') THEN
        h2hmv_routine_id = 1;
    ELSE
        h2hmv_routine_id = 2;
    END IF;
    
    PERFORM MADLIB_SCHEMA.__create_traininginfo();
    PERFORM MADLIB_SCHEMA.__insert_into_traininginfo
                (
                result_tree_table_name,
                training_table_name,
                null,
                null,
                validation_table_name,
                how2handle_missing_value
                );

    cont_feature_col_names = MADLIB_SCHEMA.__csvstr_to_array(continuous_feature_names);
    
    IF ( verbosity > 0 ) THEN
        RAISE INFO 'continuous features:%', cont_feature_col_names;
    END IF;
    
    tree_table_name = MADLIB_SCHEMA.__strip_schema_name(result_tree_table_name);
    tree_schema_name = MADLIB_SCHEMA.__get_schema_name(result_tree_table_name);
    
    IF(verbosity > 0) THEN
        RAISE INFO 'table name after strip schema:%',tree_table_name;
    END IF;
    
    training_encoded_table_name = 'MADLIB_SCHEMA.' || tree_schema_name || '_' ||tree_table_name || '_encoded';
    training_metatable_name =  tree_schema_name || '_' || tree_table_name || '_data_info';
 
    IF(verbosity > 0) THEN
        RAISE INFO 'Before encoding: %', clock_timestamp() - exec_begin;
    END IF; 
        
    IF ( feature_col_names IS NOT NULL ) THEN
        PERFORM MADLIB_SCHEMA.__encode_tabular_table
            (
            training_table_name,
            id_col_name,
            feature_col_names,
            class_col_name,
            cont_feature_col_names,
            training_encoded_table_name,
            training_metatable_name,
            h2hmv_routine_id,
            't'
            );
    ELSE
        PERFORM MADLIB_SCHEMA.__encode_tabular_table
            (
            training_table_name,
            id_col_name,
            class_col_name,
            cont_feature_col_names,
            training_encoded_table_name,
            training_metatable_name,
            h2hmv_routine_id,
            't'
            );
    END IF;

    
    PERFORM  MADLIB_SCHEMA.__set_encode_and_metatable_name
                ( 
                result_tree_table_name, 
                training_metatable_name,
                training_encoded_table_name
                );
                
    IF(verbosity > 0) THEN
            RAISE INFO 'After encoding: %', clock_timestamp() - exec_begin;
            RAISE INFO 'successfully encode the input table :%',training_encoded_table_name;
    END IF;    

    ret = MADLIB_SCHEMA.__train_tree
            (
            split_criterion ,
            training_encoded_table_name ,
            training_metatable_name,
            result_tree_table_name ,
            validation_table_name , 
            'id', 
            'class', 
            confidence_level * 0.01 ,
            max_num_iter , 
            max_tree_depth , 
            min_percent_mode ,
            min_percent_split,
            verbosity
            );

    IF ( verbosity > 0 ) THEN
            RAISE INFO 'Training Total Time: %', clock_timestamp() - exec_begin;
    END IF;
    
    RETURN ret;
END
$$ LANGUAGE PLPGSQL;

/*
 * @brief Another C45 train algorithm in short form
 *
 * @param the same as the ones accepted by the long form 
 *
 * @return the same as the one returned by the long form
 *
 * @note  
 *      validation_table_name:      NULL 
 *      continuous_feature_names:   NULL
 *      id_col_name Name:           'id'
 *      class_col_name Name:        'class'
 *      confidence_level:           25
 *      max_num_iter:               2000
 *      max_tree_depth:             10 
 *      min_percent_mode:           0.001
 *      min_percent_split:          0.01 
 *      verbosity:                  0
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_train
    (
    split_criterion         TEXT,
    training_table_name     TEXT, 
    result_tree_table_name  TEXT
    ) 
RETURNS MADLIB_SCHEMA.c45_train_result AS $$
DECLARE
    ret MADLIB_SCHEMA.c45_train_result;
BEGIN   
    ret = MADLIB_SCHEMA.c45_train(
        split_criterion,
        training_table_name, 
        result_tree_table_name,
        null,
        null,       
        null,
        'id',
        'class',
        25,
        'ignore',
        2000,
        10,
        0.001,
        0.01,
        0           
    );
    
    RETURN ret;
END
$$ LANGUAGE PLPGSQL;

/*
 * @brief Another C45 train algorithm in short form
 *
 * @param the same as the ones accepted by the long form 
 *
 * @return the same as the one returned by the long form
 *
 * @note     
 *      max_num_iter:      			2000
 *      max_tree_depth:     		10 
 *      min_percent_mode:   		0.001
 *      min_percent_split:  		0.01 
 *      verbosity:          		0
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_train
    (
    split_criterion             TEXT,
    training_table_name         TEXT, 
    result_tree_table_name      TEXT,
    validation_table_name       TEXT, 
    continuous_feature_names    TEXT, 
    feature_col_names           TEXT, 
    id_col_name                 TEXT, 
    class_col_name              TEXT, 
    confidence_level            FLOAT,
    how2handle_missing_value    TEXT
    ) 
RETURNS MADLIB_SCHEMA.c45_train_result AS $$
DECLARE
    ret MADLIB_SCHEMA.c45_train_result;
BEGIN   
    ret = MADLIB_SCHEMA.c45_train
            (
            split_criterion,
            training_table_name, 
            result_tree_table_name,
            validation_table_name , 
            continuous_feature_names , 
            feature_col_names , 
            id_col_name , 
            class_col_name , 
            confidence_level,
            how2handle_missing_value,
            2000,
            10,
            0.001,
            0.01,
            0           
            );
    RETURN ret;
END
$$ LANGUAGE PLPGSQL;

/*
 * This is a internal function for displaying the tree in human readable format.
 * It use the depth-first strategy to traverse a tree and print values.
 * Parameters:
 *      tree_table:         The name of the table with information for the 
 *                          trained tree.
 *      id:                 The id of current node. This node and all of its  
 *                          children are displayed.
 *      feature_id:         The id of a feature, which was used to split in the 
 *                          parent of current node.
 *      depth:              The depth of current node.
 *      is_cont:            It specifies whether the feature denoted by 'feature_id'
 *                          is continuous or not.
 *      split_value:        For continuous feature, it specifies the split value. 
 *                          Otherwise, it is of no meaning.
 *      metatable_name:     For tabular format, this table contains the meta data
 *                          to encode the input table.
 * Return:
 *      It returns the text containing the information of human readable tree.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__display_tree
    (
    tree_table      TEXT, 
    id              INT, 
    feature_id      INT, 
    depth           INT, 
    is_cont         BOOLEAN, 
    split_value     FLOAT,
    metatable_name  TEXT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__display_tree
    (
    tree_table      TEXT, 
    id              INT, 
    feature_id      INT, 
    depth           INT, 
    is_cont         BOOLEAN, 
    split_value     FLOAT,
    metatable_name  TEXT
    ) 
RETURNS TEXT AS $$ 
DECLARE
    ret                     TEXT := '';
    tree_location           INT[];
    feature                 INT;
    jump                    INT[];
    maxclass                INT;
    cat_size                INT;
    is_feature_cont         BOOLEAN;
    temp_split_value        FLOAT;
    index                   INT;
    curr_value              INT;
    probability             FLOAT;
BEGIN
    IF (id IS NULL OR id <= 0) THEN
        RETURN ret;
    END IF;
    
    EXECUTE 'select tree_location, feature, jump,is_feature_cont, split_value,
        maxclass,cat_size,probability from '
        || tree_table || ' where id =' || id ||';' INTO tree_location, feature,jump, 
        is_feature_cont,temp_split_value,maxclass, cat_size, probability; 

    curr_value = tree_location[array_upper(tree_location,1)];

    FOR index IN 0..depth LOOP
        ret = ret || '    ';
    END LOOP;
    
    IF (id > 1) THEN
        ret = ret ||MADLIB_SCHEMA.__get_feature_name(feature_id,metatable_name)||': ';

        IF (is_cont) THEN
            IF (curr_value = 1) THEN
                ret = ret || ' <= ';
            ELSE
                ret = ret || ' > ';
            END IF;
            ret = ret || split_value;
        ELSE
            ret = ret   || 
                  ' = ' || 
                  MADLIB_SCHEMA.__get_feature_value
                    (
                    feature_id, 
                    curr_value, 
                    metatable_name
                    );
        END IF;
    ELSE
        ret = 'Root Node ';
    END IF;

    ret = ret                                                       || 
          ' : class('                                               ||  
          MADLIB_SCHEMA.__get_class_value(maxclass,metatable_name)  || 
          ')   num_elements('                                       || 
          cat_size                                                  || 
          ')  predict_prob('                                        ||
          probability                                               ||
          ')';

    ret = ret || E'\n';

    index = array_lower(jump,1);
    WHILE index <= array_upper(jump,1) LOOP
        ret = ret || MADLIB_SCHEMA.__display_tree(
                            tree_table, 
                            jump[index], 
                            feature, 
                            depth+1, 
                            is_feature_cont, 
                            temp_split_value, 
                            metatable_name);
        index = index +1;
    END LOOP; 

    RETURN ret;
END $$ LANGUAGE PLPGSQL;


/*
 * @brief Display the trained decision tree model with human readable format
 *
 * @param tree_table Name of the table containing the tree's information
 *
 * @return the text representing the tree with human readable format.
 *
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.c45_display
    (
    tree_table TEXT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_display
    (
    tree_table TEXT
    ) 
RETURNS TEXT AS $$
DECLARE
    metatable_name TEXT := null;
BEGIN
    PERFORM MADLIB_SCHEMA.__assert
            (
                MADLIB_SCHEMA.__get_schema_name(tree_table),
                MADLIB_SCHEMA.__strip_schema_name(tree_table),
                't'
            );   
            
    metatable_name = MADLIB_SCHEMA.__get_metatable_name( tree_table );
    
    RETURN MADLIB_SCHEMA.__display_tree(tree_table, 1, 0, 0, 'f', 0, metatable_name);
END $$ LANGUAGE PLPGSQL;

/*
 *  An internal c45 classification function. It is used to perform
 *  the real classification process.
 *
 *  Parameters:
 *      classification_table_name:  The table containing the classification set.
 *      tree_table_name:            The table containing the final tree.
 *      result_table_name:          The table containing the classification
 *                                  result.
 *      is_result_temp              It specifies whether the result_table should
 *                                  be temporary.
 *      verbosity:                  Whether printing those debug information.
 *  Return:
 *      The caller may need to clean that internal table.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__c45_classify_internal
    (
    classification_table_name   TEXT, 
    tree_table_name             TEXT, 
    result_table_name           TEXT, 
    is_result_temp              BOOLEAN,
    verbosity                   BOOLEAN
    );
    
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__c45_classify_internal
    (
    classification_table_name   TEXT, 
    tree_table_name             TEXT, 
    result_table_name           TEXT, 
    is_result_temp              BOOLEAN,
    verbosity                   BOOLEAN
    ) 
RETURNS TEXT AS $$
DECLARE
    table_pick              INT := 1;
    remains_to_classify     INT;
    size_finished           INT;
    time_stamp              TIMESTAMP;
    metatable_name          TEXT := '';
    id_col_name             TEXT := 'id';
    curr_level              INT := 1;
    max_level               INT := 0;
    create_text             TEXT := '';
    h2hmv_routine_id        INT := 0;
    curstmt                 TEXT := '';
    encode_table_name       TEXT := classification_table_name || '_encoded_temp';
    table_names             TEXT[] = '{classified_instance_ping,classified_instance_pong}';
BEGIN
    time_stamp = clock_timestamp();

    PERFORM MADLIB_SCHEMA.__assert
            (
                MADLIB_SCHEMA.__get_schema_name(classification_table_name),
                MADLIB_SCHEMA.__strip_schema_name(classification_table_name),
                't'
            );   

    PERFORM MADLIB_SCHEMA.__assert
            (
                MADLIB_SCHEMA.__get_schema_name(tree_table_name),
                MADLIB_SCHEMA.__strip_schema_name(tree_table_name),
                't'
            );   

    PERFORM MADLIB_SCHEMA.__assert
            (
                MADLIB_SCHEMA.__get_schema_name(result_table_name),
                MADLIB_SCHEMA.__strip_schema_name(result_table_name),
                'f'
            ); 
                                        
    SELECT MADLIB_SCHEMA.__get_metatable_name(tree_table_name) INTO metatable_name;

    SELECT MADLIB_SCHEMA.__get_routine_id(tree_table_name) INTO h2hmv_routine_id;
    
    PERFORM MADLIB_SCHEMA.__encode_tabular_table
        (
            classification_table_name, 
            encode_table_name, 
            metatable_name, 
            h2hmv_routine_id,
            't'
        );
        
    IF ( verbosity ) THEN
        RAISE INFO 'tabular format. id_col_name: %', id_col_name;
    END IF;        
    
    DROP TABLE IF EXISTS classified_instance_ping;
    CREATE TEMP TABLE classified_instance_ping
    (
        id          INT,
        jump        INT,
        class       INT,
        prob        FLOAT,
        parent_id   INT,
        leaf_id     INT
    ) m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (jump)');
    
    DROP TABLE IF EXISTS classified_instance_pong;
    CREATE TEMP TABLE classified_instance_pong
    (
        id          INT,
        jump        INT,
        class       INT,
        prob        FLOAT,
        parent_id   INT,
        leaf_id     INT
    ) m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (jump)');
    
    IF (is_result_temp) THEN
        create_text = 'CREATE TEMP TABLE ';
    ELSE
        create_text = 'CREATE TABLE ';
    END IF;
    
    EXECUTE create_text || result_table_name || E'
    (
        id          INT,
        jump        INT,
        class       INT,
        prob        FLOAT,
        parent_id   INT,
        leaf_id     INT
    ) m4_ifdef(\`GREENPLUM\',\`DISTRIBUTED BY (id)\');';


    EXECUTE 'INSERT INTO classified_instance_ping (id, jump, class, prob) SELECT '
        ||id_col_name||', 1, 0, 0 FROM ' || encode_table_name || ';';  

    
    EXECUTE 'SELECT max(array_upper(tree_location,1)) FROM '||tree_table_name||';'  INTO max_level;

    IF( max_level is NULL ) THEN
        RAISE EXCEPTION 'tree should not be empty';
    END IF;

    FOR curr_level IN 1..max_level LOOP
        IF(verbosity) THEN  
            RAISE INFO 'new_depth: %', curr_level;
        END IF;

        EXECUTE 'INSERT INTO ' || result_table_name ||' SELECT * FROM '|| 
            table_names[(table_pick) % 2 + 1] ||' WHERE jump = 0;';
        EXECUTE 'TRUNCATE '|| table_names[(table_pick) % 2 + 1] ||';';
        EXECUTE 'SELECT count(id) FROM '||result_table_name||';' INTO size_finished;
        IF(verbosity) THEN  
            RAISE INFO 'size_finished %', size_finished;
        END IF;            
        table_pick = table_pick % 2 + 1; 
        
        EXECUTE 'SELECT count(*) FROM '|| table_names[(table_pick) % 2 + 1] ||';' 
            INTO remains_to_classify;
            
        IF (remains_to_classify = 0) THEN
            IF(verbosity) THEN  
                RAISE INFO 'size_finished: % remains_to_classify: %', 
                    size_finished, remains_to_classify;
            END IF;  
                  
            EXIT;
        END IF;

        SELECT MADLIB_SCHEMA.__format(
            'INSERT INTO %
            SELECT pt.id, 
            CASE WHEN (is_feature_cont) THEN 
                    COALESCE(gt.jump[
                                     CASE WHEN (gt.split_value < farray[gt.feature]) THEN
                                        3
                                     ELSE
                                        2
                                     END
                                    ], 0)
                ELSE 
                    COALESCE(gt.jump[farray[gt.feature] + 1],0) 
                END as newjump,
            gt.maxclass, gt.probability, gt.parent_id, gt.id 
            FROM (
                SELECT t1.id, t1.jump, % as farray  
                FROM % t1 
                LEFT JOIN % t2 
                ON t1.id = t2.id
            ) AS pt,
            (
                SELECT jump, maxclass,feature, probability, parent_id, id, is_feature_cont, split_value
                FROM % 
                WHERE array_upper(tree_location,1) = %
            ) AS gt
            WHERE pt.jump = gt.id;',
            ARRAY[
                table_names[table_pick],
                MADLIB_SCHEMA.__get_feature_name_list(metatable_name),
                table_names[(table_pick) % 2 + 1],
                encode_table_name,
                tree_table_name,
                MADLIB_SCHEMA.__to_char(curr_level)
            ]
            )
        INTO curstmt;     
        EXECUTE curstmt;
         
    END LOOP;

    EXECUTE 'INSERT INTO '||result_table_name||' SELECT * FROM '|| 
        table_names[table_pick] ||' WHERE jump = 0;';
    EXECUTE 'INSERT INTO '||result_table_name||' SELECT * FROM '|| 
        table_names[table_pick % 2 + 1] ||' WHERE jump = 0;';
    
    IF(verbosity) THEN  
        RAISE INFO 'final classification time:%', clock_timestamp() - time_stamp;
    END IF;
    
    RETURN encode_table_name;
END
$$ LANGUAGE PLPGSQL;
   
    
/*
 * @brief Classify data points using trained decision tree model
 *
 * @param classification_table_name Name of the table/view with the source data
 * @param tree_table_name Name of trained tree
 * @param result_table_name Name of result table
 * @param verbosity If set to 't' will use verbose mode
 *
 * @return Table MADLIB_SCHEMA.classified_points:
 *  - <tt>id INT</tt> - Point id.
 *  - <tt>feature MADLIB_SCHEMA.SVEC</tt> - Point feature vector. 
 *  - <tt>jump INT</tt> - Intermediate value used to distinguish leaf nodes.
 *  - <tt>class INT</tt> - Class prediction. 
 *  - <tt>prob FLOAT</tt> - Probability of the predicted class.
 *
 * @note This function starts an iterative algorithm. It is not an aggregate
 *       function. Source and column names have to be passed as strings (due to
 *       limitations of the SQL syntax).
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.c45_classify
    (
    tree_table_name             TEXT, 
    classification_table_name   TEXT, 
    result_table_name           TEXT, 
    verbosity                   BOOLEAN
    );
    
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_classify
    (
    tree_table_name             TEXT, 
    classification_table_name   TEXT, 
    result_table_name           TEXT, 
    verbosity                   BOOLEAN
    ) 
RETURNS MADLIB_SCHEMA.c45_classify_result AS $$
DECLARE
    encode_table_name TEXT := '';
    begin_time  TIMESTAMP;
    ret MADLIB_SCHEMA.c45_classify_result;
BEGIN
    begin_time = clock_timestamp();
    
    SELECT MADLIB_SCHEMA.__c45_classify_internal
        (
        classification_table_name, 
        tree_table_name, 
        result_table_name, 
        'f',
        verbosity
        )  INTO encode_table_name;
    
    EXECUTE 'ALTER TABLE '||result_table_name||' DROP COLUMN jump;';
    EXECUTE 'ALTER TABLE '||result_table_name||' DROP COLUMN parent_id;';
    EXECUTE 'ALTER TABLE '||result_table_name||' DROP COLUMN leaf_id;';
    EXECUTE 'DROP TABLE IF EXISTS ' || encode_table_name || ';';
    EXECUTE 'SELECT COUNT(*) FROM ' ||classification_table_name||';' INTO ret.input_set_size;
    
    ret.cost_time = clock_timestamp() - begin_time;
    
    RETURN ret;
END
$$ LANGUAGE PLPGSQL;

/*
 * @brief   Classify the data with no verbosity
 *  
 * @param classification_table_name Name of the table/view with the source data
 * @param tree_table_name Name of trained tree
 * @param result_table_name Name of result table
 *
 * @return Table MADLIB_SCHEMA.classified_points:
 *  - <tt>id INT</tt> - Point id.
 *  - <tt>feature MADLIB_SCHEMA.SVEC</tt> - Point feature vector. 
 *  - <tt>jump INT</tt> - Intermediate value used to distinguish leaf nodes.
 *  - <tt>class INT</tt> - Class prediction. 
 *  - <tt>prob FLOAT</tt> - Probability of the predicted class.
 *
 * @note This function starts an iterative algorithm. It is not an aggregate
 *       function. Source and column names have to be passed as strings (due to
 *       limitations of the SQL syntax).   
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.c45_classify
    (
    tree_table_name             TEXT, 
    classification_table_name   TEXT, 
    result_table_name           TEXT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_classify
    (
    tree_table_name             TEXT, 
    classification_table_name   TEXT, 
    result_table_name           TEXT
    ) 
RETURNS MADLIB_SCHEMA.c45_classify_result AS $$
DECLARE
    ret MADLIB_SCHEMA.c45_classify_result;
BEGIN
	ret = MADLIB_SCHEMA.c45_classify
	       (
	       tree_table_name,
	       classification_table_name, 
	       result_table_name,
	       'f'
	       );
	       
    RETURN ret;
END $$ LANGUAGE PLPGSQL;

/*
 * @brief   Check the accuracy of the decision tree alogrithom 
 * 
 * @param tree_table_name Name of trained tree
 * @param classification_table_name Name of the table/view with the source data
 * @param verbosity If set to 't' will use verbose mode 
 * @return The estimated accuracy information.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.c45_score
    (
    tree_table_name             TEXT, 
    classification_table_name   TEXT, 
    verbosity                   BOOLEAN
    );
    
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_score
    (
    tree_table_name             TEXT, 
    classification_table_name   TEXT, 
    verbosity                   BOOLEAN
    ) 
RETURNS FLOAT AS $$
DECLARE
    
    result_table_name TEXT = 'c45_score_table_temp';
    
    id_col_name TEXT := 'id';
    class_col_name TEXT := 'class';
    curstmt TEXT := '';
    
    num_of_row FLOAT := 0.0;
    mis_of_row FLOAT := 0.0;
    
    encode_table_name TEXT := '';
BEGIN
    
    result_table_name = MADLIB_SCHEMA.__get_schema_name(tree_table_name) || 
                        MADLIB_SCHEMA.__strip_schema_name(tree_table_name) || 
                        result_table_name;
    
    SELECT MADLIB_SCHEMA.__c45_classify_internal
        (
        classification_table_name, 
        tree_table_name, 
        result_table_name, 
        't',
        verbosity
        ) 
    INTO encode_table_name;
    

    SELECT MADLIB_SCHEMA.__format
        (
        'SELECT count(id) FROM %;',
        result_table_name
        ) 
    INTO curstmt;
    
    EXECUTE curstmt INTO num_of_row;
    
    SELECT MADLIB_SCHEMA.__format
        (
        'SELECT count(b.id) FROM % a, % b WHERE a.%=b.id and a.%<>b.class',
        encode_table_name,
        result_table_name,
        id_col_name,
        class_col_name
        ) 
    INTO curstmt;
     
    EXECUTE curstmt INTO mis_of_row;
     
    EXECUTE 'DROP TABLE IF EXISTS ' || encode_table_name || ';';
    
    RETURN (num_of_row - mis_of_row) / num_of_row;
    
END;
$$ LANGUAGE PLPGSQL;

/*
 * @brief Cleanup the trained tree table and any relevant tables
 *
 * @param result_tree_table_name Name of the table containing the tree's information
 *
 * @return BOOLEAN value indicating the status of that cleanup operation.
 *
 */
DROP FUNCTION IF EXISTS  MADLIB_SCHEMA.c45_clean
    (
    TEXT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_clean
    ( 
    result_tree_table_name TEXT
    ) 
RETURNS BOOLEAN AS $$
DECLARE
    metatable_name TEXT;
BEGIN
    PERFORM MADLIB_SCHEMA.__assert
            (
                MADLIB_SCHEMA.__get_schema_name(result_tree_table_name),
                MADLIB_SCHEMA.__strip_schema_name(result_tree_table_name),
                't'
            );   
                
    IF (MADLIB_SCHEMA.__table_exists('MADLIB_SCHEMA', 'training_info')) THEN
        metatable_name = MADLIB_SCHEMA.__get_metatable_name(result_tree_table_name);
        
        IF( metatable_name IS NOT NULL) THEN
            PERFORM MADLIB_SCHEMA.__drop_metatable(metatable_name);
            EXECUTE 'DROP TABLE IF EXISTS ' || 
                     MADLIB_SCHEMA.__get_encode_table_name(result_tree_table_name) || ';';
        END IF;
        
        EXECUTE 'DROP TABLE IF EXISTS ' || result_tree_table_name;
        PERFORM MADLIB_SCHEMA.__delete_traininginfo(result_tree_table_name);
    ELSE
        EXECUTE 'DROP TABLE IF EXISTS ' || result_tree_table_name;
    END IF;
    
    RETURN 't';    
END
$$ LANGUAGE PLPGSQL;
