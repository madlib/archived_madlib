/* ----------------------------------------------------------------------- *//** 
 *
 * @file decision_tree.sql_in
 *
 * @brief SQL implementation of a decision tree
 * @date January 2011
 *
 *//* ----------------------------------------------------------------------- */

/**
@addtogroup grp_dectree

@about

This module provides an implementation of the C4.5 decision tree algorithm. 
It assumes that:

- The data set is very large.
- It is sparse (that's why it uses sparse vector data type for storage).
- Data features can be discrete or continuous. 

Because we assume very large amount of sparse data as input the following 
additional steps have been implemented: 
- The algorithm starts by eliminating all redundant points, 
by producing a smaller subset of unique, weighted points.
- Further assuming a very large number of features at each step 
algorithm test only a subset of features (instead of all possible features) 
to find the best split criteria.

@input

We support two forms of input formats, namely tabular format and 
sparse vector format.

For sparse vector format, the <b>training data</b> is expected to be of 
the following form:
<pre>{TABLE|VIEW} <em>trainingSource</em> (
    ...
    <em>id</em> INTEGER,
    <em>features</em> SVEC,
    <em>class</em> INTEGER,
    ...
)</pre>

For sparse vector format, the <b>data to classify</b> is expected to be 
of the following form:
<pre>{TABLE|VIEW} <em>classifySource</em> (
    ...
    <em>id</em> INTEGER,
    <em>features</em> SVEC,
    ...
)</pre>

For tabular format, the <b>training data</b> is expected to be of 
the following form:
<pre>{TABLE|VIEW} <em>trainingSource</em> (
    ...
    <em>id</em> INTEGER,
    <em>feature1</em> ANYTYPE,
    <em>feature2</em> ANYTYPE,
    <em>feature3</em> ANYTYPE,
    ....................
    <em>featureN</em> ANYTYPE,
    <em>class</em> INTEGER,
    ...
)</pre>

For tabular format, the <b>data to classify</b> is expected to be 
of the following form:
<pre>{TABLE|VIEW} <em>classifySource</em> (
    ...
    <em>id</em> INTEGER,
    <em>feature1</em> ANYTYPE,
    <em>feature2</em> ANYTYPE,
    <em>feature3</em> ANYTYPE,
    ....................
    <em>featureN</em> ANYTYPE,
    ...
)</pre>

@usage

- Run the training algorithm on the source data:
  <pre>SELECT * FROM \ref c45_train(
    '<em>split_criterion_name</em>','<em>training_table_name</em>', 
    '<em>result_tree_table_name</em>', '<em>validation_table_name</em>',
    '<em>continuous_feature_names</em>','<em>is_sparse_vector_format</em>',
    '<em>feature_col_names</em>','<em>id_col_name</em>', '<em>class_col_name</em>',
    '<em>prune_confidence_level</em>','<em>max_num_iter</em>',
    '<em>max_tree_depth</em>','<em>min_percent_mode</em>',
    '<em>min_percent_split</em>');</pre>
  This will create the MADLIB_SCHEMA.tree table storing an abstract object 
  (representing the model) used for further classification. Column names:
  <pre>    
id  | tree_location |    hash    | feature |    probability    |       chisq        | ebp_coeff | maxclass |    split_gain     | live | cat_size | parent_id |    jump     | is_feature_cont | split_value
----+---------------+------------+---------+-------------------+--------------------+-----------+----------+-------------------+------+----------+-----------+-------------+-----------------+-------------
                                                     ...</pre>    
    
- Run the classification function using the learned model: 
  <pre>SELECT * FROM \ref c45_classify(
    '<em>tree_table_name</em>', '<em>classification_table_name</em>', 
    '<em>result_table_name</em>');</pre>
  This will create the result_table with the 
  classification results. 
  <pre> </pre> 

- Run the display tree function using the learned model: 
  <pre>SELECT * FROM \ref c45_display(
    '<em>tree_table_name</em>');</pre>
  This will display the trained tree in human readable format. 
  <pre> </pre> 

- Run the clean tree function as below: 
  <pre>SELECT * FROM \ref c45_clean(
    '<em>tree_table_name</em>');</pre>
  This will lear the learned model and all metadata.
  <pre> </pre> 

@examp

-# Prepare an input table/view, e.g.:
\verbatim
testdb=# select * from golf_data order by id;
 id | outlook  | temperature | humidity | windy  |    class    
----+----------+-------------+----------+--------+-------------
  1 | sunny    |          85 |       85 |  false | do not play
  2 | sunny    |          80 |       90 |  true  | do not play
  3 | overcast |          83 |       78 |  false |  Play
  4 | rain     |          70 |       96 |  false |  Play
  5 | rain     |          68 |       80 |  false |  Play
  6 | rain     |          65 |       70 |  true  | do not play
  7 | overcast |          64 |       65 |  true  |  Play
  8 | sunny    |          72 |       95 |  false | do not play
  9 | sunny    |          69 |       70 |  false |  Play
 10 | rain     |          75 |       80 |  false |  Play
 11 | sunny    |          75 |       70 |  true  |  Play
 12 | overcast |          72 |       90 |  true  |  Play
 13 | overcast |          81 |       75 |  false |  Play
 14 | rain     |          71 |       80 |  true  | do not play
(14 rows)
\endverbatim
-# Train the decision tree model, e.g.:
\verbatim
sql> SELECT * FROM MADLIB.c45_train('infogain','golf_data','madlib.trained_tree_infogain',null,'f',
    'temperature,humidity',null,'id','class', 100,3000,10,0.001,0.001);   
CONTEXT:  PL/pgSQL function "c45_train" line 70 at assignment
 training_set_size | tree_nodes | tree_depth |    cost_time    | split_criterion 
-------------------+------------+------------+-----------------+-----------------
                14 |          8 |          3 | 00:00:00.973429 | infogain
(1 row)

 
(1 row)
\endverbatim
-# Check few rows from the tree model table:
\verbatim
testdb=# select * from madlib.trained_tree_infogain order by id;
 id | tree_location |    hash    | feature |    probability    |       chisq        | ebp_coeff | maxclass |    split_gain     | live | cat_size | parent_id |     jump      | is_feature_cont | split_value 
----+---------------+------------+---------+-------------------+--------------------+-----------+----------+-------------------+------+----------+-----------+---------------+-----------------+-------------
  1 | {0}           |   12459841 |       3 | 0.642857142857143 | 0.0848830787199056 |       5.5 |        1 | 0.171033941880327 |    0 |       14 |         0 | [2:4]={2,3,4} | f               |           0
  2 | {0,1}         | 2106753344 |       3 |                 1 |           0.999999 |         0 |        1 |                 0 |    0 |        4 |         1 |               | f               |           0
  3 | {0,2}         | 2106753345 |       4 |               0.6 | 0.0129787778691654 |       2.5 |        1 | 0.673011667009257 |    0 |        5 |         1 | [2:3]={5,6}   | f               |           0
  4 | {0,3}         | 2106753346 |       2 |               0.6 |  0.106881508624868 |       2.5 |        2 | 0.673011667009257 |    0 |        5 |         1 | [2:3]={7,8}   | t               |        72.5
  5 | {0,2,1}       | -856744128 |       3 |                 1 |           0.999999 |         0 |        1 |                 0 |    0 |        3 |         3 |               | f               |           0
  6 | {0,2,2}       | -856744127 |       4 |                 1 |           0.999999 |         0 |        2 |                 0 |    0 |        2 |         3 |               | f               |           0
  7 | {0,3,1}       | -856678465 |       4 |                 1 |           0.999999 |         0 |        1 |                 0 |    0 |        2 |         4 |               | f               |           0
  8 | {0,3,2}       | -856678464 |       3 |                 1 |           0.999999 |         0 |        2 |                 0 |    0 |        3 |         4 |               | f               |           0
(8 rows)
\endverbatim
-# To display the tree with human readable format:
\verbatim
testdb=# select madlib.c45_display('madlib.trained_tree_infogain');
                                      c45_display                                      
---------------------------------------------------------------------------------------
         outlook:  = overcast : class( Play)   num_elements(4)  predict_prob(1)        
         outlook:  = rain : class( Play)   num_elements(5)  predict_prob(0.6)          
             windy:  =  false : class( Play)   num_elements(3)  predict_prob(1)        
             windy:  =  true : class(do not play)   num_elements(2)  predict_prob(1)   
         outlook:  = sunny : class(do not play)   num_elements(5)  predict_prob(0.6)   
             humidity:  <= 77.5 : class( Play)   num_elements(2)  predict_prob(1)      
             humidity:  > 77.5 : class(do not play)   num_elements(3)  predict_prob(1) 
 
(1 row)

\endverbatim
-# To classify data with the learned model:
\verbatim
testdb=# select * from  madlib.c45_classify
testdb-#     (
testdb(#     'golf_data', 
testdb(#     'madlib.trained_tree_infogain', 
testdb(#     'madlib.classification_result'); 
PL/pgSQL function "c45_classify" line 2 at perform
 c45_classify 
--------------
 
(1 row)
\endverbatim
-# Check classification results: 
\verbatim
testdb=# select t.id,t.outlook,t.temperature,t.humidity,t.windy,c.class from 
    madlib.classification_result c,golf_data t where t.id=c.id order by id;
 id | outlook  | temperature | humidity | windy  | class 
----+----------+-------------+----------+--------+-------
  1 | sunny    |          85 |       85 |  false |     2
  2 | sunny    |          80 |       90 |  true  |     2
  3 | overcast |          83 |       78 |  false |     1
  4 | rain     |          70 |       96 |  false |     1
  5 | rain     |          68 |       80 |  false |     1
  6 | rain     |          65 |       70 |  true  |     2
  7 | overcast |          64 |       65 |  true  |     1
  8 | sunny    |          72 |       95 |  false |     2
  9 | sunny    |          69 |       70 |  false |     1
 10 | rain     |          75 |       80 |  false |     1
 11 | sunny    |          75 |       70 |  true  |     1
 12 | overcast |          72 |       90 |  true  |     1
 13 | overcast |          81 |       75 |  false |     1
 14 | rain     |          71 |       80 |  true  |     2
(14 rows)
(notes: The class value of 2 refers to 'do not play'. The class value of 1
refers to 'Play'. We plan to add a view to translate the numeric value to original
value soon.
\endverbatim

-# clean up the tree and metadata: 
\verbatim
testdb=# select madlib.c45_clean('madlib.trained_tree_infogain');
 c45_clean 
-----------
 
(1 row)
\endverbatim

@literature

[1] http://en.wikipedia.org/wiki/C4.5_algorithm

@sa File decision_tree.sql_in documenting the SQL functions.
*/

/*
 * This function computes one hash value from one
 * integer array. 
 *
 * Parameters:
 *      first para: The integer array.
 * Return:
 *      The integer hash value.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__hash_array(INT4[]);
CREATE FUNCTION MADLIB_SCHEMA.__hash_array(INT4[]) RETURNS INT4 
AS 'MODULE_PATHNAME', 'hash_array'
LANGUAGE C IMMUTABLE;


/*
 * This function accumulate the distribution information in one big array.
 * The distribution information is used to calculate split gains.
 *
 * Parameters:
 *      1 vals_state:         The array used to store the accumulated information.
 *      2 colValArray:        The array containing the values of various features
 *                            for current record.
 *      3 colValCntArray:     The array containing the number of distinct values
 *                            for each feature.
 *      4 contSplitValArray:  The array containing all the candidate split values
 *                            for each continuous feature.
 *      5 trueweight:         The weight of current record. (We represent multiple
 *                            duplicate records with just one row. We use 'weight' to
 *                            denote how many records there are originally. 
 *      6 numOfClasses:       The total number of distinct classes.   
 *      7 trueclass:          The true class this record belongs to.
 *      8 has_cont_feature:   For all those features, is one of them is continuous.
 * Return:
 *      The array containing all the information for the calculation of 
 *      split gain. 
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__aggr_infogain    
    (
    FLOAT8[], 
    FLOAT8[],
    INT4[], 
    FLOAT8[], 
    FLOAT8, 
    INT4,
    INT4, 
    boolean
    ); 

CREATE FUNCTION MADLIB_SCHEMA.__aggr_infogain
    (
    FLOAT8[], 
    FLOAT8[],
    INT4[], 
    FLOAT8[], 
    FLOAT8, 
    INT4,
    INT4, 
    boolean
    ) 
RETURNS FLOAT8[] 
AS 'MODULE_PATHNAME', 'aggr_infogain'
LANGUAGE C IMMUTABLE;


/*
 * This function acts as the entry for accumulating all the information
 * used for Reduced-Error Pruning to an array.
 *
 * Parameters:
 *      1 vals_state:         The array used to store the accumulated information.
 *      2 classifiedClass:    The classified class based on our trained DT model.
 *      3 originalClass:      The true class value provided in the validation set. 
 * Return:
 *      The array containing all the information for the calculation of 
 *      Reduced-Error pruning. 
 *      The first element is the number of wrongly classified cases.
 *      The following elements are the true number of cases for each class.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__aggr_rep(BIGINT[], INT4, INT4);
CREATE FUNCTION MADLIB_SCHEMA.__aggr_rep(BIGINT[], INT4, INT4) RETURNS BIGINT[] 
AS 'MODULE_PATHNAME', 'aggr_rep'
LANGUAGE C IMMUTABLE;


/*
 * This function acts as the entry for the calculation of the 
 * Reduced-Error Pruning based on the accumulated information.
 *
 * Parameters:
 *      1 vals_state:         The array containing all the information for the 
 *                            calculation of Reduced-Error pruning. 
 * Return:
 *      The array containing  the calculation result of 
 *      Reduced-Error pruning. 
 *      The first element is the id of the class, which has the most cases.
 *      The following element reflect the change of mis-classified cases
 *      if the current branch is pruned. 
 *      If that element is greater than zero, we will consider pruning that 
 *      branch
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__compute_rep(BIGINT[]);
CREATE FUNCTION MADLIB_SCHEMA.__compute_rep(BIGINT[]) RETURNS BIGINT[] 
AS 'MODULE_PATHNAME', 'compute_rep'
LANGUAGE C IMMUTABLE;


/*
 *  This function is the entry point for computing the split gain.
 *
 * Parameters:
 *      1 vals_state:         The array used to store the accumulated information.
 *      2 colValCntArray:     The array containing the number of distinct values
 *                            for each feature.
 *      3 contSplitValArray:  The array containing all the candidate split values
 *                            for each continuous feature.
 *      4 numOfClasses:       The total number of distinct classes.   
 *      5 confLevel:          This parameter is used by the 'Error-Based Pruning'.
 *                            Please refer to the paper for detailed definition.
 *                            The paper's name is 'Error-Based Pruning of Decision  
 *                            Trees Grown on Very Large Data Sets Can Work!'.
 *      6 split_criterion:    It defines the split criterion to be used.
 *                            (1- information gain. 2- gain ratio. 3- gini)
 *      7 has_cont_feature:   It specifies whether there exists one continuous 
 *                            feature.
 * Return:
 *      The array containing all the information for the calculation of 
 *      split gain. 
 *      result[0]:      max split gain.
 *      result[1]:      chisquare.
 *      result[2]:      count(records belonging to maxClass)/count(all records)
 *      result[3]:      the Id of the class containing most records.
 *      result[4]:      total error for error-based pruning.
 *      result[5]:      the index of the selected feature.
 *      result[6]:      The number of distinct values for the selected feature.
 *      result[7]:      If the selected feature is continuous, it specifies
 *                      the split value. Otherwise, it is of no use.
 *      result[8]:      It defines whether the selected feature is continuous.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__compute_infogain
    (
    FLOAT8[], 
    INT4[], 
    FLOAT8[],
    INT4, 
    FLOAT8,
    INT4,
    BOOLEAN
    );
CREATE FUNCTION MADLIB_SCHEMA.__compute_infogain
    (
    FLOAT8[], 
    INT4[], 
    FLOAT8[],
    INT4, 
    FLOAT8,
    INT4,
    BOOLEAN
    ) 
RETURNS FLOAT8[] 
AS 'MODULE_PATHNAME', 'compute_infogain'
LANGUAGE C IMMUTABLE;


/*
 * This function allocate float8 array.
 * Parameters:
 *       1 size:    It defines how many float8 values the
 *                  array should contain.
 *       2 value:   It defines the initial value. All the
 *                  elements of that array are initialized
 *                  with that value.
 * Return:
 *      The array requested by user.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__malloc_and_set(INT4, INT4);
CREATE FUNCTION MADLIB_SCHEMA.__malloc_and_set(INT4, INT4) RETURNS FLOAT8[] 
AS 'MODULE_PATHNAME', 'malloc_and_set'
LANGUAGE C IMMUTABLE;

/*
 * This function allocate int64 array.
 * Parameters:
 *       1 size:    It defines how many int64 values the
 *                  array should contain.
 *       2 value:   It defines the initial value. All the
 *                  elements of that array are initialized
 *                  with that value.
 * Return:
 *      The array requested by user.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__malloc_and_set_int64(INT4, INT4);
CREATE FUNCTION MADLIB_SCHEMA.__malloc_and_set_int64(INT4, INT4) RETURNS BIGINT[] 
AS 'MODULE_PATHNAME', 'malloc_and_set_int64'
LANGUAGE C IMMUTABLE;


/*
 * Randomly select number of 'value1' distinct values
 * Those distinct values should between 1 and 'value2'.
 *
 * Parameters:
 *       1 value1:    It defines how many distinct values 
 *                    we should generate.
 *       2 value2:    It defines the upper limit for those
 *                    requested values. It should be greater 
 *                    than 1 since the lower limit is fixed
 *                    to 1 by default. 
 * Return:
 *      The array containing all those values 
 *      requested by user.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__weighted_no_replacement(INT4, INT4);
CREATE FUNCTION MADLIB_SCHEMA.__weighted_no_replacement(INT4, INT4) RETURNS INT8[] 
AS 'MODULE_PATHNAME', 'weighted_no_replacement'
LANGUAGE C IMMUTABLE;


/*
 *  This function returns the min value of two float.
 * Parameters:
 *       0 x:    a float value
 *       1 y:    a float value
 *                  
 * Return:
 *       The value which is smaller than the other.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__min
    (
    double precision, 
    double precision
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__min 
    ( 
    x double precision,
    y double precision
    ) RETURNS double precision
AS 'MODULE_PATHNAME', 'min'
LANGUAGE C IMMUTABLE;


/*
 * This function judges whether a float value is less than the other
 *
 * Parameters:
 *       0 x:    a float value
 *       1 y:    a float value
 *                  
 * Return:
 *       0- If x<y; 1- If x>=y.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__is_less( double precision, double precision);
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__is_less ( 
  x double precision,
  y double precision
) RETURNS INT
AS 'MODULE_PATHNAME', 'is_less'
LANGUAGE C IMMUTABLE;


/*
 * This structure is used to store the result for the function of c45_train.
 *      training_set_size: It means how many records there exists in 
 *                         the training set.
 *      tree_nodes:        It is the number of total tree nodes.
 *      tree_depth:        It is the depth of the trained tree.
 *      cost_time:         It is the time consumed during training the tree.
 *      split_criterion:   It is the split criterion used to train the tree.
 */
DROP TYPE IF EXISTS MADLIB_SCHEMA.c45_train_result;
CREATE TYPE MADLIB_SCHEMA.c45_train_result AS 
    (   
    training_set_size        BIGINT,   
    tree_nodes               BIGINT,
    tree_depth               INT,
    cost_time                INTERVAL,
    split_criterion          TEXT
    );


/*
 * This structure is used to store the result for the function of c45_classify.
 *      input_set_size:    It means how many records there exists in 
 *                         the classification set.
 *      cost_time:         It is the time consumed during classifying the tree.
 */
DROP TYPE IF EXISTS MADLIB_SCHEMA.c45_classify_result;
CREATE TYPE MADLIB_SCHEMA.c45_classify_result AS 
    (   
    input_set_size        BIGINT,   
    cost_time             INTERVAL
    );


/*
 * This structure is used in the aggregation to remove redundant records.
 *      id:                 The ID of this record passed in.
 *      feature:            The sparse vector containing the value for this record.
 *      class:              The class that record belongs to.
 *      weight:             The count of all records whose value is the same as the
 *                          value stored in 'feature' field.
 *      selection:          This field is not used while removing redundant records.
 *                          It is used to train a decision tree.
 */
DROP TYPE IF EXISTS MADLIB_SCHEMA.__hash_val CASCADE;
CREATE TYPE MADLIB_SCHEMA.__hash_val AS
    (
	id          INTEGER,
	feature     MADLIB_SCHEMA.svec,
	class       INTEGER,
	weight      INTEGER,
	selection   INTEGER 
    );

DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__weight_count
    (
    MADLIB_SCHEMA.__hash_val, 
    int, 
    MADLIB_SCHEMA.svec, 
    int
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__weight_count(
        hash_value    MADLIB_SCHEMA.__hash_val, 
        id            int, 
        feature       MADLIB_SCHEMA.svec, 
        class         int) 
RETURNS MADLIB_SCHEMA.__hash_val AS $$
declare
begin

IF (hash_value.weight IS NOT NULL) THEN
	hash_value.weight = hash_value.weight + 1;
ELSE
	$1.weight = 1;
	$1.feature = feature;
	$1.id = id;
	$1.class = class;
	$1.selection = 1;
END IF;

RETURN $1;
end
$$ LANGUAGE plpgsql;


DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__weight_aggr(
        MADLIB_SCHEMA.__hash_val, 
        MADLIB_SCHEMA.__hash_val) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__weight_aggr(
        hash_val1   MADLIB_SCHEMA.__hash_val, 
        hash_val2   MADLIB_SCHEMA.__hash_val) 
RETURNS MADLIB_SCHEMA.__hash_val AS $$
declare
begin
    IF (hash_val2.id IS NOT NULL) THEN
    	hash_val1.id = hash_val2.id;
    	hash_val1.feature = hash_val2.feature;
    	hash_val1.class = hash_val2.class;
    	hash_val1.weight = COALESCE(hash_val1.weight, 0) 
                + COALESCE(hash_val2.weight, 0);
    	hash_val1.selection = 1;
    END IF;
RETURN $1;
end
$$ LANGUAGE plpgsql;


DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.__create_hash
    (
    int, 
    MADLIB_SCHEMA.svec, 
    int
    );
CREATE AGGREGATE MADLIB_SCHEMA.__create_hash
    (
    int, 
    MADLIB_SCHEMA.svec, 
    int
    ) 
(
  SFUNC=MADLIB_SCHEMA.__weight_count,
  m4_ifdef(`GREENPLUM',`prefunc=MADLIB_SCHEMA.__weight_aggr,')
  STYPE=MADLIB_SCHEMA.__hash_val
);


/*
 * Returns the Gamma probability density function
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.gampdf
    ( 
    X double precision,
    A double precision,
    B double precision
    ) 
RETURNS double precision
AS 'MODULE_PATHNAME', 'gampdf'
LANGUAGE C IMMUTABLE;


/*
 * Returns Chi-square probability density function
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.chi2pdf 
    ( 
    X double precision,
    V double precision
    ) 
RETURNS double precision
AS $$
DECLARE
BEGIN
	IF (X <= 0) THEN
  		RETURN .999999;
  	END IF;
  RETURN MADLIB_SCHEMA.gampdf(X, V/2.0, 2.0);
END;
$$ LANGUAGE 'plpgsql';


/**
 * remove any redundant data from the training set
 * Parameters:
 *      table_input:    The name of the original table containing all
 *                      the records in training set.
 *      id_col_name:    The name of the column used as 'ID' column to
 *                      uniquely identify one record.
 *      id_feature_name:The name of the column used to store the sparse vector,
 *                      which contains all the values of that record.
 *      class_col_name: The name of the column used to store the class
 *                      that record belongs to.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__remove_set_redundant
    (
    table_input     TEXT, 
    id_col_name     TEXT, 
    id_feature_name TEXT, 
    class_col_name  TEXT,
    out_table_name  TEXT
    );
    
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__remove_set_redundant
    (
    table_input     TEXT, 
    id_col_name     TEXT, 
    id_feature_name TEXT, 
    class_col_name  TEXT,
    out_table_name  TEXT
    )
RETURNS void AS $$
DECLARE
    curStmt TEXT := '';
BEGIN
	
	SELECT MADLIB_SCHEMA.__format
	   (
	   'INSERT INTO % SELECT (MADLIB_SCHEMA.__create_hash(id, feature, class)).* 
	   FROM (SELECT % as id, % as feature, % as class, MADLIB_SCHEMA.svec_hash(%) as hash FROM %) as A 
	   GROUP BY A.hash,A.class;',
	   ARRAY[
	   out_table_name,
	   id_col_name,
	   id_feature_name,
	   class_col_name,
	   id_feature_name,
	   table_input
	   ]
	   )
	   INTO curStmt;

    EXECUTE curStmt;
END
$$ language plpgsql;


DROP TYPE IF EXISTS MADLIB_SCHEMA.__rep_type CASCADE;
CREATE TYPE MADLIB_SCHEMA.__rep_type AS(
  numOfOrgClasses BIGINT[]
);


DROP TYPE IF EXISTS MADLIB_SCHEMA.__rep_result CASCADE;
CREATE TYPE MADLIB_SCHEMA.__rep_result AS(
  maxClass  INT,
  isReplace INT
);


DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__rep_sfunc(MADLIB_SCHEMA.__rep_type, INT, INT, INT) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__rep_sfunc
    (
    result          MADLIB_SCHEMA.__rep_type,     
    classifiedClass INT,
    originalClass   INT,
    maxNumOfClasses INT
    ) 
RETURNS MADLIB_SCHEMA.__rep_type AS $$
begin
    IF(result.numOfOrgClasses IS NULL) THEN
        result.numOfOrgClasses = MADLIB_SCHEMA.__malloc_and_set_int64(maxNumOfClasses + 1,0);
    END IF;
    -- value, feature, weight, num_class, num_values, class
    result.numOfOrgClasses = MADLIB_SCHEMA.__aggr_rep(result.numOfOrgClasses, classifiedClass, originalClass); 
    RETURN result;
end
$$ LANGUAGE plpgsql;


DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__rep_prefunc
    (
    MADLIB_SCHEMA.__rep_type, 
    MADLIB_SCHEMA.__rep_type
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__rep_prefunc(MADLIB_SCHEMA.__rep_type, 
    MADLIB_SCHEMA.__rep_type) 
RETURNS MADLIB_SCHEMA.__rep_type AS $$
begin
    IF(($1.numOfOrgClasses IS NOT NULL) AND ($2.numOfOrgClasses IS NOT NULL)) THEN
        $1.numOfOrgClasses = MADLIB_SCHEMA.array_add($1.numOfOrgClasses, $2.numOfOrgClasses);
        RETURN $1;
    END IF;
    
    IF($1.numOfOrgClasses IS NOT NULL) THEN
        RETURN $1;
    ELSE
        RETURN $2;
    END IF;
end
$$ LANGUAGE plpgsql;


DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__rep_finalfunc
    (
    MADLIB_SCHEMA.__rep_type
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__rep_finalfunc(MADLIB_SCHEMA.__rep_type) 
RETURNS MADLIB_SCHEMA.__rep_result AS $$
declare
    temp BIGINT[];
    result MADLIB_SCHEMA.__rep_result;
begin
    IF($1.numOfOrgClasses IS NOT NULL) THEN
       temp = MADLIB_SCHEMA.__compute_rep($1.numOfOrgClasses);
       result.maxClass = temp[1];
       result.isReplace = temp[2];        
    ELSE
       result.maxClass = -1;
       result.isReplace = -1;
    END IF;
    return result;
end
$$ LANGUAGE plpgsql;


DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.__rep_compute_errors(INT, INT, INT);
CREATE AGGREGATE MADLIB_SCHEMA.__rep_compute_errors
    (
    INT,
    INT,
    INT
    ) 
(
  SFUNC=MADLIB_SCHEMA.__rep_sfunc,
  m4_ifdef(`GREENPLUM',`prefunc=MADLIB_SCHEMA.__rep_prefunc,')
  FINALFUNC=MADLIB_SCHEMA.__rep_finalfunc,
  STYPE=MADLIB_SCHEMA.__rep_type
);


/*
 * This type is used to accumulate the distribution information in one big array.
 * The distribution information is used to calculate split gains.
 * Must return Info Gain, Gain Significance and Probability of main class
 *
 * Parameters:
 *      0 value:            The array used to store the accumulated information.
 *      1 featurevalCount:  The array containing the number of distinct values
 *                          for each feature.
 *      2 contSplitVals:    The array containing all the candidate split values
 *                          for each continuous feature.
 *      3 num_class:        The total number of distinct classes.   
 *      4 conflevel:        This parameter is used by the 'Error-Based Pruning'.
 *                          Please refer to the paper for detailed definition.
 *                          The paper's name is 'Error-Based Pruning of Decision  
 *                          Trees Grown on Very Large Data Sets Can Work!'.
 *      5 is_cont:          Whether the selected feature is continuous.
 *      6 sp_val:           For continuous feature, it is the split value. 
 *                          Otherwise, it is of no use.
 *      7 split_criterion:  It defines the split criterion to be used.
 *                          (1- information gain. 2- gain ratio. 3- gini)
 */
DROP TYPE IF EXISTS MADLIB_SCHEMA.__find_infogain_type CASCADE;
CREATE TYPE MADLIB_SCHEMA.__find_infogain_type AS
    (
    value             FLOAT[],
    featurevalCount   INT4[],
    contSplitVals     FLOAT[],
    num_class         INT,
    conflevel         FLOAT,
    is_cont           boolean,
    sp_val            FLOAT,
    sp_criterion      INT
    );


/*
 * This type is used to store the calculated information gain.
 *
 * Parameters:
 *      0 dim:              The ID of the selected feature.
 *      1 infoGain:         The information gain.
 *      2 gainSign:         The chisquare value used for chisquare pre-pruning.
 *      3 classProb:        The predicted probability of our chosen class.
 *      4 classID:          The ID of the class chosen by the algorithm
 *      5 relativeSize:     Total number of records used for that calculation.
 *      6 ebp_coeff:        total error for error-based pruning.
 *      7 is_cont_feature:  whether the selected feature is continuous.
 *      8 split_value:      If the selected feature is continuous, it specifies
 *                          the split value. Otherwise, it is of no use.
 *      9 distinct_features: The number of distinct values for the selected feature.
 */
DROP TYPE IF EXISTS MADLIB_SCHEMA.__find_infogain_result CASCADE;
CREATE TYPE MADLIB_SCHEMA.__find_infogain_result AS
    (
    dim           FLOAT,
    infoGain      FLOAT,
    gainSign      FLOAT,
    classProb     FLOAT,
    classID       FLOAT,
    relativeSize  FLOAT,
    totalSize     FLOAT,
    ebp_coeff     FLOAT,  
    is_cont_feature   boolean,
    split_value       FLOAT,
    distinct_features INT
    );


/*
 * This function executes on segments. It accumulate the distribution information
 * for certain segment only.
 *
 * Parameters:
 *      0 result:           It is used to store the distribution information.
 *      1 featureval:       The array containing the values of various features
 *                          for current record.
 *      2 featurevalCount:  The array containing the number of distinct values
 *                          for each feature.
 *      3 contSplitVals:    The array containing all the candidate split values
 *                          for each continuous feature.
 *      4 weight:           The weight of current record. (We represent multiple
 *                          duplicate records with just one row. We use 'weight' to
 *                          denote how many records there are originally. 
 *      5 numOfClasses:     The total number of distinct classes.   
 *      6 classVal:         The true class this record belongs to.
 *      7 conflevel:        This parameter is used by the 'Error-Based Pruning'.
 *                          Please refer to the paper for detailed definition.
 *                          The paper's name is 'Error-Based Pruning of Decision  
 *                          Trees Grown on Very Large Data Sets Can Work!'.
 *      8 is_cont:          whether the selected feature is continuous.
 *      9 sp_criterion:     It defines the split criterion to be used.
 *                          (1- information gain. 2- gain ratio. 3- gini)
 * Return:
 *      The accumulated distribution information.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__find_infogain_sfunc
    (
    result          MADLIB_SCHEMA.__find_infogain_type,     
    featureval      FLOAT[], 
    featurevalCount INT4[],
    contSplitVals   FLOAT[],
    weight          FLOAT, 
    numOfClasses    INT, 
    classVal        INT, 
    conflevel       FLOAT,
    is_cont         boolean,
    sp_criterion    INT
    );


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__find_infogain_sfunc
    (
    result          MADLIB_SCHEMA.__find_infogain_type,     
    featureval      FLOAT[], 
    featurevalCount INT4[],
    contSplitVals   FLOAT[],
    weight          FLOAT, 
    numOfClasses    INT, 
    classVal        INT, 
    conflevel       FLOAT,
    is_cont         boolean,
    sp_criterion    INT
    ) 
RETURNS MADLIB_SCHEMA.__find_infogain_type AS $$
DECLARE
    numOfVals INT := 0;
    i INT := 0;
    numOfContFeats INT := 0;
begin
	IF((result.num_class IS NULL) OR (result.num_class < 2)) THEN
	   FOR i IN 1..array_upper(featurevalCount, 1) LOOP
	       numOfVals = numOfVals + (featurevalCount[i] + 1) *(1 + numOfClasses);
	   END LOOP;
	   
	   numOfVals = numOfVals + 1;
	   
	   IF (is_cont) THEN
    	   numOfContFeats = contSplitVals[1]::INT4;
    	   FOR i IN 1..(numOfContFeats) LOOP
    	       numOfVals = numOfVals + (contSplitVals[i*2 + 1]::INT4) * (2 * (numOfClasses) + 2);
    	   END LOOP;
	   END IF;
	  
	   --RAISE INFO 'Total allocated memory: %', numOfVals;
	   
	   result.value = MADLIB_SCHEMA.__malloc_and_set(numOfVals,0);
	   result.featurevalCount = featurevalCount;
	   result.contSplitVals = contSplitVals;
	   result.num_class = numOfClasses;
	   result.is_cont = is_cont;
       result.sp_criterion = sp_criterion;
	   result.conflevel = conflevel;
	END IF;
	
	result.value = MADLIB_SCHEMA.__aggr_infogain(
            result.value, 
            featureval, 
            result.featurevalCount, 
            contSplitVals, 
            weight, 
            result.num_class, 
            classVal, 
            is_cont); 
	RETURN result;
end
$$ LANGUAGE plpgsql;


/*
 * This function aggregates the distribution information
 * for all segments together.
 *
 * Parameters:
 *      1st parameter: Current accumulated information in master.
 *      2nd parameter: The accumulated information for another segment.
 * Return:
 *      The accumulated distribution information.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__find_infogain_prefunc
    (
    MADLIB_SCHEMA.__find_infogain_type, 
    MADLIB_SCHEMA.__find_infogain_type
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__find_infogain_prefunc(
        MADLIB_SCHEMA.__find_infogain_type, 
        MADLIB_SCHEMA.__find_infogain_type) 
RETURNS MADLIB_SCHEMA.__find_infogain_type AS $$
begin
	IF(($1.num_class IS NOT NULL) AND ($2.num_class IS NOT NULL)) THEN
		$1.value = MADLIB_SCHEMA.array_add($1.value, $2.value);
		RETURN $1;
	END IF;
	
	IF($1.num_class IS NOT NULL) THEN
		RETURN $1;
	ELSE
		RETURN $2;
	END IF;
end
$$ LANGUAGE plpgsql;


/*
 * This function computes the information gain based 
 * on the aggregated distribution information for whole cluster.
 *
 * Parameters:
 *      1st parameter: accumulated distribution information for cluster.
 * Return:
 *      The result for split gain, chisq, etc. Please refer to the 
 *      definition of 'MADLIB_SCHEMA.__find_infogain_result'.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__find_infogain_finalfunc
    (
    MADLIB_SCHEMA.__find_infogain_type
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__find_infogain_finalfunc
    (
    MADLIB_SCHEMA.__find_infogain_type
    ) 
RETURNS MADLIB_SCHEMA.__find_infogain_result AS $$
declare
	temp FLOAT[];
	result MADLIB_SCHEMA.__find_infogain_result;
begin
	IF($1.value IS NOT NULL) THEN
		temp = MADLIB_SCHEMA.__compute_infogain(
                    $1.value, 
                    $1.featurevalCount, 
                    $1.contSplitVals, 
                    $1.num_class,
                    $1.conflevel, 
                    $1.sp_criterion, 
                    $1.is_cont);
            
		result.dim = temp[6]::int4;
		result.infoGain = temp[1];
		result.gainSign = temp[2];
		result.classProb = temp[3];
		result.classID = temp[4];
		result.relativeSize = $1.value[1];
		result.ebp_coeff = temp[5];
        result.is_cont_feature = temp[9]::INT4::BOOLEAN;
        result.split_value = temp[8];
        result.distinct_features = temp[7]::int4;
        result.totalSize = temp[10];
	ELSE
		result.dim = 0;
		result.infoGain = 0;
		result.gainSign = 1;
		result.classProb = 1;
		result.classID = 0;
		result.relativeSize = 0;
		result.ebp_coeff = 0.0;
        result.is_cont_feature = $1.is_cont;
        result.split_value = 0;
        result.distinct_features = 0;
        result.totalSize = 0;
	END IF;
	return result;
end
$$ LANGUAGE plpgsql;


DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.__find_infogain
    (
    FLOAT[], 
    INT4[],
    FLOAT[],
    FLOAT, 
    INT,   
    INT,    
    FLOAT,  
    boolean,
    INT     
    );
CREATE AGGREGATE MADLIB_SCHEMA.__find_infogain
    (
    FLOAT[],  -- featureval
    INT4[],
    FLOAT[],
    FLOAT,  -- weight
    INT,    -- numOfClasses
    INT,    -- class
    FLOAT,  -- conflevel
    boolean,-- is cont
    INT     -- sp_criterion
    ) 
(
  SFUNC=MADLIB_SCHEMA.__find_infogain_sfunc,
  m4_ifdef(`GREENPLUM',`prefunc=MADLIB_SCHEMA.__find_infogain_prefunc,')
  FINALFUNC=MADLIB_SCHEMA.__find_infogain_finalfunc,
  STYPE=MADLIB_SCHEMA.__find_infogain_type
);


/*
 * This type is used to store information for the calculated best split 
 *
 * Parameters:
 *      feature:              The ID of the selected feature.
 *      probability:          The predicted probability of our chosen class.
 *      maxclass:             The ID of the class chosen by the algorithm
 *      infoGain:             The information gain.
 *      live:                 1- For the chosen split, we should split further.
 *                            0- For the chosen split, we shouldn't split further.
 *      chisq:                The chisquare value used for chisquare pre-pruning.
 *      ebp_coeff:            total error for error-based pruning.
 *      is_cont_feature:      whether the selected feature is continuous.
 *      split_value:          If the selected feature is continuous, it specifies
 *                            the split value. Otherwise, it is of no use.
 *      distinct_features:    The number of distinct values for the selected feature.
 */
DROP TYPE IF EXISTS MADLIB_SCHEMA.__best_split_result CASCADE;
CREATE TYPE MADLIB_SCHEMA.__best_split_result AS
    (
    node_id             INT,
	feature             INT,
	probability         FLOAT,
	maxclass            INTEGER,
	infogain            FLOAT,
	live                INT,
	chisq               FLOAT,
	ebp_coeff           FLOAT,
    is_cont_feature     boolean,
    split_value         FLOAT,
    distinct_features   INT,
    totalSize           INT
    );


/**
 * This function find the best split and return the information.
 *
 * Parameters:
 *  feature_dimensions:     The total number of different features
 *  featureValCountStr:     A string in csv format. Each element is 
 *                          a numeric value equal to the count of 
 *                          distinct features for each feature.
 *  distinct_classes:       Total number of different classes.
 *  selection:              It specifies which part of records should 
 *                          be used to calculate the best split.
 *  sample_limit:           A upper limit for the total number of records
 *                          used to calculate the best split.
 *  table_name:             The name of the table containing the training
 *                          set.
 *  confLevel:              This parameter is used by the 'Error-Based Pruning'.
 *                          Please refer to the paper for detailed definition.
 *                          The paper's name is 'Error-Based Pruning of Decision  
 *                          Trees Grown on Very Large Data Sets Can Work!'.
 *  feature_table_name:     Is is the name of one internal table, which contains
 *                          meta data for each feature.
 *  sp_criterion:           It defines the split criterion to be used.
 *                          (1- information gain. 2- gain ratio. 3- gini)
 *  continue_gow:           It specifies whether we should still grow the tree
 *                          on the selected branch.
 *  enable_chisq_pruning:   It specifies whether we perform the chisq pre-pruning.
 * Return:
 *  The return is of the type of MADLIB_SCHEMA.__best_split_result, which contains the information
 *  for best split. Please refer to that structure for detailed definition. 
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__find_best_split
    (
    feature_dimensions      INT, 
    featureValCountStr      TEXT,
    distinct_classes        INT,  
    selection_begin         INT, 
    selection_cnt           INT,
    sample_limit            INT, 
    table_name              TEXT, 
    conflevel               FLOAT,
    feature_table_name      TEXT, 
    sp_criterion            INT, 
    continue_gow            INT, 
    enable_chisq_pruning    boolean
    );
    
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__find_best_split
    (
    feature_dimensions      INT, 
    featureValCountStr      TEXT,
    distinct_classes        INT,  
    selection_begin         INT, 
    selection_cnt           INT, 
    sample_limit            INT, 
    table_name              TEXT, 
    conflevel               FLOAT,
    feature_table_name      TEXT, 
    sp_criterion            INT, 
    continue_gow            INT, 
    enable_chisq_pruning    boolean
    ) 
RETURNS SETOF MADLIB_SCHEMA.__best_split_result AS $$
declare
	sample_dimensions INT;
	selected_dimensions INT[];
	tablesize INT;
	i INT;
	new_sample_limit FLOAT := sample_limit;
	pre_result MADLIB_SCHEMA.__find_infogain_result;
	result MADLIB_SCHEMA.__best_split_result;
	vdebug FLOAT[];
	hdebug INT;
	total_size INT;
	has_cont_text TEXT := 't';
	curStmt TEXT := '';
	resultRec RECORD;
	cont_feature_split_vals TEXT;
	beginExec TIMESTAMP;
    temp_float_val  FLOAT;
    temp TEXT;
    best_answer MADLIB_SCHEMA.__find_infogain_result;
begin	 
	--this computes how many dimensions need to samples to find one that is in 90th percentile with
	-- .999 probability.
	
    sample_dimensions = MADLIB_SCHEMA.__min(
                floor(-ln(1-(.999)^(1/CAST(feature_dimensions AS FLOAT)))*feature_dimensions),
                feature_dimensions);

	beginExec = clock_timestamp();
	cont_feature_split_vals = MADLIB_SCHEMA.__get_cont_feature_split_vals(
                table_name, 
                feature_table_name, 
                selection_cnt, 
                selection_begin);

	--RAISE INFO 'time of get cont feature split value: %', clock_timestamp() - beginExec;
	--RAISE INFO 'featureValCountStr:%', featureValCountStr;
	--RAISE INFO 'cont_feature_split_vals:%', cont_feature_split_vals;
	
	IF (cont_feature_split_vals = 'null') THEN
	   has_cont_text = 'f';
	END IF;

    SELECT MADLIB_SCHEMA.__format
        (
            'SELECT selection,
                MADLIB_SCHEMA.__find_infogain
                    (
                        wp.feature::FLOAT8[],
                        %,
                        MADLIB_SCHEMA.__get_cont_feature_selection_split(''%'', selection - %),
                        wp.weight, 
                        %, 
                        wp.class, 
                        %,
                        ''%'',
                        %
                    ) AS t 
                FROM 
                    % AS wp
                GROUP BY selection 
                ORDER BY selection;',
            ARRAY[featureValCountStr,
            cont_feature_split_vals,
            MADLIB_SCHEMA.__to_char(selection_begin - 1),
            MADLIB_SCHEMA.__to_char(distinct_classes),
            MADLIB_SCHEMA.__to_char(conflevel),
            has_cont_text,
            MADLIB_SCHEMA.__to_char(sp_criterion),
            table_name]
        )
        INTO curStmt;
        
    FOR resultRec IN EXECUTE (curStmt) LOOP
        result.node_id = resultRec.selection;
        best_answer = resultRec.t;
        result.feature = best_answer.dim;
        result.maxclass = best_answer.classID;
        result.probability = best_answer.classProb;
        result.infogain = best_answer.infoGain;
        result.totalSize = best_answer.totalSize;
        total_size = result.totalSize;
        
        IF ( best_answer.is_cont_feature ) THEN
            -- continuous features should use the real number of distinct
            -- values rather than 2.
            -- For discrete features, we already get the real number. No-op.
            EXECUTE 'select distinct_values from feature_attr_table where id =' 
                ||best_answer.dim||';' INTO best_answer.distinct_features;
        END IF;
        
        result.chisq = MADLIB_SCHEMA.chi2pdf(best_answer.gainSign, best_answer.distinct_features - 1);  
        
        IF (result.probability > 0.999999999 ) THEN
            result.live = 0;
        ELSIF ( enable_chisq_pruning ) THEN
            temp_float_val = best_answer.relativeSize-total_size;
            temp_float_val = (temp_float_val * temp_float_val)/total_size;
            IF (
                ((result.chisq < 0.5/sample_dimensions) OR 
                    (MADLIB_SCHEMA.chi2pdf(temp_float_val, 1) < .1))
                AND
                (result.infogain > .1/sample_dimensions)
               ) THEN
                result.live = 1;
            ELSE
                result.live = 0;
            END IF;
        ELSE
            result.live = 1;
        END IF;
        
        result.ebp_coeff = best_answer.ebp_coeff;
        
        result.split_value = best_answer.split_value;
        result.is_cont_feature = best_answer.is_cont_feature;
        result.distinct_features = best_answer.distinct_features;
        
        return next result;
    END LOOP;
    	
	RETURN;
end
$$ language plpgsql;


DROP FUNCTION IF EXISTS MADLIB_SCHEMA.jump_sfunc
    (
    INT[], 
    INT, 
    INT
    ) CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.jump_sfunc
    (
    INT[], 
    INT, 
    INT
    ) 
RETURNS INT[] AS $$
declare
	temp INT[];
begin
	temp = $1;
	temp[$2+1] = $3;
	RETURN temp;
end
$$ LANGUAGE plpgsql;


DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.JumpCalc
    (
    INT, 
    INT
    );
CREATE AGGREGATE MADLIB_SCHEMA.JumpCalc
    (
    INT, 
    INT
    ) 
(
  SFUNC=MADLIB_SCHEMA.jump_sfunc,
  STYPE=INT[]
);


/**
 *   For training one decision tree, we need some internal tables
 *   to store intermediate results. This function creates those
 *   tables. Moreover, this function also creates the tree table
 *   specified by user.
 *
 *   Parameters:
 *      result_tree_table_name: The name of the tree specified by user.      
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__create_tree_tables
    (
    TEXT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__create_tree_tables
    (
    result_tree_table_name TEXT
    ) 
RETURNS void AS $$ 
BEGIN
    -- The training algorithm starts by eliminating all redundant points, 
    -- by producing a smaller subset of unique, weighted points,
    -- which was stored by the two tables below.
    --  Columns:
    --      id:         It is used to uniquely identify one record.
    --      feature:    It is used to store the value of one unique
    --                  record.
    --      class:      The class of that record.
    --      weight:     The count of such a record.
    --      selection:  This field is not used while removing redundant records.
    --                  It is used to train a decision tree.

    
	DROP TABLE IF EXISTS weighted_instance CASCADE;
	CREATE TEMP TABLE weighted_instance(
		id          INTEGER,
		feature     MADLIB_SCHEMA.svec,
		class       INTEGER,
		weight      INTEGER,
		selection   INTEGER
	) m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (id)');
	
	CREATE TEMP TABLE weighted_instance1(
        id          INTEGER,
        feature     MADLIB_SCHEMA.svec,
        class       INTEGER,
        weight      INTEGER,
        selection   INTEGER
    ) m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (selection)');
    
    CREATE TEMP TABLE weighted_instance2(
        id          INTEGER,
        feature     MADLIB_SCHEMA.svec,
        class       INTEGER,
        weight      INTEGER,
        selection   INTEGER
    ) m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (selection)');

    -- The table below stores the meta data for each feature.
    --  Columns:
    --      id:                 It is used to uniquely identify one feature.
    --      is_continuous:      It specifies whether that feature is 
    --                          continuous or not.
    --      distinct_values:    It specifies the number of distinct values
    --                          for that feature.
	DROP TABLE IF EXISTS feature_attr_table CASCADE;
    CREATE TEMP TABLE feature_attr_table(
		id              INTEGER,
        is_continuous   boolean,
        distinct_values INT
	) m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (id)');
    
    -- The table below stores the decision tree information just constructed.
    -- It is an internal table, which contains some redundant nodes. 
    -- In the last step, we will remove those redundant nodes and move the
    -- useful records to the table specified by user.
    -- Columns:
    --      id:             Tree node id
    --      tree_location:  Set of values that lead to this branch. 
    --                      0 is the initial point (no value). But this path 
    --                      does not specify which feature was used
    --                      for the branching.
    --      hash:           Hash value of the path. For quick unique identification.
    --      feature:        Which element of the feature vector was used for 
    --                      branching at this node. Notice that this feature is not 
    --                      used in the current tree_location. It will be added 
    --                      in the next step.
    --      probability:    If forced to make a call for a dominant class 
    --                      at a given point this would be the confidence of the 
    --                      call (this is only an estimated value).
    --      chisq:          Chi-square value of the branching significance, 
    --                      used to determine termination of the branch.
    --      maxclass:       If forced to make a call for a dominant class 
    --                      at a given point this is the selected class.
    --      split_gain:     Information gain computed using entropy (at this 
    --                      node), also used to determine termination of the branch.
    --      live:           Indication that the branch is still growing. 1 means "live". 
    --      cat_size:       Number of data point at this node.
    --      parent_id:      Id of the parent branch.
    --      jump:           Location of children for each feature value. 
    --                      Result such as [2:3]={2,3}, should be read: 
    --                      jump['feature value'+1], so in this case there were no 
    --                      0-value points for this feature. For value 1 jump to 2; 
    --                      for value 2 jump to 3;
    --      is_feature_cont: It specifies whether the selected feature is a continuous feature.
    --      split_value:    For continuous feature, it specifies the split value. Otherwise, 
    --                      it is of no meaning and fixed to 0.    
    --
	DROP TABLE IF EXISTS tree_internal CASCADE;
	CREATE TEMP TABLE tree_internal(
		id              SERIAL,
		tree_location   INT[],
		hash            INT,
		feature         INT,
		probability     FLOAT,
		chisq           FLOAT,
		ebp_coeff       FLOAT,
		maxclass        INTEGER,
		split_gain      FLOAT,
		live            INT,
		cat_size        INT,
		parent_id       INT,
		jump            INT[],
        is_feature_cont boolean,
        split_value     FLOAT
	) m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (id)');

    -- The table below stores the final decision tree information.
    -- It is an the table specified by users. 
    -- Please refer the table above for detailed column definition.
	EXECUTE 'DROP TABLE IF EXISTS '||result_tree_table_name||' CASCADE;';
	EXECUTE 'CREATE TABLE '||result_tree_table_name||E'(
		id              SERIAL,
		tree_location   INT[],
		hash            INT,
		feature         INT,
		probability     FLOAT,
		chisq           FLOAT,
		ebp_coeff       FLOAT,
		maxclass        INTEGER,
		split_gain      FLOAT,
		live            INT,
		cat_size        INT,
		parent_id       INT,
		jump            INT[],
        is_feature_cont boolean,
        split_value     FLOAT    
	) m4_ifdef(\`GREENPLUM\',\`DISTRIBUTED BY (id)\');';


    -- These two auxiliary internal tables help to
    -- remove redundant tree nodes and move the useful
    -- node to the final tree table.
	DROP TABLE IF EXISTS auxiliary_tree_info CASCADE;
	CREATE TEMP TABLE auxiliary_tree_info(
		id          INT,
		new_id      INT,
		parent_id   INT
	) m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (id)');

	DROP TABLE IF EXISTS auxiliary_tree_info2 CASCADE;
	CREATE TEMP TABLE auxiliary_tree_info2(
		id          INT,
		new_id      INT,
		parent_id   INT
	) m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (id)');

END 
$$ LANGUAGE plpgsql;


/**
 * Prune the trained tree with "Reduced Error Pruning" algorithm
 *  
 * Parameters:
 *      tree_table_name:    The name of the table containing the tree.
 *      validation_table:   The name of the table containing validation set.
 *      maxNumOfClasses:    The count of different classes.
 */
DROP FUNCTION IF EXISTS  MADLIB_SCHEMA.__prune_tree_rep
    (
    tree_table_name     TEXT, 
    validation_table    TEXT, 
    maxNumOfClasses     INT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__prune_tree_rep
    (
    tree_table_name     TEXT, 
    validation_table    TEXT, 
    maxNumOfClasses     INT
    ) 
RETURNS void AS $$
declare
    numOfParentIds INTEGER;
    id_col_name TEXT := 'id';
    feature_col_name TEXT :='feature';
    classified_tree TEXT := validation_table || '_classified_temp';
    is_sparse_vector BOOLEAN := 'f';
    svec_name TEXT;
    meta_table_name TEXT := '';
    curStmt TEXT;
begin
    SELECT MADLIB_SCHEMA.__get_input_format(tree_table_name) INTO is_sparse_vector;
    
    SELECT MADLIB_SCHEMA.__c45_classify_internal
    (
        validation_table, 
        tree_table_name, 
        classified_tree, 
        is_sparse_vector,
        't'
    ) INTO svec_name;
                            
    LOOP
        DROP TABLE IF EXISTS selectedParentIds_rep;
        CREATE TEMP TABLE selectedParentIds_rep(parent_id BIGINT) DISTRIBUTED BY(parent_id);
       
        SELECT MADLIB_SCHEMA.__format
            (
                'INSERT INTO selectedParentIds_rep 
                SELECT parent_id 
                FROM 
                (
                    SELECT parent_id, 
                           MADLIB_SCHEMA.__rep_compute_errors
                                (c.class, s.class, % + 1) as g 
                    FROM % c, % s 
                    WHERE c.id=s.id 
                    GROUP BY parent_id
                ) t 
                WHERE (t.g).isreplace >= 0 AND 
                      t.parent_id IN 
                      (
                          Select parent_id FROM % 
                          WHERE parent_id NOT IN
                              (
                                  Select parent_id  
                                  FROM % 
                                  WHERE jump IS NOT NULL
                              )
                      );',
                  ARRAY[
                      MADLIB_SCHEMA.__to_char(maxNumOfClasses),
                      classified_tree,
                      svec_name,
                      tree_table_name,
                      tree_table_name
                  ]
              )
              INTO curStmt;
            
        EXECUTE curStmt;
                        
        EXECUTE 'SELECT parent_id FROM selectedParentIds_rep limit 1;' INTO numOfParentIds;
        IF (numOfParentIds is NULL)  THEN
            EXIT;
        END IF;
        
        SELECT MADLIB_SCHEMA.__format
            (
                'DELETE FROM % WHERE parent_id IN (SELECT parent_id FROM selectedParentIds_rep)',
                tree_table_name
            )
            INTO curStmt;
        
        EXECUTE curStmt;

        SELECT MADLIB_SCHEMA.__format
            (
                'UPDATE % set jump = NULL WHERE id IN (SELECT parent_id FROM selectedParentIds_rep)',
                tree_table_name
            )
            INTO curStmt;
        
        EXECUTE curStmt;
        
    END LOOP;
    
    EXECUTE 'DROP TABLE IF EXISTS ' || svec_name || ' CASCADE;';
end
$$ language plpgsql;


/**
 * Prune the trained tree with " Error based Pruning" algorithm
 *  
 * Parameters:
 *      tree_table_name:    The name of the table containing the tree.
 */
DROP FUNCTION IF EXISTS  MADLIB_SCHEMA.__prune_tree_ebp
    (
    tree_table_name TEXT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__prune_tree_ebp
    (
    tree_table_name TEXT
    ) 
RETURNS void AS $$
declare
    numOfParentIds INTEGER;
    curStmt TEXT;
begin

    LOOP
        DROP TABLE IF EXISTS selectedParentIds_ebp;
        CREATE TEMP TABLE selectedParentIds_ebp(parent_id BIGINT) DISTRIBUTED BY(parent_id);
        
        SELECT MADLIB_SCHEMA.__format
            (
                'INSERT INTO selectedParentIds_ebp 
                SELECT s.parent_id as parent_id 
                FROM  
                (
                    Select parent_id, sum(ebp_coeff) as ebp_coeff 
                    FROM 
                    (
                        Select parent_id, ebp_coeff 
                        FROM % 
                        WHERE parent_id NOT IN 
                            (
                            Select parent_id  FROM % WHERE jump IS NOT NULL
                            )
                    ) m 
                    GROUP BY m.parent_id
                 ) s 
                 LEFT JOIN  % p 
                    ON p.id = s.parent_id 
                 WHERE  p.ebp_coeff < s.ebp_coeff;',
                 tree_table_name,
                 tree_table_name,
                 tree_table_name
            )
            INTO curStmt;
         
        EXECUTE curStmt;
                 
        EXECUTE 'SELECT parent_id FROM selectedParentIds_ebp LIMIT 1;' INTO numOfParentIds;

        IF (numOfParentIds IS NULL)  THEN
            EXIT;
        END IF;
        
        SELECT MADLIB_SCHEMA.__format
            (
                'DELETE FROM % 
                WHERE parent_id IN 
                    (SELECT parent_id FROM selectedParentIds_ebp)',
                tree_table_name
            )
            INTO curStmt;
            
        EXECUTE curStmt;
        
        SELECT MADLIB_SCHEMA.__format
            (
                'UPDATE %  
                SET jump = NULL 
                WHERE id IN 
                    (SELECT parent_id FROM selectedParentIds_ebp)',
                tree_table_name
            )
            INTO curStmt;
            
        EXECUTE curStmt;
        
    END LOOP;
end
$$ language plpgsql;


/**
 * Generate the final trained tree
 *  
 * Parameters:
 *      result_tree_table_name:    The name of the table containing the tree.
 */
DROP FUNCTION IF EXISTS  MADLIB_SCHEMA.__generate_final_tree(TEXT);
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__generate_final_tree
    (
    result_tree_table_name TEXT
    ) 
RETURNS void AS $$
declare
    tree_size INTEGER;
begin
    TRUNCATE auxiliary_tree_info;
    TRUNCATE auxiliary_tree_info2;
    
    EXECUTE 'DELETE FROM tree_internal WHERE COALESCE(cat_size,0) = 0';
    
    
    EXECUTE 'SELECT count(*) FROM tree_internal' INTO tree_size;
    EXECUTE 'INSERT INTO auxiliary_tree_info (id, parent_id, new_id) SELECT id, 
            MAX(parent_id), ('||tree_size||'+1) - count(1) 
            OVER(ORDER BY id DESC ROWS UNBOUNDED PRECEDING) FROM tree_internal GROUP BY id';
    EXECUTE 'INSERT INTO auxiliary_tree_info2 (id, parent_id, new_id) 
            SELECT  g2.id,g.new_id,g2.new_id FROM auxiliary_tree_info g, 
            auxiliary_tree_info g2  WHERE g.id = g2.parent_id';
    
    TRUNCATE auxiliary_tree_info;
    EXECUTE 'TRUNCATE '||result_tree_table_name||';';
    
    
    EXECUTE 'INSERT INTO '|| result_tree_table_name||' SELECT n.new_id, 
            g.tree_location, g.hash, g.feature, g.probability, g.chisq, 
            g.ebp_coeff,g.maxclass, g.split_gain, g.live, g.cat_size, 
            n.parent_id, g.jump, g.is_feature_cont, g.split_value 
            FROM tree_internal g, auxiliary_tree_info2 n WHERE n.id = g.id';
    EXECUTE 'INSERT INTO '||result_tree_table_name
            ||' SELECT * FROM tree_internal WHERE id = 1';
    TRUNCATE tree_internal;
    EXECUTE 'INSERT INTO tree_internal (id, jump) SELECT parent_id, 
            MADLIB_SCHEMA.JumpCalc(tree_location[array_upper(tree_location,1)], id) FROM '
            ||result_tree_table_name||' GROUP BY parent_id';
    TRUNCATE auxiliary_tree_info2;
    EXECUTE 'UPDATE '||result_tree_table_name||' k 
            SET jump = g.jump FROM tree_internal g WHERE g.id = k.id';
    TRUNCATE tree_internal;
end
$$ language plpgsql;


/**
 * construct the feature attribute table based on training table
 *  
 * Parameters:
 *      feature_dimension:       The total count of different features.
 *      cont_dimensions:         An array specifying all continuous features.
 *      training_table:          The name of the table containing training set.
 *
 * Return:
 *      The feature attribute table. Please refer to its definition in the 
 *      function of '__create_tree_tables'.
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__construct_feature_attr_table
    (
    feature_dimension   INT, 
    cont_dimensions     INT[],
    training_table      TEXT
    ) 
RETURNS void AS $$
DECLARE
    is_feature_cont boolean[];
    index INT := 0;
begin   
    FOR index in 1..feature_dimension LOOP
        is_feature_cont[index] = 'f';
    END LOOP;

    if (cont_dimensions is not null) then
        index = array_lower( cont_dimensions, 1);

        WHILE( index <= array_upper( cont_dimensions, 1) ) LOOP
            is_feature_cont[ cont_dimensions[index] ] = 't';
            index = index +1;
        END LOOP;
    end if;

    index = array_lower( is_feature_cont, 1);

    WHILE( index <= array_upper( is_feature_cont, 1) ) LOOP
        IF( is_feature_cont[index] ) THEN
            EXECUTE 'insert into feature_attr_table(id,is_continuous) values('||index||E',\'t\');';
        ELSE
            EXECUTE 'insert into feature_attr_table(id,is_continuous) values('||index||E',\'f\');';
        END IF;
        index = index +1;
    END LOOP;

    EXECUTE 'update feature_attr_table set distinct_values = dt.distinct_feature_value from 
              (select dt2.dim as dim, count(dt2.value)::integer as distinct_feature_value from 
                (SELECT distinct s_dim.id as dim, MADLIB_SCHEMA.svec_proj(tb.feature, s_dim.id) as value 
                    FROM feature_attr_table s_dim,' || training_table || ' tb) 
                        as dt2 group by dt2.dim) as dt where dt.dim = feature_attr_table.id;';
end
$$ language plpgsql;

/**
 * This function trains a tree based on a training set in sparse vector format.
 *  
 * Parameters:
 *      split_criterion:            This parameter specifies which split criterion 
 *                                  should be used for tree construction and pruning. The  
 *                                  valid values are gain, gainratio and gini.
 *      training_table_name:        Name of the table/view with the source data
 *      result_tree_table_name:     The name of the table where the resulting DT will be stored.
 *      validation_table_name:      The validation table used for pruning tree. 
 *      cont_dimensions:            An array specifying the features whose values are continuous. 
 *      feature_col_name:           Name of the table column, which defines a feature.
 *      id_col_name:                Name of the column containing id of each point.
 *      class_col_name:             Name of the column containing correct class of each point.
 *      conflevel:                  A statistical confidence interval of the resubstitution error.
 *      max_num_iter:               Max number of branches to follow (e.g. 2000)
 *      max_tree_depth:             Maximum decision tree depth 
 *      min_percent_mode:           Specifies the minimum number of cases required in a child node
 *      min_percent_split:          specifies the minimum number of cases required in a node  
 *                                  in order for a further split to be possible.
 *      enable_chisq_pruning:       Specifies whether enable chisq pre-pruning.
 *      verbosity:                  If True (or 1) will run in verbose mode
 *
 * Return:
 *      One summary result for training tree. Please refer to the structure of
 *      'MADLIB_SCHEMA.c45_train_result' for detailed definition.
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__train_tree_svec
    (
    split_criterion         TEXT,
    training_table_name     TEXT, 
    result_tree_table_name  TEXT,
    validation_table_name   TEXT,
    cont_dimensions         INT[], 
    feature_col_name        TEXT, 
    id_col_name             TEXT, 
    class_col_name          TEXT, 
    conflevel               FLOAT, 
    max_num_iter            INT, 
    max_tree_depth          INT,
    min_percent_mode        FLOAT,
    min_percent_split       FLOAT,
    enable_chisq_pruning    boolean,
    verbosity               INT
    ) 
RETURNS MADLIB_SCHEMA.c45_train_result AS $$
DECLARE
    feature_dimension INT;
    dimensions FLOAT[];
    lifenodes INT;
    selection INT;
    sample_limit INT := 0;
    location INT[];
    parent_location INT[][];
    temp_location INT[];
    num_classes INT;
    max_iter INT = max_num_iter;
    max_depth INT := max_tree_depth;
    answer MADLIB_SCHEMA.__best_split_result;
    location_size INT;
    max_id INT;
    flip INT := 1;
    category_size FLOAT[];
    category_class INT;
    execBegin TIMESTAMP;
    findbesttime interval;
    begin_findbesttime TIMESTAMP;
    datatransfertime INTERVAL;
    begin_datatransfer TIMESTAMP;
    misc_size FLOAT;
    total_size FLOAT;
    sp_crit   INT := 1;
    curStmt TEXT := '';
    toCharBase TEXT := '999999999';
    grow_tree INT := 0;
    featureCountArrayStr TEXT := '';
    ret MADLIB_SCHEMA.c45_train_result;
    selection_visit BOOLEAN[];
    selIdx INT := 1;
    temp INT := 0;
    select_max_id INT := 0;
    table_names TEXT[] := '{weighted_instance1,weighted_instance2}';
    table_index INT := 1;
    best_time_cmp BOOLEAN := 'f';
begin   
    execBegin = clock_timestamp();
    findbesttime = execBegin - execBegin;

    PERFORM MADLIB_SCHEMA.assert(
            conflevel >= 0.001 OR conflevel <= 100.0, 
            'Confidence level value should be in [0.001, 100]'
            );
    PERFORM MADLIB_SCHEMA.assert(
            split_criterion = 'infogain' OR split_criterion = 'gainratio' OR split_criterion = 'gini',
            'split_criterion must be one of infogain, gainratio or gini'
            );
    ret.split_criterion = split_criterion;                 
    IF( split_criterion = 'infogain' ) THEN
        sp_crit = 1;
    ELSIF ( split_criterion = 'gainratio' ) THEN
        sp_crit = 2;
    ELSE  -- gini
        sp_crit = 3;
    END IF;

    PERFORM MADLIB_SCHEMA.__create_tree_tables(result_tree_table_name);
    
    
    EXECUTE 'SELECT count(id) FROM '|| training_table_name ||';' INTO total_size;
    IF(verbosity > 0) THEN
        RAISE INFO 'INPUT TABLE SIZE: %', total_size;
    END IF;
    ret.training_set_size = total_size;
    
    PERFORM MADLIB_SCHEMA.__remove_set_redundant(
            training_table_name, 
            id_col_name, 
            feature_col_name, 
            class_col_name,
            table_names[table_index]);
    
    EXECUTE 'SELECT count(selection) FROM ' || table_names[table_index] || ';' INTO misc_size;
    IF(verbosity > 0) THEN
        RAISE INFO 'TABLE SIZE AFTER COMPRESSION: %', misc_size;
    END IF;
    
    EXECUTE 'SELECT MADLIB_SCHEMA.svec_dimension(feature) FROM ' || table_names[table_index]  || ' LIMIT 1;' INTO feature_dimension;

    PERFORM MADLIB_SCHEMA.__construct_feature_attr_table(feature_dimension, cont_dimensions, table_names[table_index]); 
    featureCountArrayStr = MADLIB_SCHEMA.__get_feature_count_array('feature_attr_table');
    --RAISE INFO 'featureCountArrayStr:%', featureCountArrayStr;

    EXECUTE 'SELECT COUNT(DISTINCT class) FROM ' || table_names[table_index] || ';' INTO num_classes; 
    
    IF(verbosity > 0) THEN
        RAISE INFO 'NUMBER OF CLASSES IN THE TRAINING SET %', num_classes;
    END IF;
    
    IF(num_classes < 2)THEN
        RAISE EXCEPTION 'Number of classes cannot be less than 2';
    END IF;
    
    EXECUTE 'INSERT INTO tree_internal (tree_location, hash, feature, probability, chisq, maxclass, 
        split_gain, live, cat_size, parent_id) 
        VALUES(ARRAY[0], MADLIB_SCHEMA.__hash_array(ARRAY[0]), 0, 1, 1, 1, 1, 1, 0, 0)';
               
    location_size = 0;
    begin_datatransfer = clock_timestamp();
    LOOP
        
        EXECUTE 'SELECT COUNT(id) FROM tree_internal WHERE live = 1' INTO lifenodes;
        
        IF ((max_depth < 0) OR (lifenodes < 1)) THEN
            IF(verbosity > 0) THEN
                RAISE INFO 'EXIT: LIMIT tree depth: % OR LIMIT iteration: % OR NO NODES LEFT', max_depth, max_iter;
            END IF;
            
            EXIT;
        END IF;
        
        max_depth = max_depth - 1;
        IF(verbosity > 0) THEN
            RAISE INFO 'current level: %', max_tree_depth - max_depth;
        END IF;

        EXECUTE 'SELECT id FROM tree_internal WHERE live = 1 ORDER BY id LIMIT 1' INTO selection;
        EXECUTE 'SELECT id FROM tree_internal WHERE live = 1 ORDER BY id DESC LIMIT 1' INTO max_id;
        
        FOR selIdx IN 1..(max_id - selection + 1) LOOP
            selection_visit[selIdx] = 'f';
        END LOOP;
        
        select_max_id = max_id;
        
        TRUNCATE weighted_instance;
        EXECUTE 'INSERT INTO weighted_instance SELECT * FROM ' || table_names[table_index] || 
                  ' WHERE selection IS NOT NULL;';
                  
        begin_findbesttime = clock_timestamp();
        best_time_cmp = 't';
        
        FOR answer IN (SELECT * FROM MADLIB_SCHEMA.__find_best_split(feature_dimension, featureCountArrayStr, num_classes, 
                        selection, max_id - selection + 1, sample_limit, 'weighted_instance',conflevel,'feature_attr_table',
                        sp_crit,grow_tree,enable_chisq_pruning )) LOOP
            
            IF (0 = answer.feature) THEN
                CONTINUE;
            END IF;
            
            -- mark this node id was visited
            selection_visit[answer.node_id - selection + 1] = 't';
            
            IF ( answer.is_cont_feature ) THEN
                IF(verbosity > 0) THEN
                    RAISE INFO 'selected feature is continuous';
                    RAISE INFO 'answer:%', answer;  
                END IF;
                EXECUTE 'UPDATE tree_internal SET   feature = '||answer.feature||',
                                        probability = '|| answer.probability||',
                                        maxclass = '||answer.maxclass||',
                                        split_gain = '||answer.infogain||',
                                        ebp_coeff = '||answer.ebp_coeff||',
                                        cat_size = '||answer.totalSize|| E',
                                        live = 0,
                                        is_feature_cont = \'t\',
                                        split_value = '|| answer.split_value ||',
                                        chisq = '||answer.chisq||'                                      
                                WHERE id =' || answer.node_id || ';';
            ELSE
                IF(verbosity > 0) THEN
                    RAISE INFO 'selected feature is discrete';
                    RAISE INFO 'answer:%', answer;       
                END IF;     
                EXECUTE 'UPDATE tree_internal SET   feature = '||answer.feature||',
                                        probability = '|| answer.probability||',
                                        maxclass = '||answer.maxclass||',
                                        split_gain = '||answer.infogain||',
                                        ebp_coeff = '||answer.ebp_coeff||',
                                        cat_size = '||answer.totalSize|| E',
                                        live = 0,
                                        is_feature_cont = \'f\',
                                        split_value = '|| answer.split_value ||',
                                        chisq = '||answer.chisq||'                                      
                                WHERE id =' || answer.node_id || ';';
            END IF;   
            
            -- no need to grow tree with the attribute 
            -- if comes up the maximum number;
            -- if its percent is lower than minimum split value
            IF (answer.node_id >= max_num_iter) THEN
                max_iter = 0;
                CONTINUE;
            END IF;

            IF (answer.totalSize < min_percent_split * total_size) THEN
                CONTINUE;
            END IF;
                        
            -- grow the tree
            EXECUTE 'SELECT gt.tree_location FROM tree_internal gt WHERE gt.id =' || answer.node_id ||';' INTO location;
            
            IF (answer.live > 0 and answer.is_cont_feature = 'f') THEN --here insert live determination function 
                IF(verbosity > 0) THEN
                    RAISE INFO 'determine live for discrete';
                END IF;              
                FOR i IN 1..answer.distinct_features LOOP
                    temp_location = location;
                    temp_location[array_upper(temp_location,1)+1] = i;
                    EXECUTE 'INSERT INTO tree_internal (tree_location, hash, feature, probability, 
                        maxclass, split_gain, live, parent_id) 
                        VALUES(ARRAY['||array_to_string(temp_location, ',')||'], ' || 
                        MADLIB_SCHEMA.__hash_array(temp_location) || 
                        ', 0, 1, 1, 1, 1, '|| answer.node_id ||');';
                END LOOP;
                
                SELECT MADLIB_SCHEMA.__format
                    (
                    'INSERT INTO % 
                        SELECT id, feature, class, weight, MADLIB_SCHEMA.svec_proj(feature, %) + %
                        FROM %
                        WHERE selection = %;',
                    ARRAY[
                        table_names[table_index%2 + 1],
                        MADLIB_SCHEMA.__to_char(answer.feature),
                        MADLIB_SCHEMA.__to_char(select_max_id),
                        table_names[table_index],
                        MADLIB_SCHEMA.__to_char(answer.node_id)
                        ]
                    ) 
                    INTO curStmt;
                EXECUTE curStmt;
                
                select_max_id = select_max_id + answer.distinct_features;
                
            ELSIF (answer.live > 0 and answer.is_cont_feature = 't') THEN
                IF(verbosity > 0) THEN
                    RAISE INFO 'determine live for continuous';
                END IF;  
                FOR i IN 1..2 LOOP
                    temp_location = location;
                    temp_location[array_upper(temp_location,1)+1] = i;
                    EXECUTE 'INSERT INTO tree_internal (tree_location, hash, feature, probability, 
                        maxclass, split_gain, live, parent_id) 
                        VALUES(ARRAY['||array_to_string(temp_location, ',')||'], ' 
                        || MADLIB_SCHEMA.__hash_array(temp_location) || 
                        ', 0, 1, 1, 1, 1, '|| answer.node_id ||');';
                    
                END LOOP;

                SELECT MADLIB_SCHEMA.__format
                    (
                    'INSERT INTO % 
                        SELECT id, feature, class, weight, 
                                MADLIB_SCHEMA.__is_less(%, MADLIB_SCHEMA.svec_proj(feature, %)) + 1 + %
                        FROM %
                        WHERE selection = %;',
                    ARRAY[
                        table_names[table_index%2 + 1],
                        MADLIB_SCHEMA.__to_char(answer.split_value),
                        MADLIB_SCHEMA.__to_char(answer.feature),
                        MADLIB_SCHEMA.__to_char(select_max_id),
                        table_names[table_index],
                        MADLIB_SCHEMA.__to_char(answer.node_id)
                        ]
                    )
                    INTO curStmt;
                    
                EXECUTE curStmt;
                               
                select_max_id = select_max_id + 2;
                           
            ELSE
                -- answer.live = 0
                -- process the min_percent_mode
                IF (total_size * min_percent_mode > answer.totalSize) THEN
                    --RAISE INFO '%', 'min_percent_mode';
                    EXECUTE 'DELETE FROM tree_internal WHERE id = ' || answer.node_id || ';';
                END IF;
            END IF;                      
        END LOOP;
        
        -- Remove the nodes which contains no data
        FOR selIdx IN selection..max_id  LOOP
            IF (NOT selection_visit[selIdx - selection + 1]) THEN
                --RAISE NOTICE '%, %', 'unneccesary node!', selIdx;
                EXECUTE 'DELETE FROM tree_internal WHERE id = '|| selIdx ||';';
            END IF;
        END LOOP;
               
        EXECUTE 'TRUNCATE ' || table_names[table_index] || ';';
        
        table_index = table_index % 2 + 1;
        
        IF(verbosity > 0) THEN
            RAISE INFO 'computation time in this level:%',(clock_timestamp() - begin_findbesttime);
        END IF;
        IF (best_time_cmp) THEN
            findbesttime = findbesttime + (clock_timestamp() - begin_findbesttime);
            best_time_cmp = 'f';
        END IF;
    END LOOP;
    
    datatransfertime =  (clock_timestamp() - begin_datatransfer) - findbesttime;
    
    PERFORM MADLIB_SCHEMA.__generate_final_tree( result_tree_table_name );
    
    IF (conflevel < 100.0) THEN
       EXECUTE 'SELECT MADLIB_SCHEMA.__prune_tree_ebp(' || quote_literal(result_tree_table_name) || ');';
    END IF;
    
    IF (validation_table_name IS NOT NULL) THEN
       --RAISE INFO 'validation table name %', validation_table_name;
       PERFORM MADLIB_SCHEMA.__prune_tree_rep(result_tree_table_name, validation_table_name , num_classes);
    END IF;
    
    EXECUTE 'select count(*) from '||result_tree_table_name||';' into ret.tree_nodes;
    EXECUTE 'select max(array_upper(tree_location,1)) from '||result_tree_table_name||';' into ret.tree_depth;
    IF(verbosity > 0) THEN
        RAISE INFO 'total of find best time: %', findbesttime;
        RAISE INFO 'total of data transfer time: %', datatransfertime;
        RAISE INFO 'total of training time: %', clock_timestamp() - execBegin;        
    END IF;
    
    ret.cost_time = clock_timestamp() - execBegin;
    
    return ret;
end
$$ language plpgsql;


/**
 * This function trains a tree based on a training set in tabular format.
 * Internally, it transforms the training set to sparse vector format for
 * further processing.
 *  
 * Parameters:
 *      split_criterion:            This parameter specifies which split criterion 
 *                                  should be used for tree construction and pruning. The  
 *                                  valid values are gain, gainratio and gini.
 *      training_table_name:        Name of the table/view with the source data
 *      result_tree_table_name:     The name of the table where the resulting DT will be stored.
 *      validation_table_name:      The validation table used for pruning tree. 
 *      cont_feature_col_names:     An array specifying the features whose values are continuous. 
 *      feature_col_name:           Name of the table columns, which defines all those feature.
 *      id_col_name:                Name of the column containing id of each point.
 *      class_col_name:             Name of the column containing correct class of each point.
 *      conflevel:                  A statistical confidence interval of the resubstitution error.
 *      max_num_iter:               Max number of branches to follow (e.g. 2000)
 *      max_tree_depth:             Maximum decision tree depth 
 *      min_percent_mode:           Specifies the minimum number of cases required in a child node
 *      min_percent_split:          specifies the minimum number of cases required in a node  
 *                                  in order for a further split to be possible.
 *      enable_chisq_pruning:       Specifies whether enable chisq pre-pruning.
 *      verbosity:                  If True (or 1) will run in verbose mode
 *
 * Return:
 *      One summary result for training tree. Please refer to the structure of
 *      'MADLIB_SCHEMA.c45_train_result' for detailed definition.
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__train_tree_tabular
    (
    split_criterion         TEXT,
    training_table_name     TEXT, 
    result_tree_table_name  TEXT,
    validation_table_name   TEXT, 
    cont_feature_col_names  TEXT[], 
    feature_col_names       TEXT[], 
    id_col_name             TEXT, 
    class_col_name          TEXT, 
    conflevel               FLOAT,
    max_num_iter            INT, 
    max_tree_depth          INT, 
    min_percent_mode        FLOAT,
    min_percent_split       FLOAT, 
    enable_chisq_pruning    boolean,
    verbosity               INT
    ) 
RETURNS MADLIB_SCHEMA.c45_train_result AS $$
declare
    training_table_name_svec TEXT;
    training_table_name_meta TEXT;
    cont_dimensions INT[];
    index INT;
    str_val TEXT;
    str_val2 TEXT;
    ret MADLIB_SCHEMA.c45_train_result;
    execBegin TIMESTAMP;
begin
    execBegin = clock_timestamp();
    
    str_val = MADLIB_SCHEMA.__strip_schema_name(result_tree_table_name);
    str_val2 = MADLIB_SCHEMA.__get_schema_name(result_tree_table_name);
    IF(verbosity > 0) THEN
            RAISE INFO 'table name after strip schema:%',str_val;
    END IF;
    training_table_name_svec = 'MADLIB_SCHEMA.' || str_val2 || '_' ||str_val || '_svec';
    training_table_name_meta =  str_val2 || '_' || str_val || '_data_info';
 
    IF(verbosity > 0) THEN
            RAISE INFO 'Before convert: %', clock_timestamp() - execBegin;
    END IF; 
        
    IF ( feature_col_names is not NULL ) THEN
        PERFORM MADLIB_SCHEMA.__mad_convert_tbl_to_svec(
            training_table_name,
            id_col_name,
            feature_col_names,
            class_col_name,
            cont_feature_col_names,
            training_table_name_svec,
            training_table_name_meta,
            't');
    ELSE
        PERFORM MADLIB_SCHEMA.__mad_convert_tbl_to_svec(
            training_table_name,
            id_col_name,
            class_col_name,
            cont_feature_col_names,
            training_table_name_svec,
            training_table_name_meta,
            't');
    END IF;

    
    PERFORM  MADLIB_SCHEMA.__set_conversion_metatable_name( 
            result_tree_table_name, 
            training_table_name_meta,
            training_table_name_svec);
    IF(verbosity > 0) THEN
            RAISE INFO 'After convert: %', clock_timestamp() - execBegin;
            RAISE INFO 'successfully convert to svec table :%',training_table_name_svec;
    END IF;    

    if (cont_feature_col_names is null) then
        cont_dimensions = null;
    else
        index = array_lower(cont_feature_col_names, 1);
        while ( index <= array_upper(cont_feature_col_names, 1) ) loop
            cont_dimensions[ index ] = MADLIB_SCHEMA.__get_feature_index(
                    cont_feature_col_names[index],
                    training_table_name_meta);
            index = index +1;
        end loop;
    end if;

    IF ( verbosity > 0 ) THEN
            RAISE INFO 'svec continuous features:%', cont_dimensions;
    END IF;

    ret = MADLIB_SCHEMA.__train_tree_svec(
            split_criterion ,
            training_table_name_svec ,
            result_tree_table_name ,
            validation_table_name , 
            cont_dimensions , 
            'feature', 
            'id', 
            'class', 
            conflevel ,
            max_num_iter , 
            max_tree_depth , 
            min_percent_mode ,
            min_percent_split,
            enable_chisq_pruning,
            verbosity );

    IF ( verbosity > 0 ) THEN
            RAISE INFO 'Total Time: %', clock_timestamp() - execBegin;
    END IF;
                
    return ret;
end
$$ language plpgsql;


/**
 * @brief Train a decision tree model. User must specify all those data with that 
 *        long form function.
 *
 * @param split_criterion_name This parameter specifies which split criterion 
 *          should be used for tree construction and pruning. 
 *          The valid values are gain, gainratio and gini.
 * @param training_table_name Name of the table/view with the source data
 * @param result_tree_table_name The name of the table where the resulting DT will be stored.
 * @param validation_table_name The validation table used for pruning tree. 
 * @param continuous_feature_names A comma-separated list of the names of the features whose values are continuous. 
 * @param is_sparse_vector_format A true indicates sparse vector format; a false indicates tabular format.  
 * @param feature_col_names A comma-separated list of names of the table columns, each of which defines a feature.
 *          If the training table if encoded as a sparse vector, then there is only one name in the list.  
 * @param id_col_name Name of the column containing id of each point.
 * @param class_col_name Name of the column containing correct class of each point.
 * @param confidence_level A  statistical  confidence  interval of  the   resubstitution  error.
 * @param max_num_iter Max number of branches to follow (e.g. 2000)
 * @param max_tree_depth Maximum decision tree depth 
 * @param min_percent_mode Specifies the minimum number of cases required in a child node
 * @param min_percent_split specifies the minimum number of cases required in a node in order for 
 *           a further split to be possible.
 * @param verbosity If True (or 1) will run in verbose mode
 *
 * @return Table MADLIB_SCHEMA.tree:
 *  - <tt>id SERIAL</tt> - Tree node id
 *  - <tt>tree_location INT[]</tt> - Set of values that lead to this branch. 
 *     0 is the initial point (no value). But this path does not specify which 
 *     feature was used for the branching.
 *  - <tt>hash INT</tt>: Hash value of the path. For quick unique identification.
 *  - <tt>feature INT</tt>: Which element of the feature vector was used for 
 *     branching at this node. Notice that this feature is not used in the current 
 *     <tt>tree_location</tt>, it will be added in the next step.
 *  - <tt>probability FLOAT</tt> - If forced to make a call for a dominant class 
 *     at a given point this would be the confidence of the call (this is only an 
 *     estimated value).
 *  - <tt>chisq FLOAT</tt> - Chi-square value of the branching significance, 
 *     used to determine termination of the branch.
 *  - <tt>maxclass INTEGER</tt> - If forced to make a call for a dominant class 
 *     at a given point this is the selected class.
 *  - <tt>split_gain FLOAT</tt> - Information gain computed using entropy (at this 
 *     node), also used to determine termination of the branch.
 *  - <tt>live INT</tt> - Indication that the branch is still growing. 1 means "live". 
 *     Exit value may be 1 if number of branches reached before termination condition is met.
 *  - <tt>cat_size INT</tt> - Number of data point at this node.
 *  - <tt>parent_id INT</tt> - Id of the parent branch.
 *  - <tt>jump INT[]</tt> - Location of children for each feature value. Notice 
 *     that assuming that the data is sparse we can have value 0 - representing 
 *     no value. Result such as [2:3]={2,3}, should be read: 
 *     jump['feature value'+1], so in this case there were no 0-value points for 
 *     this feature. For value 1 jump to 2; for value 2 jump to 3;
 *  - <tt>is_feature_cont boolean</tt> - It specifies whether the selected feature is a continuous feature.
 *  - <tt>split_value FLOAT</tt> - For continuous feature, it specifies the split value. Otherwise, 
 *     it is fixed to 0.
 *
 * @note This function starts an iterative algorithm. It is not an aggregate
 *       function. Source and column names have to be passed as strings (due to
 *       limitations of the SQL syntax).
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_train
    (
    split_criterion             TEXT,
    training_table_name         TEXT, 
    result_tree_table_name      TEXT,
    validation_table_name       TEXT, 
    is_svec_format              boolean,
    continuous_feature_names    TEXT, 
    feature_col_names           TEXT, 
    id_col_name                 TEXT, 
    class_col_name              TEXT, 
    conflevel                   FLOAT,
    max_num_iter                INT, 
    max_tree_depth              INT, 
    min_percent_mode            FLOAT,
    min_percent_split           FLOAT, 
    verbosity                   INT
    ) 
RETURNS MADLIB_SCHEMA.c45_train_result AS $$
declare
    cont_dimensions INT[];
    cont_feature_col_names TEXT[];
    feature_name_array TEXT[];
    index INT;
    str_val TEXT;
    int_val INT;
    ret MADLIB_SCHEMA.c45_train_result;
begin   
    
    PERFORM MADLIB_SCHEMA.__create_global_metatable();

    PERFORM MADLIB_SCHEMA.__insert_record_to_global_metatable(
                        result_tree_table_name,
                        training_table_name,
                        id_col_name,
                        feature_col_names,
                        class_col_name,
                        is_svec_format,
                        null,
                        null,
                        continuous_feature_names,
                        validation_table_name);

    
    cont_feature_col_names = MADLIB_SCHEMA.__convert_csv_to_array(continuous_feature_names);
    
    IF ( verbosity > 0 ) THEN
            RAISE INFO 'continuous features:%', cont_feature_col_names;
    END IF;
    
    IF ( cont_feature_col_names is null ) THEN
        cont_dimensions = null;
    ELSIF ( is_svec_format ) THEN
        index = array_lower(cont_feature_col_names,1);

        WHILE ( index <= array_upper(cont_feature_col_names,1) ) LOOP
            cont_dimensions[index] = cont_feature_col_names[index]::INT;
            index = index +1;
        END LOOP;
    ELSE
        cont_dimensions = null;
    END IF;
    
    IF ( is_svec_format ) THEN
        IF(verbosity > 0) THEN
            RAISE INFO 'invoke sparce vector format';
        END IF;
        ret = MADLIB_SCHEMA.__train_tree_svec(
            split_criterion ,
            training_table_name ,
            result_tree_table_name ,
            validation_table_name , 
            cont_dimensions , 
            feature_col_names , 
            id_col_name , 
            class_col_name , 
            conflevel ,
            max_num_iter , 
            max_tree_depth , 
            min_percent_mode ,
            min_percent_split,
            't',
            verbosity );
    ELSE
        IF(verbosity > 0) THEN
            RAISE INFO 'invoke tabular format';
        END IF;
        feature_name_array = MADLIB_SCHEMA.__convert_csv_to_array(feature_col_names);
        ret = MADLIB_SCHEMA.__train_tree_tabular(
            split_criterion ,
            training_table_name , 
            result_tree_table_name ,
            validation_table_name , 
            cont_feature_col_names , 
            feature_name_array , 
            id_col_name , 
            class_col_name , 
            conflevel ,
            max_num_iter , 
            max_tree_depth , 
            min_percent_mode ,
            min_percent_split,
            't',
            verbosity );
    END IF;
    return ret;
end
$$ language plpgsql;


/**
 * @brief   C45 train algorithm in short form
 *
 * @param split_criterion_name This parameter specifies which split criterion 
 *          should be used for tree construction and pruning. 
 *          The valid values are gain, gainratio and gini.
 * @param training_table_name Name of the table/view with the source data
 * @param result_tree_table_name The name of the table where the resulting DT will be stored.
 *
 * @return Table MADLIB_SCHEMA.tree:
 *  - <tt>id SERIAL</tt> - Tree node id
 *  - <tt>tree_location INT[]</tt> - Set of values that lead to this branch. 
 *     0 is the initial point (no value). But this path does not specify which 
 *     feature was used for the branching.
 *  - <tt>hash INT</tt>: Hash value of the path. For quick unique identification.
 *  - <tt>feature INT</tt>: Which element of the feature vector was used for 
 *     branching at this node. Notice that this feature is not used in the current 
 *     <tt>tree_location</tt>, it will be added in the next step.
 *  - <tt>probability FLOAT</tt> - If forced to make a call for a dominant class 
 *     at a given point this would be the confidence of the call (this is only an 
 *     estimated value).
 *  - <tt>chisq FLOAT</tt> - Chi-square value of the branching significance, 
 *     used to determine termination of the branch.
 *  - <tt>maxclass INTEGER</tt> - If forced to make a call for a dominant class 
 *     at a given point this is the selected class.
 *  - <tt>split_gain FLOAT</tt> - Information gain computed using entropy (at this 
 *     node), also used to determine termination of the branch.
 *  - <tt>live INT</tt> - Indication that the branch is still growing. 1 means "live". 
 *     Exit value may be 1 if number of branches reached before termination condition is met.
 *  - <tt>cat_size INT</tt> - Number of data point at this node.
 *  - <tt>parent_id INT</tt> - Id of the parent branch.
 *  - <tt>jump INT[]</tt> - Location of children for each feature value. Notice 
 *     that assuming that the data is sparse we can have value 0 - representing 
 *     no value. Result such as [2:3]={2,3}, should be read: 
 *     jump['feature value'+1], so in this case there were no 0-value points for 
 *     this feature. For value 1 jump to 2; for value 2 jump to 3;
 *  - <tt>is_feature_cont boolean</tt> - It specifies whether the selected feature is a continuous feature.
 *  - <tt>split_value FLOAT</tt> - For continuous feature, it specifies the split value. Otherwise, 
 *     it is fixed to 0.
 *
 * @note Some parameters are fixed as below. Please refer to the long form definition.  
 *      validation_table_name:      NULL 
 *      continuous_feature_names:   NULL
 *      is_sparse_vector_format:    't'
 *      feature_col_names:          'feature' 
 *      id_col_name Name:           'id'
 *      class_col_name Name:        'class'
 *      confidence_level:           25
 *      max_num_iter:               2000
 *      max_tree_depth:             10 
 *      min_percent_mode:           0.001
 *      min_percent_split:          0.01 
 *      verbosity:                  0
 */

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_train
    (
    split_criterion         TEXT,
    training_table_name     TEXT, 
    result_tree_table_name  TEXT
    ) 
RETURNS MADLIB_SCHEMA.c45_train_result AS $$
declare
    ret MADLIB_SCHEMA.c45_train_result;
begin   
    
    ret = MADLIB_SCHEMA.c45_train(
        split_criterion,
        training_table_name, 
        result_tree_table_name,
        null,
        null,
        't',
        'feature',
        'ID',
        'class',
        25,
        2000,
        10,
        0.001,
        0.01,
        0           
    );
    return ret;
end
$$ language plpgsql;

/**
 * @brief   Another C45 train algorithm in short form
 *
 * @param split_criterion_name This parameter specifies which split criterion 
 *          should be used for tree construction and pruning. 
 *          The valid values are gain, gainratio and gini.
 * @param training_table_name Name of the table/view with the source data
 * @param result_tree_table_name The name of the table where the resulting DT will be stored.
 * @param validation_table_name The validation table used for pruning tree. 
 * @param continuous_feature_names A comma-separated list of the names of the features whose values are continuous. 
 * @param is_sparse_vector_format A true indicates sparse vector format; a false indicates tabular format.  
 * @param feature_col_names A comma-separated list of names of the table columns, each of which defines a feature.
 *          If the training table if encoded as a sparse vector, then there is only one name in the list.  
 * @param id_col_name Name of the column containing id of each point.
 * @param class_col_name Name of the column containing correct class of each point.
 * @param confidence_level A  statistical  confidence  interval of  the   resubstitution  error.
 *
 * @return Table MADLIB_SCHEMA.tree:
 *  - <tt>id SERIAL</tt> - Tree node id
 *  - <tt>tree_location INT[]</tt> - Set of values that lead to this branch. 
 *     0 is the initial point (no value). But this path does not specify which 
 *     feature was used for the branching.
 *  - <tt>hash INT</tt>: Hash value of the path. For quick unique identification.
 *  - <tt>feature INT</tt>: Which element of the feature vector was used for 
 *     branching at this node. Notice that this feature is not used in the current 
 *     <tt>tree_location</tt>, it will be added in the next step.
 *  - <tt>probability FLOAT</tt> - If forced to make a call for a dominant class 
 *     at a given point this would be the confidence of the call (this is only an 
 *     estimated value).
 *  - <tt>chisq FLOAT</tt> - Chi-square value of the branching significance, 
 *     used to determine termination of the branch.
 *  - <tt>maxclass INTEGER</tt> - If forced to make a call for a dominant class 
 *     at a given point this is the selected class.
 *  - <tt>split_gain FLOAT</tt> - Information gain computed using entropy (at this 
 *     node), also used to determine termination of the branch.
 *  - <tt>live INT</tt> - Indication that the branch is still growing. 1 means "live". 
 *     Exit value may be 1 if number of branches reached before termination condition is met.
 *  - <tt>cat_size INT</tt> - Number of data point at this node.
 *  - <tt>parent_id INT</tt> - Id of the parent branch.
 *  - <tt>jump INT[]</tt> - Location of children for each feature value. Notice 
 *     that assuming that the data is sparse we can have value 0 - representing 
 *     no value. Result such as [2:3]={2,3}, should be read: 
 *     jump['feature value'+1], so in this case there were no 0-value points for 
 *     this feature. For value 1 jump to 2; for value 2 jump to 3;
 *  - <tt>is_feature_cont boolean</tt> - It specifies whether the selected feature is a continuous feature.
 *  - <tt>split_value FLOAT</tt> - For continuous feature, it specifies the split value. Otherwise, 
 *     it is fixed to 0.
 *
 * @note Some parameters are fixed as below. Please refer to the long form definition.    
 *      max_num_iter:       2000
 *      max_tree_depth:     10 
 *      min_percent_mode:   0.001
 *      min_percent_split:  0.01 
 *      verbosity:          0
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_train
    (
    split_criterion             TEXT,
    training_table_name         TEXT, 
    result_tree_table_name      TEXT,
    validation_table_name       TEXT, 
    is_svec_format              boolean,
    continuous_feature_names    TEXT, 
    feature_col_names           TEXT, 
    id_col_name                 TEXT, 
    class_col_name              TEXT, 
    conflevel                   FLOAT
    ) 
RETURNS MADLIB_SCHEMA.c45_train_result AS $$
declare
    ret MADLIB_SCHEMA.c45_train_result;
begin   
    
    ret = MADLIB_SCHEMA.c45_train(
        split_criterion,
        training_table_name, 
        result_tree_table_name,
        validation_table_name , 
        is_svec_format ,
        continuous_feature_names , 
        feature_col_names , 
        id_col_name , 
        class_col_name , 
        conflevel,
        2000,
        10,
        0.001,
        0.01,
        0           
    );
    return ret;
end
$$ language plpgsql;

/**
 * This is a internal function for displaying the tree in human readable format.
 * It use the depth-first strategy to traverse a tree and print values.
 * Parameters:
 *      tree_table:         The name of the table with information for the 
 *                          trained tree.
 *      id:                 The id of current node. This node and all of its  
 *                          children are displayed.
 *      feature_id:         The id of a feature, which was used to split in the 
 *                          parent of current node.
 *      depth:              The depth of current node.
 *      is_cont:            It specifies whether the feature denoted by 'feature_id'
 *                          is continuous or not.
 *      sp_val:             For continuous feature, it specifies the split value. 
 *                          Otherwise, it is of no meaning.
 *      is_svec_format:     It specifies whether the training set used to train the
 *                          decision tree is of sparse vector format or tabular 
 *                          format.
 *      meta_table:         For tabular format, this table contains the meta data
 *                          to transform sparse vector value back to tabular data.
 * Return:
 *      It returns the text containing the information of human readable tree.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__display_tree
    (
    tree_table      TEXT, 
    id              INT, 
    feature_id      INT, 
    depth           INT, 
    is_cont         boolean, 
    sp_val          FLOAT,
    is_svec_format  boolean, 
    meta_table      TEXT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__display_tree
    (
    tree_table      TEXT, 
    id              INT, 
    feature_id      INT, 
    depth           INT, 
    is_cont         boolean, 
    sp_val          FLOAT,
    is_svec_format  boolean, 
    meta_table      TEXT
    ) 
RETURNS TEXT AS $$ 
declare
    ret             TEXT := '';
    tree_location   INT[];
    feature         INT;
    jump            INT[];
    maxclass        INT;
    cat_size        INT;
    is_feature_cont boolean;
    sp_value        FLOAT;
    index           INT;
    curr_value      INT;
    probability     FLOAT;
begin
    if ( id is null or id <= 0 ) then
        return ret;
    end if;
    
    execute 'select tree_location, feature, jump,is_feature_cont, split_value,
        maxclass,cat_size,probability from '
        || tree_table || ' where id =' || id ||';' INTO tree_location, feature,jump, 
        is_feature_cont,sp_value,maxclass, cat_size, probability; 

    curr_value = tree_location[array_upper(tree_location,1)];

    if( id > 1 ) then
        for index in 0..depth loop
            ret = ret || '    ';
        end loop;

        if( is_svec_format ) then
            ret = ret || 'Feature ID:' || feature_id ||' ';
        else
            ret = ret ||MADLIB_SCHEMA.__get_feature_name(feature_id,meta_table)||': ';
        end if;

        if ( is_cont ) then
            if( curr_value = 1 ) then
                ret = ret || ' <= ';
            else
                ret = ret || ' > ';
            end if;
            ret = ret || sp_val;
        else
            if( is_svec_format ) then
                ret = ret || ' = ' || curr_value;
            else
                ret = ret || ' = ' 
                    || MADLIB_SCHEMA.__get_feature_value(
                        feature_id, 
                        curr_value, 
                        meta_table);
            end if;
        end if;

        if( is_svec_format ) then
            ret = ret || ' : class(' || maxclass || ')   num_elements(' || 
                cat_size || ')  predict_prob('||probability||')';
        else
            ret = ret || ' : class(' ||  MADLIB_SCHEMA.__get_class_value(maxclass,meta_table) 
                || ')   num_elements(' || cat_size || ')  predict_prob('||probability||')';
        end if;

        ret = ret || E'\n';
    end if;

    index = array_lower(jump,1);
    WHILE index <= array_upper(jump,1) LOOP
        ret = ret || MADLIB_SCHEMA.__display_tree(
                            tree_table, 
                            jump[index], 
                            feature, 
                            depth+1, 
                            is_feature_cont, 
                            sp_value, 
                            is_svec_format, 
                            meta_table);
        index = index +1;
    END LOOP; 

    return ret;
end $$ LANGUAGE plpgsql;


/**
 * @brief Display the trained decision tree model with human readable format
 *
 * @param tree_table Name of the table containing the tree's information
 *
 * @return the text representing the tree with human readable format.
 *
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.c45_display
    (
    tree_table TEXT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_display
    (
    tree_table TEXT
    ) 
RETURNS TEXT AS $$
declare
    meta_table_name TEXT := null;
begin
    meta_table_name = MADLIB_SCHEMA.__get_conversion_metatable_name( tree_table );
    return MADLIB_SCHEMA.__display_tree(tree_table, meta_table_name);
end $$ LANGUAGE plpgsql;

/**
 * This is a internal function for displaying the tree in human readable format.
 * 
 * Parameters:
 *      tree_table:         The name of the table with information for the 
 *                          trained tree.
 *      meta_table:         For tabular format, this table contains the meta data
 *                          to transform sparse vector value back to tabular data.
 * Return:
 *      It returns the text containing the information of human readable tree.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__display_tree(TEXT,TEXT);
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__display_tree(
        tree_table TEXT,
        meta_table TEXT) 
RETURNS TEXT AS $$ 
begin
    -- If we cannot find the meta table for the tree, the meta table would be NULL.
    -- In that case, we think the training set used to train the decision tree
    -- is in sparse vector format. Otherwise, it is in tabular format.
    if ( meta_table is not null ) then
        return MADLIB_SCHEMA.__display_tree(tree_table, 1, 0, 0, 'f', 0,'f',meta_table);
    else
        return MADLIB_SCHEMA.__display_tree(tree_table, 1, 0, 0, 'f', 0,'t',null);
    end if;
end $$ LANGUAGE plpgsql;


/**
 *  An internal c45 classification function. It is used to perform
 *  the real classification process.
 *
 *  Parameters:
 *      classification_table_name:  The table containing the classification set.
 *      tree_table_name:            The table containing the final tree.
 *      result_table_name:          The table containing the classification
 *                                  result.
 *      is_sparse_vector:           Whether the classification set is in
 *                                  sparse vector format.
 *      verbosity:                  Whether printing those debug information.
 *  Return:
 *      The name of the table containing classification set in sparse vector
 *      format. The caller may need clean that internal table.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__c45_classify_internal
    (
    classification_table_name   TEXT, 
    tree_table_name             TEXT, 
    result_table_name           TEXT, 
    is_sparse_vector            BOOLEAN,
    verbosity                   BOOLEAN
    );
    
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__c45_classify_internal
    (
    classification_table_name   TEXT, 
    tree_table_name             TEXT, 
    result_table_name           TEXT, 
    is_sparse_vector            BOOLEAN,
    verbosity                   BOOLEAN
    ) 
RETURNS TEXT AS $$
declare
    table_names TEXT[] = '{classified_instance1,classified_instance2}';
    table_pick INT := 1;
    remains_to_classify INT;
    size_finished INT;
    time_stamp TIMESTAMP;
    svec_name TEXT := classification_table_name || '_svec_temp';
    meta_table_name TEXT := '';
    id_col_name TEXT := 'id';
    feature_col_name TEXT := 'feature';
    curr_level INT := 1;
    max_level INT := 0;
begin
    time_stamp = clock_timestamp();
    
    SELECT MADLIB_SCHEMA.__get_conversion_metatable_name(tree_table_name) INTO meta_table_name;
    
    IF (NOT is_sparse_vector) THEN
        PERFORM MADLIB_SCHEMA.__mad_convert_tbl_to_svec(
                classification_table_name, 
                svec_name, 
                meta_table_name, 
                't');
    ELSE
        svec_name = classification_table_name;
    END IF;
    
    DROP TABLE IF EXISTS classified_instance1;
    CREATE TEMP TABLE classified_instance1(
        id INT,
        feature MADLIB_SCHEMA.svec,
        jump INT,
        class INT,
        prob FLOAT,
        parent_id INT,
        leaf_id INT
    ) m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (jump)');
    
    DROP TABLE IF EXISTS classified_instance2;
    CREATE TEMP TABLE classified_instance2(
        id INT,
        feature MADLIB_SCHEMA.svec,
        jump INT,
        class INT,
        prob FLOAT,
        parent_id INT,
        leaf_id INT
    ) m4_ifdef(`GREENPLUM',`DISTRIBUTED BY (jump)');
    
    execute 'DROP TABLE IF EXISTS ' || result_table_name ||' cascade;';
    execute 'CREATE TABLE ' || result_table_name || E'(
        id INT,
        feature MADLIB_SCHEMA.svec,
        jump INT,
        class INT,
        prob FLOAT,
        parent_id INT,
        leaf_id INT
    ) m4_ifdef(\`GREENPLUM\',\`DISTRIBUTED BY (jump)\');';

    EXECUTE 'INSERT INTO classified_instance1 (id, feature, jump, class, prob) SELECT '
        ||id_col_name||', '||feature_col_name||', 1, 0, 0 FROM ' || svec_name || ';';  

    
    EXECUTE 'SELECT max(array_upper(tree_location,1)) FROM '||tree_table_name||';' 
        INTO max_level;

    FOR curr_level IN 1..max_level LOOP
        IF(verbosity) THEN  
            RAISE INFO 'new_depth: %', curr_level;
        END IF;

        EXECUTE 'INSERT INTO ' || result_table_name ||' SELECT * FROM '|| 
            table_names[(table_pick)%2+1] ||' WHERE jump = 0;';
        EXECUTE 'TRUNCATE '|| table_names[(table_pick)%2+1] ||';';
        EXECUTE 'SELECT count(*) FROM '||result_table_name||';' INTO size_finished;
        IF(verbosity) THEN  
            RAISE INFO 'size_finished %', size_finished;
        END IF;            
        table_pick = table_pick%2+1; 
        
        EXECUTE 'SELECT count(*) FROM '|| table_names[(table_pick)%2+1] ||';' 
            INTO remains_to_classify;
        IF (remains_to_classify = 0) THEN
            IF(verbosity) THEN  
                RAISE INFO 'size_finished: % remains_to_classify: %', 
                    size_finished, remains_to_classify;
            END IF;        
            EXIT;
        END IF;

        IF(verbosity) THEN  
            RAISE INFO 'discrete feature classification';
        END IF;         
        EXECUTE 'INSERT INTO '|| table_names[table_pick] ||
            ' SELECT pt.id, pt.feature, 
            COALESCE(gt.jump[MADLIB_SCHEMA.svec_proj(pt.feature, gt.feature)+1],0), 
            gt.maxclass, gt.probability, gt.parent_id, gt.id FROM 
                (SELECT * FROM '||table_names[(table_pick)%2+1] || E') AS pt, 
                (SELECT * FROM '||tree_table_name||E' WHERE is_feature_cont != \'t\' 
                    and array_upper(tree_location,1) = '||curr_level ||') AS gt 
                where pt.jump = gt.id ;';

        IF(verbosity) THEN  
            RAISE INFO 'continuous feature classification';
        END IF;           
        EXECUTE 'INSERT INTO '|| table_names[table_pick] ||
            ' SELECT pt.id, pt.feature, 
            COALESCE(gt.jump[MADLIB_SCHEMA.__is_less(gt.split_value,MADLIB_SCHEMA.svec_proj(pt.feature, gt.feature))+2],0), 
            gt.maxclass, gt.probability,gt.parent_id, gt.id  FROM 
            (SELECT * FROM '|| table_names[(table_pick)%2+1] ||E') AS pt, 
            (SELECT * FROM '||tree_table_name||E' WHERE is_feature_cont = \'t\' 
                and array_upper(tree_location,1) = '||curr_level ||') AS gt 
            where pt.jump = gt.id ;'; 
    END LOOP;

    EXECUTE 'INSERT INTO '||result_table_name||' SELECT * FROM '|| 
        table_names[table_pick] ||' WHERE jump = 0;';
    EXECUTE 'INSERT INTO '||result_table_name||' SELECT * FROM '|| 
        table_names[table_pick%2+1] ||' WHERE jump = 0;';
    
    IF(verbosity) THEN  
        RAISE INFO 'final classification time:%', clock_timestamp()-time_stamp;
    END IF;
    
    return svec_name;
end
$$ language plpgsql;
   
    
/**
 * @brief Classify data points using trained decision tree model
 *
 * @param classification_table_name Name of the table/view with the source data
 * @param tree_table_name Name of trained tree
 * @param result_table_name Name of result table
 * @param verbosity If set to 't' will use verbose mode
 *
 * @return Table MADLIB_SCHEMA.classified_points:
 *  - <tt>id INT</tt> - Point id.
 *  - <tt>feature MADLIB_SCHEMA.SVEC</tt> - Point feature vector. 
 *  - <tt>jump INT</tt> - Intermediate value used to distinguish leaf nodes.
 *  - <tt>class INT</tt> - Class prediction. 
 *  - <tt>prob FLOAT</tt> - Probability of the predicted class.
 *
 * @note This function starts an iterative algorithm. It is not an aggregate
 *       function. Source and column names have to be passed as strings (due to
 *       limitations of the SQL syntax).
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.c45_classify
    (
    classification_table_name   TEXT, 
    tree_table_name             TEXT, 
    result_table_name           TEXT, 
    verbosity                   BOOLEAN
    );
    
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_classify
    (
    classification_table_name   TEXT, 
    tree_table_name             TEXT, 
    result_table_name           TEXT, 
    verbosity                   BOOLEAN
    ) 
RETURNS MADLIB_SCHEMA.c45_classify_result AS $$
declare
    svec_table_name TEXT := '';
    is_sparse_vector BOOLEAN := 'f';
    begin_time  TIMESTAMP;
    ret MADLIB_SCHEMA.c45_classify_result;
begin
    begin_time = clock_timestamp();
    SELECT MADLIB_SCHEMA.__get_input_format(tree_table_name) INTO is_sparse_vector;
    
    SELECT MADLIB_SCHEMA.__c45_classify_internal
    (
        classification_table_name, 
        tree_table_name, 
        result_table_name, 
        is_sparse_vector,
        verbosity
    ) INTO svec_table_name;
    
    IF (NOT is_sparse_vector) THEN
        EXECUTE 'DROP TABLE IF EXISTS ' || svec_table_name || ';';
    END IF;
    EXECUTE 'SELECT COUNT(*) FROM ' ||classification_table_name||';' INTO ret.input_set_size;
    ret.cost_time = clock_timestamp()-begin_time;
    RETURN ret;
end
$$ language plpgsql;


/**
 * @brief   Classify the data with no verbosity
 *  
 * @param classification_table_name Name of the table/view with the source data
 * @param tree_table_name Name of trained tree
 * @param result_table_name Name of result table
 *
 * @return Table MADLIB_SCHEMA.classified_points:
 *  - <tt>id INT</tt> - Point id.
 *  - <tt>feature MADLIB_SCHEMA.SVEC</tt> - Point feature vector. 
 *  - <tt>jump INT</tt> - Intermediate value used to distinguish leaf nodes.
 *  - <tt>class INT</tt> - Class prediction. 
 *  - <tt>prob FLOAT</tt> - Probability of the predicted class.
 *
 * @note This function starts an iterative algorithm. It is not an aggregate
 *       function. Source and column names have to be passed as strings (due to
 *       limitations of the SQL syntax).   
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.c45_classify
    (
    classification_table_name   TEXT, 
    tree_table_name             TEXT, 
    result_table_name           TEXT
    );
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_classify
    (
    classification_table_name   TEXT, 
    tree_table_name             TEXT, 
    result_table_name           TEXT
    ) 
RETURNS MADLIB_SCHEMA.c45_classify_result AS $$
declare
    ret MADLIB_SCHEMA.c45_classify_result;
begin
	ret = MADLIB_SCHEMA.c45_classify
	   (
	       classification_table_name, 
	       tree_table_name,
	       result_table_name,
	       'f'
	   );
    RETURN ret;
end $$ LANGUAGE plpgsql;



/**
 * @brief   Check the accuracy of the decision tree alogrithom 
 * 
 * @param classification_table_name Name of the table/view with the source data
 * @param tree_table_name Name of trained tree
 * @param verbosity If set to 't' will use verbose mode 
 * @return The estimated accuracy information.
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.c45_score
    (
    classification_table_name   TEXT, 
    tree_table_name             TEXT, 
    verbosity                   BOOLEAN
    );
    
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_score
    (
    classification_table_name   TEXT, 
    tree_table_name             TEXT, 
    verbosity                   BOOLEAN
    ) 
RETURNS FLOAT AS $$
declare
    result_table_name TEXT = 'mad_predict_table_temp';
    id_col_name TEXT := 'id';
    class_col_name TEXT := 'class';
    curStmt TEXT := '';
    num_of_row FLOAT := 0.0;
    mis_of_row FLOAT := 0.0;
    svec_table_name TEXT := '';
    is_sparse_vector BOOLEAN := 'f';    
begin
    SELECT MADLIB_SCHEMA.__get_input_format(tree_table_name) INTO is_sparse_vector;
    
    SELECT MADLIB_SCHEMA.__c45_classify_internal
    (
        classification_table_name, 
        tree_table_name, 
        result_table_name, 
        is_sparse_vector,
        verbosity
    ) INTO svec_table_name;
    
    SELECT MADLIB_SCHEMA.__format('SELECT count(%) FROM %;',
        id_col_name,
        result_table_name) INTO curStmt;
    
    --RAISE INFO '%', curStmt;
    EXECUTE curStmt INTO num_of_row;
    
    SELECT MADLIB_SCHEMA.__format('SELECT count(b.id) FROM % a, % b WHERE a.%=b.id and a.%<>b.class',
        ARRAY[svec_table_name,
        result_table_name,
        id_col_name,
        class_col_name]) INTO curStmt;
     --RAISE INFO '%', curStmt;
     
     EXECUTE curStmt INTO mis_of_row;
     
    IF (NOT is_sparse_vector) THEN
        EXECUTE 'DROP TABLE IF EXISTS ' || svec_table_name || ';';
    END IF;
    
     return (num_of_row - mis_of_row) / num_of_row;
    
end;
$$ LANGUAGE plpgsql;


/**
 * @brief   Cleanup the trained tree table and any relevant tables
 *
 * @param result_tree_table_name Name of the table containing the tree's information
 *
 * @return boolean value indicating the status of that cleanup operation.
 *
 */
DROP FUNCTION IF EXISTS  MADLIB_SCHEMA.c45_clean(TEXT);
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.c45_clean
    ( 
    result_tree_table_name TEXT
    ) 
RETURNS BOOLEAN AS $$
declare
    meta_table TEXT;
begin
    meta_table = MADLIB_SCHEMA.__get_conversion_metatable_name( result_tree_table_name );
    IF( meta_table is not NULL) THEN
        PERFORM MADLIB_SCHEMA.__drop_meta_table(meta_table);
        EXECUTE 'DROP TABLE IF EXISTS ' || 
            MADLIB_SCHEMA.__get_conversion_svec_name( result_tree_table_name ) || ';';
    END IF;
    EXECUTE 'DROP TABLE IF EXISTS ' || result_tree_table_name || ';';
    PERFORM MADLIB_SCHEMA.__remove_record_from_global_metatable(result_tree_table_name);
    RETURN 't';    
end
$$ language plpgsql;
